
"""
Tushareé‡åŒ–äº¤æ˜“ç³»ç»Ÿåç«¯
éœ€è¦å®‰è£…: pip install tushare flask flask-cors pandas numpy
"""

import tushare as ts
import pandas as pd
import numpy as np
from flask import Flask, jsonify, request, send_from_directory, send_file
from flask_cors import CORS
from datetime import datetime, timedelta
import json
import hashlib
import io
import os
import re
import shutil
import sys
import tempfile
import threading
import time
import urllib.request
import urllib.parse
import urllib.error
import xgboost as xgb
try:
    from tensorflow import keras
    from tensorflow.keras.models import Sequential
    from tensorflow.keras.layers import LSTM, Dense, Dropout, BatchNormalization, Bidirectional
    from tensorflow.keras.optimizers import Adam
    from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau
    from tensorflow.keras.regularizers import l2
    from sklearn.preprocessing import StandardScaler
    from sklearn.model_selection import train_test_split
    from sklearn.metrics import precision_score, recall_score, f1_score
    TENSORFLOW_AVAILABLE = True
except ImportError:
    TENSORFLOW_AVAILABLE = False
    print("è­¦å‘Š: TensorFlowæˆ–scikit-learnæœªå®‰è£…ï¼ŒLSTMåŠŸèƒ½å°†ä¸å¯ç”¨ã€‚å®‰è£…å‘½ä»¤: pip install tensorflow scikit-learn")

app = Flask(__name__)

# è¯·æ±‚æ—¥å¿—ä¸­é—´ä»¶
@app.before_request
def log_request():
    print(f"[REQUEST] {request.method} {request.path}", flush=True)

CORS(
    app,
    resources={
        r"/api/*": {
            "origins": [
                r"^https?://localhost(:\d+)?$",
                r"^https?://127\.0\.0\.1(:\d+)?$",
                r"^https?://.*\.vercel\.app$",
                "null",
            ]
        }
    },
    supports_credentials=True,
)

VERBOSE = (os.getenv('QUANT_VERBOSE') or '0').strip() == '1'

def _log(*args, **kwargs):
    if VERBOSE:
        print(*args, **kwargs)

_RUNTIME_STATS_LOCK = threading.Lock()
_RUNTIME_STATS = {
    'stock_data_requests': 0,
    'stock_data_lite_requests': 0,
    'stock_pool_quotes_requests': 0,
    'local_state_writes': 0,
    'local_state_skipped_writes': 0,
    'cache_df_mem_hits': 0,
    'cache_df_disk_reads': 0,
    'cache_csv_writes': 0,
    'tushare_fetch_calls': 0,
}

def _inc_stat(name, value=1):
    try:
        with _RUNTIME_STATS_LOCK:
            _RUNTIME_STATS[name] = int(_RUNTIME_STATS.get(name) or 0) + int(value)
    except Exception:
        pass

def _get_app_dir():
    if getattr(sys, 'frozen', False):
        return os.path.dirname(sys.executable)
    return os.path.dirname(os.path.abspath(__file__))


APP_DIR = _get_app_dir()
RESOURCE_DIR = getattr(sys, '_MEIPASS', APP_DIR)


def _get_data_root():
    if os.environ.get('VERCEL'):
        return '/tmp'
    configured = os.getenv('QUANT_DATA_DIR')
    if configured:
        return configured
    if getattr(sys, 'frozen', False):
        base = os.getenv('LOCALAPPDATA') or APP_DIR
        return os.path.join(base, 'TushareQuantSystem')
    return APP_DIR


DATA_ROOT = _get_data_root()
try:
    os.makedirs(DATA_ROOT, exist_ok=True)
except Exception:
    pass


STOCK_LIST_CACHE_FILE = os.path.join(DATA_ROOT, 'stock_list_cache.json')
CONCEPT_LIST_CACHE_FILE = os.path.join(DATA_ROOT, 'concept_list_cache.json')
CONCEPT_MEMBERS_CACHE_DIR = os.path.join(DATA_ROOT, 'concept_members')
try:
    os.makedirs(CONCEPT_MEMBERS_CACHE_DIR, exist_ok=True)
except Exception:
    pass

FUNDAMENTALS_CACHE_DIR = os.path.join(DATA_ROOT, 'fundamentals_cache')
try:
    os.makedirs(FUNDAMENTALS_CACHE_DIR, exist_ok=True)
except Exception:
    pass


LOCAL_STATE_DIR = os.path.join(DATA_ROOT, 'local_state')
try:
    os.makedirs(LOCAL_STATE_DIR, exist_ok=True)
except Exception:
    pass


_LOCAL_STATE_LOCK = threading.Lock()
_LOCAL_STATE_LAST_DIGEST = {}
_ALLOWED_LOCAL_STATE_KEYS = {
    'quant_ui_state_v1',
    'quant_kline_zoom_state_v1',
    'quant_stock_pool',
    'quant_last_session',
    'strategyConfig',
    'optimizationResults',
}


def _load_json_cache_file(file_path):
    try:
        if not os.path.exists(file_path):
            return None
        with open(file_path, 'r', encoding='utf-8') as f:
            data = json.load(f)
        if not isinstance(data, dict):
            return None
        cached_at = data.get('cached_at')
        items = data.get('data')
        if not cached_at or not isinstance(items, list):
            return None
        return {'cached_at': cached_at, 'data': items}
    except Exception:
        return None


def _save_json_cache_file(file_path, items):
    try:
        payload = {'cached_at': datetime.now().isoformat(), 'data': items}
        with open(file_path, 'w', encoding='utf-8') as f:
            json.dump(payload, f, ensure_ascii=False)
        return True
    except Exception:
        return False


def _load_json_cache_payload(file_path):
    try:
        if not os.path.exists(file_path):
            return None
        with open(file_path, 'r', encoding='utf-8') as f:
            data = json.load(f)
        if not isinstance(data, dict):
            return None
        cached_at = data.get('cached_at')
        if not cached_at:
            return None
        return {'cached_at': cached_at, 'data': data.get('data')}
    except Exception:
        return None


def _save_json_cache_payload(file_path, payload_data):
    try:
        payload = {'cached_at': datetime.now().isoformat(), 'data': payload_data}
        with open(file_path, 'w', encoding='utf-8') as f:
            json.dump(payload, f, ensure_ascii=False)
        return True
    except Exception:
        return False


def _sanitize_local_state_filename(key):
    if not key:
        return None
    safe = re.sub(r"[^a-zA-Z0-9_.-]", "_", str(key))
    safe = safe.strip("._-")
    if not safe:
        return None
    return safe + ".json"


def _local_state_path_for_key(key):
    filename = _sanitize_local_state_filename(key)
    if not filename:
        return None
    return os.path.join(LOCAL_STATE_DIR, filename)


def _read_local_state_value(key):
    path = _local_state_path_for_key(key)
    if not path or not os.path.exists(path):
        return None
    try:
        with _LOCAL_STATE_LOCK:
            with open(path, 'r', encoding='utf-8') as f:
                data = json.load(f)
        if isinstance(data, dict) and 'value' in data:
            return data.get('value')
        return data
    except Exception:
        return None


def _write_local_state_value(key, value):
    path = _local_state_path_for_key(key)
    if not path:
        return False
    try:
        value_json = json.dumps(value, ensure_ascii=False, sort_keys=True, separators=(',', ':'))
    except Exception:
        value_json = repr(value)
    digest = hashlib.sha1(value_json.encode('utf-8', errors='ignore')).hexdigest()
    payload = {'saved_at': datetime.now().isoformat(), 'key': key, 'value': value}
    tmp_path = path + ".tmp"
    try:
        with _LOCAL_STATE_LOCK:
            if _LOCAL_STATE_LAST_DIGEST.get(key) == digest and os.path.exists(path):
                _inc_stat('local_state_skipped_writes', 1)
                return True
            with open(tmp_path, 'w', encoding='utf-8') as f:
                json.dump(payload, f, ensure_ascii=False)
            os.replace(tmp_path, path)
            _LOCAL_STATE_LAST_DIGEST[key] = digest
            _inc_stat('local_state_writes', 1)
        return True
    except Exception:
        try:
            if os.path.exists(tmp_path):
                os.remove(tmp_path)
        except Exception:
            pass
        return False

# æ·»åŠ æ ¹è·¯å¾„è·¯ç”±ï¼Œæä¾›å‰ç«¯é¡µé¢
@app.route('/')
def index():
    """æä¾›å‰ç«¯HTMLé¡µé¢"""
    return send_from_directory(RESOURCE_DIR, 'quant_frontend.html')

# æ·»åŠ é™æ€æ–‡ä»¶è·¯ç”±
@app.route('/<path:filename>')
def static_files(filename):
    """æä¾›é™æ€æ–‡ä»¶"""
    return send_from_directory(RESOURCE_DIR, filename)

@app.route('/api/health', methods=['GET'])
def api_health():
    return jsonify({
        'success': True,
        'token_present': bool(TUSHARE_TOKEN),
        'token_source': TUSHARE_TOKEN_SOURCE,
        'pro_initialized': pro is not None,
        'pro_init_error': _PRO_INIT_ERROR,
        'data_root': DATA_ROOT
    })

@app.route('/api/runtime_stats', methods=['GET'])
def api_runtime_stats():
    with _RUNTIME_STATS_LOCK:
        stats = dict(_RUNTIME_STATS)
    return jsonify({'success': True, 'verbose': VERBOSE, 'stats': stats})

@app.route('/api/stock_pool_quotes', methods=['POST'])
def api_stock_pool_quotes():
    _inc_stat('stock_pool_quotes_requests', 1)
    payload = request.get_json(silent=True) or {}
    codes = payload.get('codes') or []
    if not isinstance(codes, list):
        return jsonify({'success': False, 'message': 'codes å¿…é¡»æ˜¯æ•°ç»„'}), 400
    end_date = (payload.get('end_date') or '').strip() or datetime.now().strftime('%Y-%m-%d')
    freq = (payload.get('freq') or 'D').strip().upper()
    # æ­£ç¡®å¤„ç† cache_only å‚æ•°ï¼ˆæ”¯æŒå¸ƒå°”å€¼å’Œå­—ç¬¦ä¸²ï¼‰
    cache_only_raw = payload.get('cache_only')
    if isinstance(cache_only_raw, bool):
        cache_only = cache_only_raw
    else:
        cache_only = str(cache_only_raw if cache_only_raw is not None else '1').strip().lower() in ('1', 'true', 'yes', 'y')
    
    print(f"[DEBUG] stock_pool_quotes: codes={codes}, end_date={end_date}, cache_only={cache_only}")

    results = []
    for raw in codes:
        ts_code = (str(raw or '')).strip().upper()
        if not ts_code:
            continue
        quote = cache_manager.get_latest_quote(ts_code, end_date=end_date, freq=freq, cache_only=cache_only)
        print(f"[DEBUG] Quote for {ts_code}: {quote}")
        if not quote:
            results.append({'ts_code': ts_code, 'ok': False})
            continue
        results.append({
            'ts_code': ts_code,
            'ok': True,
            'date': quote.get('date'),
            'close': quote.get('close'),
            'pe': quote.get('pe'),
            'pb': quote.get('pb'),
        })

    print(f"[DEBUG] Returning {len(results)} results")
    return jsonify({'success': True, 'end_date': end_date, 'freq': freq, 'cache_only': cache_only, 'data': results})

def _load_tushare_token():
    candidates = [
        'TUSHARE_TOKEN',
        'TS_TOKEN',
        'TUSHARETOKEN',
        'TU_SHARE_TOKEN',
    ]
    for name in candidates:
        env_token = os.getenv(name)
        if env_token and env_token.strip():
            return env_token.strip(), f'env:{name}'
    token_file = os.path.join(DATA_ROOT, 'tushare_token.txt')
    try:
        if os.path.exists(token_file):
            with open(token_file, 'r', encoding='utf-8') as f:
                return ((f.read() or '').strip(), f'file:{token_file}')
    except Exception:
        return '', ''
    return '', ''


TUSHARE_TOKEN = ''
TUSHARE_TOKEN_SOURCE = ''
_PRO_INIT_ERROR = ''
_PRO_INIT_LOCK = threading.Lock()


def _is_writable_dir(path):
    if not path or not os.path.isdir(path):
        return False
    probe = os.path.join(path, '.probe_write')
    try:
        with open(probe, 'w', encoding='utf-8') as f:
            f.write('')
        os.remove(probe)
        return True
    except Exception:
        return False


def _init_tushare_pro():
    global TUSHARE_TOKEN, TUSHARE_TOKEN_SOURCE, pro, _PRO_INIT_ERROR
    token, source = _load_tushare_token()
    TUSHARE_TOKEN = token
    TUSHARE_TOKEN_SOURCE = source

    print("æ­£åœ¨åˆå§‹åŒ–Tushare...")
    try:
        if os.environ.get('VERCEL') and os.name != 'nt':
            fallback_home = DATA_ROOT or '/tmp'
            try:
                os.makedirs(fallback_home, exist_ok=True)
            except Exception:
                pass
            current_home = os.environ.get('HOME')
            if not _is_writable_dir(current_home):
                os.environ['HOME'] = fallback_home
            current_userprofile = os.environ.get('USERPROFILE')
            if not _is_writable_dir(current_userprofile):
                os.environ['USERPROFILE'] = fallback_home
            os.environ.setdefault('XDG_CACHE_HOME', fallback_home)
            os.environ.setdefault('XDG_CONFIG_HOME', fallback_home)

        if not TUSHARE_TOKEN:
            raise RuntimeError("æœªé…ç½®Tushare Tokenï¼Œè¯·è®¾ç½®ç¯å¢ƒå˜é‡TUSHARE_TOKEN/TS_TOKENæˆ–åœ¨æ•°æ®ç›®å½•æ”¾ç½®tushare_token.txt")
        try:
            ts.set_token(TUSHARE_TOKEN)
        except Exception:
            pass
        pro = ts.pro_api(TUSHARE_TOKEN)
        _PRO_INIT_ERROR = ''
        print("[OK] Tushareåˆå§‹åŒ–æˆåŠŸ")
        return pro
    except Exception as e:
        _PRO_INIT_ERROR = str(e)
        print(f"[ERROR] Tushareåˆå§‹åŒ–å¤±è´¥: {e}")
        pro = None
        return None


def _get_pro():
    global pro
    if pro is not None:
        return pro
    with _PRO_INIT_LOCK:
        if pro is not None:
            return pro
        return _init_tushare_pro()

_init_tushare_pro()


INDEX_CONSTITUENTS_CACHE_FILE = os.path.join(DATA_ROOT, 'index_constituents_cache.json')


def _load_index_cache_file():
    try:
        if not os.path.exists(INDEX_CONSTITUENTS_CACHE_FILE):
            return {}
        with open(INDEX_CONSTITUENTS_CACHE_FILE, 'r', encoding='utf-8') as f:
            data = json.load(f)
        return data if isinstance(data, dict) else {}
    except Exception as e:
        print(f"è¯»å–æŒ‡æ•°æˆåˆ†ç¼“å­˜æ–‡ä»¶å¤±è´¥: {e}")
        return {}


def _save_index_cache_file(cache_dict):
    try:
        with open(INDEX_CONSTITUENTS_CACHE_FILE, 'w', encoding='utf-8') as f:
            json.dump(cache_dict, f, ensure_ascii=False)
    except Exception as e:
        print(f"å†™å…¥æŒ‡æ•°æˆåˆ†ç¼“å­˜æ–‡ä»¶å¤±è´¥: {e}")


def _get_index_code_numeric(index_code):
    if not index_code:
        return None
    s = str(index_code).strip().upper()
    return s.split('.')[0] if '.' in s else s


def _fetch_index_constituents_from_sina(index_code):
    index_id = _get_index_code_numeric(index_code)
    if not index_id or not re.fullmatch(r"\d{6}", index_id):
        return []

    url = f"http://vip.stock.finance.sina.com.cn/corp/go.php/vII_NewestComponent/indexid/{index_id}.phtml"
    headers = {
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120 Safari/537.36',
        'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8'
    }
    req = urllib.request.Request(url, headers=headers)

    try:
        with urllib.request.urlopen(req, timeout=6) as resp:
            raw = resp.read()
        html = raw.decode('gbk', errors='ignore')
    except Exception as e:
        print(f"ä»æ–°æµªæŠ“å–æŒ‡æ•°æˆåˆ†å¤±è´¥: {index_code}: {e}")
        return []

    matches = re.findall(r"\b(sh|sz)(\d{6})\b", html, flags=re.IGNORECASE)
    if not matches:
        return []

    codes = []
    seen = set()
    for market, code in matches:
        ts_code = f"{code}.{'SH' if market.lower() == 'sh' else 'SZ'}"
        if ts_code in seen:
            continue
        seen.add(ts_code)
        codes.append({'ts_code': ts_code, 'name': ts_code})

    return codes


def _fetch_index_constituents_from_legulegu(index_code):
    index_id = str(index_code).strip().upper()
    if not index_id:
        return []
    if '.' not in index_id:
        index_id = f"{index_id}.SH"

    url = f"https://legulegu.com/stockdata/index-basic-composition?indexCode={index_id}"
    headers = {
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120 Safari/537.36',
        'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8'
    }
    req = urllib.request.Request(url, headers=headers)

    try:
        with urllib.request.urlopen(req, timeout=10) as resp:
            raw = resp.read()
        html = raw.decode('utf-8', errors='ignore')
    except Exception as e:
        print(f"ä»ä¹å’•ä¹è‚¡æŠ“å–æŒ‡æ•°æˆåˆ†å¤±è´¥: {index_code}: {e}")
        return []

    expected_count = None
    index_numeric = _get_index_code_numeric(index_id)
    if index_numeric == '000016':
        expected_count = 50
    elif index_numeric == '000300':
        expected_count = 300

    codes = re.findall(r"\b\d{6}\.(?:SH|SZ)\b", html, flags=re.IGNORECASE)
    if not codes:
        return []

    ordered = []
    seen = set()
    skip_code = index_id.upper()
    for c in codes:
        c = c.upper()
        if c == skip_code:
            continue
        if c in seen:
            continue
        seen.add(c)
        ordered.append(c)

    if expected_count and len(ordered) >= expected_count:
        ordered = ordered[:expected_count]

    return [{'ts_code': c, 'name': c} for c in ordered]

class CacheManager:
    """ç¼“å­˜ç®¡ç†å™¨ï¼ˆæ”¯æŒæœ¬åœ°CSVæŒä¹…åŒ–ï¼‰"""
    
    def __init__(self):
        self.stock_cache = {}
        self.params_cache = {}
        self.data_dir = os.path.join(DATA_ROOT, 'data')
        self._df_mem_cache = {}
        self._df_mem_cache_ttl_sec = int((os.getenv('QUANT_DF_CACHE_TTL_SEC') or '300').strip() or 300)
        self._df_mem_cache_max = int((os.getenv('QUANT_DF_CACHE_MAX') or '64').strip() or 64)
        self._no_update_until = {}
        self._no_update_ttl_sec = int((os.getenv('QUANT_NO_UPDATE_TTL_SEC') or '3600').strip() or 3600)
        if not os.path.exists(self.data_dir):
            try:
                os.makedirs(self.data_dir)
            except Exception as e:
                print(f"åˆ›å»ºæ•°æ®ç›®å½•å¤±è´¥: {e}")
    
    def get_stock_data(self, ts_code, start_date, end_date, freq='D'):
        """è·å–è‚¡ç¥¨æ•°æ®ï¼ˆä¼˜å…ˆè¯»å–æœ¬åœ°CSVï¼Œæ”¯æŒå¢é‡æ›´æ–°ï¼‰
        
        Args:
            ts_code: è‚¡ç¥¨ä»£ç 
            start_date: å¼€å§‹æ—¥æœŸ
            end_date: ç»“æŸæ—¥æœŸ
            freq: å‘¨æœŸï¼Œ'D'=æ—¥çº¿, 'W'=å‘¨çº¿, 'M'=æœˆçº¿
        """
        # ä¸åŒå‘¨æœŸä½¿ç”¨ä¸åŒçš„ç¼“å­˜æ–‡ä»¶
        freq_suffix = '' if freq == 'D' else f'_{freq}'
        file_path = os.path.join(self.data_dir, f"{ts_code}{freq_suffix}.csv")
        local_df = None
        mem_cache_key = (ts_code, freq)
        file_mtime = None
        try:
            if os.path.exists(file_path):
                file_mtime = os.path.getmtime(file_path)
        except Exception:
            file_mtime = None
        
        # 1. å°è¯•è¯»å–æœ¬åœ°æ–‡ä»¶
        mem_hit = False
        cached_entry = self._df_mem_cache.get(mem_cache_key)
        if cached_entry and cached_entry.get('mtime') == file_mtime:
            loaded_at = cached_entry.get('loaded_at') or 0
            if (time.time() - loaded_at) <= self._df_mem_cache_ttl_sec:
                df = cached_entry.get('df')
                if isinstance(df, pd.DataFrame) and not df.empty:
                    local_df = df
                    mem_hit = True

        if local_df is None and os.path.exists(file_path):
            try:
                # æŒ‡å®šdtypeä»¥é˜²æ­¢æ•°æ®ç±»å‹é”™è¯¯
                local_df = pd.read_csv(file_path, dtype={'date': str})
                if 'date' in local_df.columns:
                    local_df = local_df.sort_values('date').drop_duplicates(subset=['date']).reset_index(drop=True)
                else:
                    _log(f"[æ•°æ®æŸå] {file_path} ç¼ºå°‘dateåˆ—")
                    local_df = None
            except Exception as e:
                _log(f"[è¯»å–ç¼“å­˜å¤±è´¥] {file_path}: {e}")
                local_df = None

        if local_df is not None and not local_df.empty and not mem_hit:
            _inc_stat('cache_df_disk_reads', 1)
            try:
                if file_mtime is None and os.path.exists(file_path):
                    file_mtime = os.path.getmtime(file_path)
            except Exception:
                file_mtime = None
            self._df_mem_cache[mem_cache_key] = {'df': local_df, 'mtime': file_mtime, 'loaded_at': time.time()}
            if len(self._df_mem_cache) > self._df_mem_cache_max:
                try:
                    oldest_key = min(self._df_mem_cache.items(), key=lambda kv: kv[1].get('loaded_at') or 0)[0]
                    self._df_mem_cache.pop(oldest_key, None)
                except Exception:
                    pass
        
        # 2. æ£€æŸ¥å¹¶è¡¥å……æ•°æ®
        if local_df is not None and not local_df.empty:
            if mem_hit:
                _inc_stat('cache_df_mem_hits', 1)
            local_min = local_df['date'].min()
            local_max = local_df['date'].max()
            data_changed = False
            base_df = local_df
            
            # A. å‘å‰è¡¥å……ï¼ˆè¯·æ±‚å¼€å§‹æ—¶é—´æ—©äºæœ¬åœ°æœ€æ—©æ—¶é—´ï¼‰
            if start_date < local_min:
                pre_end_date = (datetime.strptime(local_min, '%Y-%m-%d') - timedelta(days=1)).strftime('%Y-%m-%d')
                if start_date <= pre_end_date:
                    now_ts = time.time()
                    if self._no_update_until.get((ts_code, freq, 'pre'), 0) > now_ts:
                        pre_data = None
                    else:
                        _log(f"[å‘å‰è¡¥å……] {ts_code} {freq}: {start_date} ~ {pre_end_date}")
                        pre_data = TushareDataFetcher.get_stock_data(ts_code, start_date, pre_end_date, freq)
                    if pre_data:
                        if local_df is base_df:
                            local_df = local_df.copy()
                        pre_df = pd.DataFrame(pre_data)
                        local_df = pd.concat([pre_df, local_df]).drop_duplicates(subset=['date']).sort_values('date').reset_index(drop=True)
                        data_changed = True
                    else:
                        self._no_update_until[(ts_code, freq, 'pre')] = time.time() + self._no_update_ttl_sec
            
            # B. å‘åè¡¥å……ï¼ˆè¯·æ±‚ç»“æŸæ—¶é—´æ™šäºæœ¬åœ°æœ€æ–°æ—¶é—´ï¼‰
            if end_date > local_max:
                inc_start_date = (datetime.strptime(local_max, '%Y-%m-%d') + timedelta(days=1)).strftime('%Y-%m-%d')
                if inc_start_date <= end_date:
                    now_ts = time.time()
                    if self._no_update_until.get((ts_code, freq, 'inc'), 0) > now_ts:
                        inc_data = None
                    else:
                        _log(f"[å¢é‡æ›´æ–°] {ts_code} {freq}: {inc_start_date} ~ {end_date}")
                        inc_data = TushareDataFetcher.get_stock_data(ts_code, inc_start_date, end_date, freq)
                    if inc_data:
                        if local_df is base_df:
                            local_df = local_df.copy()
                        inc_df = pd.DataFrame(inc_data)
                        local_df = pd.concat([local_df, inc_df]).drop_duplicates(subset=['date']).sort_values('date').reset_index(drop=True)
                        data_changed = True
                    else:
                        self._no_update_until[(ts_code, freq, 'inc')] = time.time() + self._no_update_ttl_sec
            
            # å¦‚æœæœ‰æ›´æ–°ï¼Œä¿å­˜å›æ–‡ä»¶
            if data_changed:
                try:
                    local_df.to_csv(file_path, index=False)
                    _log(f"[æŒä¹…åŒ–] å·²æ›´æ–° {file_path}, æ¡æ•°: {len(local_df)}")
                    _inc_stat('cache_csv_writes', 1)
                    try:
                        file_mtime = os.path.getmtime(file_path)
                    except Exception:
                        file_mtime = None
                    self._df_mem_cache[mem_cache_key] = {'df': local_df, 'mtime': file_mtime, 'loaded_at': time.time()}
                except Exception as e:
                    _log(f"[ä¿å­˜å¤±è´¥] {e}")

            # åªæœ‰æ—¥çº¿æ•°æ®æ‰è¡¥å…… pe/pb
            if freq == 'D':
                try:
                    need_fill = (
                        pro is not None and
                        (('pe' not in local_df.columns) or local_df['pe'].isna().all() or
                         ('pb' not in local_df.columns) or local_df['pb'].isna().all())
                    )
                    if need_fill:
                        df_basic = pro.daily_basic(
                            ts_code=ts_code,
                            start_date=start_date.replace('-', ''),
                            end_date=end_date.replace('-', ''),
                            fields='trade_date,pe,pb'
                        )
                        if df_basic is not None and not df_basic.empty:
                            if local_df is base_df:
                                local_df = local_df.copy()
                            df_basic = df_basic.rename(columns={'trade_date': 'date'})
                            df_basic['date'] = pd.to_datetime(df_basic['date']).dt.strftime('%Y-%m-%d')
                            if 'pe' not in local_df.columns:
                                local_df['pe'] = None
                            if 'pb' not in local_df.columns:
                                local_df['pb'] = None
                            merged = local_df.merge(df_basic[['date', 'pe', 'pb']], on='date', how='left', suffixes=('', '_new'))
                            if 'pe_new' in merged.columns:
                                merged['pe'] = merged['pe'].fillna(merged['pe_new'])
                                merged.drop(columns=['pe_new'], inplace=True)
                            if 'pb_new' in merged.columns:
                                merged['pb'] = merged['pb'].fillna(merged['pb_new'])
                                merged.drop(columns=['pb_new'], inplace=True)
                            local_df = merged
                            local_df.to_csv(file_path, index=False)
                            try:
                                file_mtime = os.path.getmtime(file_path)
                            except Exception:
                                file_mtime = None
                            self._df_mem_cache[mem_cache_key] = {'df': local_df, 'mtime': file_mtime, 'loaded_at': time.time()}
                except Exception as e:
                    _log(f"è¡¥é½pe/pbå¤±è´¥: {e}")

        else:
            # 3. æœ¬åœ°æ— æ•°æ®ï¼Œå…¨é‡è·å–
            _log(f"[æœ¬åœ°æ— ç¼“å­˜] å…¨é‡è·å– {ts_code} {freq}: {start_date} ~ {end_date}")
            data = TushareDataFetcher.get_stock_data(ts_code, start_date, end_date, freq)
            if data:
                local_df = pd.DataFrame(data)
                try:
                    local_df.to_csv(file_path, index=False)
                    _log(f"[æŒä¹…åŒ–] å·²ä¿å­˜ {file_path}, æ¡æ•°: {len(local_df)}")
                    _inc_stat('cache_csv_writes', 1)
                    try:
                        file_mtime = os.path.getmtime(file_path)
                    except Exception:
                        file_mtime = None
                    self._df_mem_cache[mem_cache_key] = {'df': local_df, 'mtime': file_mtime, 'loaded_at': time.time()}
                except Exception as e:
                    _log(f"[ä¿å­˜å¤±è´¥] {e}")
            else:
                # å°è¯•å¤‡ç”¨æ•°æ®æºï¼ˆä»…æ—¥çº¿ï¼‰
                if freq == 'D':
                    try:
                        _log("å°è¯•ä½¿ç”¨å¤‡ç”¨æ•°æ®æº(Eastmoney)...")
                        data = EastmoneyDataFetcher.get_stock_data(ts_code, start_date, end_date)
                        if data:
                            local_df = pd.DataFrame(data)
                            local_df.to_csv(file_path, index=False)
                            _log(f"[æŒä¹…åŒ–] (å¤‡ç”¨æº) å·²ä¿å­˜ {file_path}")
                            _inc_stat('cache_csv_writes', 1)
                            try:
                                file_mtime = os.path.getmtime(file_path)
                            except Exception:
                                file_mtime = None
                            self._df_mem_cache[mem_cache_key] = {'df': local_df, 'mtime': file_mtime, 'loaded_at': time.time()}
                    except Exception as e:
                        _log(f"å¤‡ç”¨æºè·å–å¤±è´¥: {e}")

        # 4. è¿”å›è¯·æ±‚åŒºé—´çš„æ•°æ®
        if local_df is not None and not local_df.empty:
            # è¿‡æ»¤æ—¶é—´åŒºé—´
            mask = (local_df['date'] >= start_date) & (local_df['date'] <= end_date)
            result_df = local_df.loc[mask]
            
            if result_df.empty:
                return []
            
            # è½¬æ¢ä¸ºå­—å…¸åˆ—è¡¨ï¼Œå¹¶å°†NaNè½¬ä¸ºNone
            return result_df.where(pd.notnull(result_df), None).to_dict('records')
            
        return []

    def get_latest_quote(self, ts_code, end_date, freq='D', cache_only=True):
        freq = (freq or 'D').upper()
        end_date = (end_date or '').strip() or datetime.now().strftime('%Y-%m-%d')
        freq_suffix = '' if freq == 'D' else f'_{freq}'
        file_path = os.path.join(self.data_dir, f"{ts_code}{freq_suffix}.csv")
        mem_cache_key = (ts_code, freq)

        local_df = None
        file_mtime = None
        try:
            if os.path.exists(file_path):
                file_mtime = os.path.getmtime(file_path)
        except Exception:
            file_mtime = None

        cached_entry = self._df_mem_cache.get(mem_cache_key)
        if cached_entry and cached_entry.get('mtime') == file_mtime:
            loaded_at = cached_entry.get('loaded_at') or 0
            if (time.time() - loaded_at) <= self._df_mem_cache_ttl_sec:
                df = cached_entry.get('df')
                if isinstance(df, pd.DataFrame) and not df.empty:
                    local_df = df
                    _inc_stat('cache_df_mem_hits', 1)

        if local_df is None and os.path.exists(file_path):
            try:
                local_df = pd.read_csv(file_path, dtype={'date': str})
                if 'date' in local_df.columns:
                    local_df = local_df.sort_values('date').drop_duplicates(subset=['date']).reset_index(drop=True)
                    self._df_mem_cache[mem_cache_key] = {'df': local_df, 'mtime': file_mtime, 'loaded_at': time.time()}
                    _inc_stat('cache_df_disk_reads', 1)
                else:
                    local_df = None
            except Exception:
                local_df = None

        # æ£€æŸ¥ç¼“å­˜æ•°æ®æ˜¯å¦åŒ…å«æœ€æ–°çš„end_dateæ•°æ®
        cache_up_to_date = False
        latest_date = None
        if local_df is not None and not local_df.empty and 'date' in local_df.columns:
            filtered = local_df.loc[local_df['date'] <= end_date]
            if not filtered.empty:
                latest_date = filtered.iloc[-1]['date']
                # å¦‚æœç¼“å­˜çš„æœ€æ–°æ—¥æœŸ >= è¯·æ±‚çš„end_dateï¼Œè¯´æ˜ç¼“å­˜æ˜¯æœ€æ–°çš„
                cache_up_to_date = latest_date >= end_date
        
        # å¦‚æœcache_only=Falseä¸”ç¼“å­˜ä¸æ˜¯æœ€æ–°çš„ï¼Œåˆ™ä»APIè·å–
        if not cache_only and not cache_up_to_date:
            # ç›´æ¥ä½¿ç”¨TushareDataFetcherè·å–æœ€æ–°æ•°æ®ï¼Œç»•è¿‡ç¼“å­˜
            # æ‰©å¤§æ—¥æœŸèŒƒå›´ä»¥ç¡®ä¿èƒ½è·å–åˆ°æ•°æ®ï¼ˆå¤„ç†å‘¨æœ«/èŠ‚å‡æ—¥æƒ…å†µï¼‰
            fetch_start = (datetime.strptime(end_date, '%Y-%m-%d') - timedelta(days=7)).strftime('%Y-%m-%d')
            data = TushareDataFetcher.get_stock_data(ts_code, fetch_start, end_date, freq)
            if isinstance(data, list) and data:
                # åˆå¹¶æ–°æ•°æ®åˆ°æœ¬åœ°ç¼“å­˜
                new_df = pd.DataFrame(data)
                if local_df is not None and not local_df.empty:
                    combined_df = pd.concat([local_df, new_df]).drop_duplicates(subset=['date']).sort_values('date').reset_index(drop=True)
                else:
                    combined_df = new_df
                # ä¿å­˜åˆ°æ–‡ä»¶
                try:
                    combined_df.to_csv(file_path, index=False)
                    _inc_stat('cache_csv_writes', 1)
                    try:
                        file_mtime = os.path.getmtime(file_path)
                    except Exception:
                        file_mtime = None
                    self._df_mem_cache[mem_cache_key] = {'df': combined_df, 'mtime': file_mtime, 'loaded_at': time.time()}
                except Exception:
                    pass
                return data[-1]
            # APIè·å–å¤±è´¥ï¼Œå›é€€åˆ°ç¼“å­˜æ•°æ®
            if local_df is None or local_df.empty:
                return None

        if local_df is None or local_df.empty or 'date' not in local_df.columns:
            return None

        filtered = local_df.loc[local_df['date'] <= end_date]
        if filtered.empty:
            return None
        row = filtered.iloc[-1].to_dict()
        for k, v in list(row.items()):
            try:
                if pd.isna(v):
                    row[k] = None
            except Exception:
                pass
        return row
    
    def get(self, cache_type, key):
        """è·å–ç¼“å­˜"""
        if cache_type == 'params':
            return self.params_cache.get(key)
        return None
    
    def set(self, cache_type, key, data):
        """è®¾ç½®ç¼“å­˜"""
        if cache_type == 'params':
            self.params_cache[key] = {
                'meta': {
                    'cached_at': datetime.now().isoformat()
                },
                'data': data
            }

# åˆå§‹åŒ–ç¼“å­˜ç®¡ç†å™¨
cache_manager = CacheManager()

class TushareDataFetcher:
    """Tushareæ•°æ®è·å–ç±»"""
    
    @staticmethod
    def get_stock_data(ts_code, start_date, end_date, freq='D'):
        """è·å–è‚¡ç¥¨æ•°æ®ï¼Œæ”¯æŒæ—¥çº¿/å‘¨çº¿/æœˆçº¿
        
        Args:
            ts_code: è‚¡ç¥¨ä»£ç 
            start_date: å¼€å§‹æ—¥æœŸ
            end_date: ç»“æŸæ—¥æœŸ
            freq: å‘¨æœŸï¼Œ'D'=æ—¥çº¿, 'W'=å‘¨çº¿, 'M'=æœˆçº¿
        """
        if pro is None:
            _log("[ERROR] Tushare APIæœªåˆå§‹åŒ–")
            return None
            
        try:
            _inc_stat('tushare_fetch_calls', 1)
            _log(f"æ­£åœ¨è·å– {ts_code} ä» {start_date} åˆ° {end_date} çš„{freq}çº¿æ•°æ®...")
            
            # æ ¹æ®å‘¨æœŸé€‰æ‹©æ¥å£
            if freq == 'W':
                df = pro.weekly(
                    ts_code=ts_code,
                    start_date=start_date.replace('-', ''),
                    end_date=end_date.replace('-', '')
                )
            elif freq == 'M':
                df = pro.monthly(
                    ts_code=ts_code,
                    start_date=start_date.replace('-', ''),
                    end_date=end_date.replace('-', '')
                )
            else:  # é»˜è®¤æ—¥çº¿
                df = pro.daily(
                    ts_code=ts_code,
                    start_date=start_date.replace('-', ''),
                    end_date=end_date.replace('-', '')
                )
            
            _log(f"è·å–åˆ°åŸå§‹æ•°æ®: {len(df) if df is not None else 'None'} æ¡")
            
            if df is None or df.empty:
                _log("æ•°æ®ä¸ºç©ºæˆ–None")
                return None
            
            # æŒ‰æ—¥æœŸå‡åºæ’åˆ—
            df = df.sort_values('trade_date').reset_index(drop=True)
            
            # é‡å‘½ååˆ—
            df = df.rename(columns={
                'trade_date': 'date',
                'vol': 'volume',
                'pct_chg': 'pctChange',
                'change': 'priceChange'
            })
            
            # æ ¼å¼åŒ–æ—¥æœŸ
            df['date'] = pd.to_datetime(df['date']).dt.strftime('%Y-%m-%d')
            
            # è·å–æ¢æ‰‹ç‡æ•°æ®ï¼ˆå¯é€‰ï¼Œå› ä¸ºå¯èƒ½éœ€è¦æƒé™ï¼‰
            try:
                df_basic = pro.daily_basic(
                    ts_code=ts_code,
                    start_date=start_date.replace('-', ''),
                    end_date=end_date.replace('-', ''),
                    fields='trade_date,turnover_rate,pe,pb'
                )
                
                if df_basic is not None and not df_basic.empty:
                    df_basic = df_basic.rename(columns={
                        'trade_date': 'date',
                        'turnover_rate': 'turnover'
                    })
                    df_basic['date'] = pd.to_datetime(df_basic['date']).dt.strftime('%Y-%m-%d')
                    df = df.merge(df_basic[['date', 'turnover', 'pe', 'pb']], on='date', how='left')
            except Exception as e:
                _log(f"è·å–æ¢æ‰‹ç‡æ•°æ®å¤±è´¥ï¼ˆå¯èƒ½éœ€è¦æ›´é«˜æƒé™ï¼‰: {e}")
                # æ·»åŠ ç©ºåˆ—ä½œä¸ºå ä½ç¬¦
                df['turnover'] = None
                df['pe'] = None
                df['pb'] = None

            if 'turnover' not in df.columns or df['turnover'].isna().all():
                float_share = None
                try:
                    df_share = pro.stock_basic(ts_code=ts_code, fields='ts_code,float_share')
                    if df_share is not None and not df_share.empty:
                        float_share = df_share.iloc[0].get('float_share')
                except Exception:
                    float_share = None
                
                if float_share and isinstance(float_share, (int, float)) and float_share > 0:
                    turnover_est = df['volume'] / (float_share * 100)
                    df['turnover'] = turnover_est.clip(lower=0, upper=100)

            try:
                if 'turnover' not in df.columns or df['turnover'].isna().all():
                    em = EastmoneyDataFetcher.get_stock_data(ts_code, start_date, end_date)
                    em_by_date = {row.get('date'): row for row in (em or []) if isinstance(row, dict) and row.get('date')}
                    if em_by_date:
                        df['turnover'] = df['date'].map(lambda d: em_by_date.get(d, {}).get('turnover'))
                        if 'amount' in df.columns:
                            df['amount'] = df['amount'].fillna(df['date'].map(lambda d: em_by_date.get(d, {}).get('amount')))
            except Exception as e:
                print(f"ä»ä¸œæ–¹è´¢å¯Œè¡¥å…¨æ¢æ‰‹ç‡å¤±è´¥: {e}")

            try:
                df_adj = pro.adj_factor(
                    ts_code=ts_code,
                    start_date=start_date.replace('-', ''),
                    end_date=end_date.replace('-', '')
                )
                if df_adj is not None and not df_adj.empty:
                    df_adj = df_adj.rename(columns={'trade_date': 'date'})
                    df_adj['date'] = pd.to_datetime(df_adj['date']).dt.strftime('%Y-%m-%d')
                    df_adj = df_adj.sort_values('date').reset_index(drop=True)
                    latest_adj = df_adj['adj_factor'].iloc[-1]
                    df = df.merge(df_adj[['date', 'adj_factor']], on='date', how='left')
                    df['adj_factor'] = df['adj_factor'].ffill()
                    if pd.notna(latest_adj) and latest_adj > 0:
                        factor = df['adj_factor'] / latest_adj
                        for col in ['open', 'high', 'low', 'close']:
                            if col in df.columns:
                                df[col] = df[col] * factor
            except Exception as e:
                print(f"è·å–å¤æƒå› å­å¤±è´¥: {e}")
            
            return df.to_dict('records')
            
        except Exception as e:
            print(f"è·å–æ•°æ®é”™è¯¯: {e}")
            return None
    
    @staticmethod
    def get_stock_list():
        """è·å–è‚¡ç¥¨åˆ—è¡¨"""
        if pro is None:
            print("[ERROR] Tushare APIæœªåˆå§‹åŒ–ï¼Œä½¿ç”¨é»˜è®¤è‚¡ç¥¨åˆ—è¡¨")
            return [
                {'ts_code': '000001.SZ', 'symbol': '000001', 'name': 'å¹³å®‰é“¶è¡Œ', 'industry': 'é“¶è¡Œ', 'market': 'ä¸»æ¿'},
                {'ts_code': '000002.SZ', 'symbol': '000002', 'name': 'ä¸‡ç§‘A', 'industry': 'æˆ¿åœ°äº§', 'market': 'ä¸»æ¿'},
                {'ts_code': '600519.SH', 'symbol': '600519', 'name': 'è´µå·èŒ…å°', 'industry': 'ç™½é…’', 'market': 'ä¸»æ¿'},
                {'ts_code': '000858.SZ', 'symbol': '000858', 'name': 'äº”ç²®æ¶²', 'industry': 'ç™½é…’', 'market': 'ä¸»æ¿'},
                {'ts_code': '601318.SH', 'symbol': '601318', 'name': 'ä¸­å›½å¹³å®‰', 'industry': 'ä¿é™©', 'market': 'ä¸»æ¿'}
            ]
        try:
            df = pro.stock_basic(
                exchange='',
                list_status='L',
                fields='ts_code,symbol,name,industry,market'
            )
            return df.to_dict('records') if df is not None else []
        except Exception as e:
            print(f"è·å–è‚¡ç¥¨åˆ—è¡¨é”™è¯¯: {e}")
            return [
                {'ts_code': '000001.SZ', 'symbol': '000001', 'name': 'å¹³å®‰é“¶è¡Œ', 'industry': 'é“¶è¡Œ', 'market': 'ä¸»æ¿'},
                {'ts_code': '000002.SZ', 'symbol': '000002', 'name': 'ä¸‡ç§‘A', 'industry': 'æˆ¿åœ°äº§', 'market': 'ä¸»æ¿'},
                {'ts_code': '600519.SH', 'symbol': '600519', 'name': 'è´µå·èŒ…å°', 'industry': 'ç™½é…’', 'market': 'ä¸»æ¿'},
                {'ts_code': '000858.SZ', 'symbol': '000858', 'name': 'äº”ç²®æ¶²', 'industry': 'ç™½é…’', 'market': 'ä¸»æ¿'},
                {'ts_code': '601318.SH', 'symbol': '601318', 'name': 'ä¸­å›½å¹³å®‰', 'industry': 'ä¿é™©', 'market': 'ä¸»æ¿'},
                {'ts_code': '600036.SH', 'symbol': '600036', 'name': 'æ‹›å•†é“¶è¡Œ', 'industry': 'é“¶è¡Œ', 'market': 'ä¸»æ¿'},
                {'ts_code': '000333.SZ', 'symbol': '000333', 'name': 'ç¾çš„é›†å›¢', 'industry': 'å®¶ç”µ', 'market': 'ä¸»æ¿'},
                {'ts_code': '002415.SZ', 'symbol': '002415', 'name': 'æµ·åº·å¨è§†', 'industry': 'è½¯ä»¶æœåŠ¡', 'market': 'ä¸­å°æ¿'},
                {'ts_code': '300750.SZ', 'symbol': '300750', 'name': 'å®å¾·æ—¶ä»£', 'industry': 'ç”µæ°”è®¾å¤‡', 'market': 'åˆ›ä¸šæ¿'},
                {'ts_code': '600309.SH', 'symbol': '600309', 'name': 'ä¸‡ååŒ–å­¦', 'industry': 'åŒ–å·¥', 'market': 'ä¸»æ¿'}
            ]
    
    @staticmethod
    def get_index_weights(index_code='000300.SH'):
        """è·å–æŒ‡æ•°æˆåˆ†è‚¡"""
        file_cache = _load_index_cache_file()

        cached_item = file_cache.get(index_code)
        if isinstance(cached_item, dict) and isinstance(cached_item.get('data'), list):
            try:
                cached_at = datetime.fromisoformat(cached_item.get('cached_at'))
                if (datetime.now() - cached_at).days < 30:
                    return cached_item['data']
            except Exception:
                pass

        if pro is None:
            data = _fetch_index_constituents_from_legulegu(index_code)
            if not data:
                data = _fetch_index_constituents_from_sina(index_code)
            if data:
                file_cache[index_code] = {'cached_at': datetime.now().isoformat(), 'data': data}
                _save_index_cache_file(file_cache)
            return data
        try:
            # è·å–æŒ‡æ•°æˆåˆ†è‚¡
            df = pro.index_weight(index_code=index_code, start_date='20230101', end_date=datetime.now().strftime('%Y%m%d'))
            if df is None or df.empty:
                return []
            
            # åªå–æœ€æ–°çš„æˆåˆ†è‚¡
            latest_date = df['trade_date'].max()
            df = df[df['trade_date'] == latest_date]
            
            # è·å–è‚¡ç¥¨åç§°
            codes = df['con_code'].tolist()
            # åˆ†æ‰¹è·å–åç§°
            all_stocks = []
            for i in range(0, len(codes), 100):
                batch_codes = codes[i:i+100]
                df_basic = pro.stock_basic(ts_code=','.join(batch_codes), fields='ts_code,symbol,name,industry')
                if df_basic is not None:
                    all_stocks.extend(df_basic.to_dict('records'))
            
            return all_stocks
        except Exception as e:
            print(f"è·å–æŒ‡æ•°æˆåˆ†è‚¡é”™è¯¯: {e}")
            data = _fetch_index_constituents_from_legulegu(index_code)
            if not data:
                data = _fetch_index_constituents_from_sina(index_code)
            if data:
                file_cache[index_code] = {'cached_at': datetime.now().isoformat(), 'data': data}
                _save_index_cache_file(file_cache)
                return data

            if isinstance(cached_item, dict) and isinstance(cached_item.get('data'), list):
                return cached_item['data']

            return []

    @staticmethod
    def search_stock(keyword):
        """æœç´¢è‚¡ç¥¨"""
        try:
            df = pro.stock_basic(
                exchange='',
                list_status='L',
                fields='ts_code,symbol,name,industry'
            )
            # æ¨¡ç³Šæœç´¢
            mask = (
                df['ts_code'].str.contains(keyword, case=False, na=False) |
                df['name'].str.contains(keyword, case=False, na=False) |
                df['symbol'].str.contains(keyword, case=False, na=False)
            )
            return df[mask].head(20).to_dict('records')
        except Exception as e:
            print(f"æœç´¢è‚¡ç¥¨é”™è¯¯: {e}")
            return []

    @staticmethod
    def gen_mock_data(ts_code, start_date, end_date):
        """ç”Ÿæˆæ¨¡æ‹Ÿè‚¡ç¥¨æ•°æ®"""
        print(f"æ­£åœ¨ç”Ÿæˆæ¨¡æ‹Ÿæ•°æ® {ts_code} ä» {start_date} åˆ° {end_date}")
        
        # è§£ææ—¥æœŸ
        start = datetime.strptime(start_date, '%Y-%m-%d')
        end = datetime.strptime(end_date, '%Y-%m-%d')
        
        # ç”Ÿæˆæ—¥æœŸåºåˆ—ï¼ˆåªåŒ…å«å·¥ä½œæ—¥ï¼‰
        dates = []
        current_date = start
        while current_date <= end:
            if current_date.weekday() < 5:  # åªåŒ…å«å·¥ä½œæ—¥
                dates.append(current_date.strftime('%Y-%m-%d'))
            current_date += timedelta(days=1)
        
        if not dates:
            print("âŒ æ²¡æœ‰ç”Ÿæˆä»»ä½•æ—¥æœŸ")
            return []
        
        print(f"ğŸ“… ç”Ÿæˆäº† {len(dates)} ä¸ªäº¤æ˜“æ—¥")
        
        # ç”Ÿæˆæ¨¡æ‹Ÿæ•°æ®
        data = []
        base_price = 10 + np.random.random() * 40  # éšæœºåŸºç¡€ä»·æ ¼ 10-50
        
        for i, date in enumerate(dates):
            # ä»·æ ¼æ³¢åŠ¨
            daily_change = (np.random.random() - 0.48) * 0.1  # ç•¥å¾®ä¸Šæ¶¨è¶‹åŠ¿
            if i == 0:
                open_price = base_price
            else:
                prev_close = data[i-1]['close']
                open_price = prev_close * (1 + np.random.random() * 0.02 - 0.01)  # Â±1% å¼€ç›˜æ³¢åŠ¨
            
            close_price = open_price * (1 + daily_change)
            
            # ç¡®å®šå½“æ—¥ä»·æ ¼èŒƒå›´ï¼ŒåŸºäºå¼€ç›˜ä»·å’Œæ”¶ç›˜ä»·
            higher_price = max(open_price, close_price)
            lower_price = min(open_price, close_price)
            
            # è®¡ç®—æœ€é«˜ä»·å’Œæœ€ä½ä»·
            high_price = higher_price * (1 + np.random.random() * 0.03)  # æœ€é«˜ä»·æ ¼
            low_price = lower_price * (1 - np.random.random() * 0.03)   # æœ€ä½ä»·æ ¼
            
            # ç¡®ä¿é«˜ä½ä»·æ ¼åˆç†
            high_price = max(high_price, higher_price)
            low_price = min(low_price, lower_price)
            low_price = max(0, low_price)  # ç¡®ä¿æœ€ä½ä»·éè´Ÿ
            
            volume = int(np.random.random() * 10000000) + 1000000  # 100ä¸‡åˆ°1100ä¸‡è‚¡
            
            row = {
                'date': date,
                'open': round(open_price, 2),
                'high': round(high_price, 2),
                'low': round(low_price, 2),
                'close': round(close_price, 2),
                'volume': volume,
                'pctChange': round((close_price - open_price) / open_price * 100, 2),
                'priceChange': round(close_price - open_price, 2),
                'turnover': round(np.random.random() * 5, 2),  # æ¢æ‰‹ç‡ 0-5%
                'pe': round(15 + np.random.random() * 20, 2),  # å¸‚ç›ˆç‡ 15-35
                'pb': round(1 + np.random.random() * 3, 2)     # å¸‚å‡€ç‡ 1-4
            }
            
            data.append(row)
        
        print(f"ç”Ÿæˆäº† {len(data)} æ¡æ¨¡æ‹Ÿæ•°æ®")
        return data


def _eastmoney_secid(ts_code):
    if not ts_code:
        return None
    s = str(ts_code).strip().upper()
    if '.' in s:
        symbol, exch = s.split('.', 1)
    else:
        symbol = re.sub(r"\D", "", s)
        exch = 'SH' if symbol.startswith('6') else 'SZ'
    symbol = re.sub(r"\D", "", symbol)
    if not re.fullmatch(r"\d{6}", symbol):
        return None
    market = '1' if exch == 'SH' else '0'
    return f"{market}.{symbol}"


class EastmoneyDataFetcher:
    @staticmethod
    def get_stock_data(ts_code, start_date, end_date):
        secid = _eastmoney_secid(ts_code)
        if not secid:
            return None

        beg = start_date.replace('-', '')
        end = end_date.replace('-', '')
        params = {
            'secid': secid,
            'klt': 101,
            'fqt': 1,
            'beg': beg,
            'end': end,
            'fields1': 'f1,f2,f3,f4,f5,f6',
            'fields2': 'f51,f52,f53,f54,f55,f56,f57,f58,f59,f60,f61'
        }
        url = 'https://push2his.eastmoney.com/api/qt/stock/kline/get?' + urllib.parse.urlencode(params)
        req = urllib.request.Request(url, headers={'User-Agent': 'Mozilla/5.0', 'Referer': 'https://quote.eastmoney.com/'})
        with urllib.request.urlopen(req, timeout=12) as resp:
            payload = json.loads(resp.read().decode('utf-8'))

        data = (payload or {}).get('data') or {}
        klines = data.get('klines') or []
        out = []
        for row in klines:
            if not isinstance(row, str):
                continue
            parts = row.split(',')
            if len(parts) < 11:
                continue
            date = parts[0]
            try:
                out.append({
                    'date': date,
                    'open': float(parts[1]),
                    'close': float(parts[2]),
                    'high': float(parts[3]),
                    'low': float(parts[4]),
                    'volume': float(parts[5]),
                    'amount': float(parts[6]),
                    'pctChange': float(parts[8]),
                    'priceChange': float(parts[9]),
                    'turnover': float(parts[10]),
                    'ts_code': ts_code
                })
            except Exception:
                continue
        return out


class TechnicalIndicators:
    """æŠ€æœ¯æŒ‡æ ‡è®¡ç®—ç±»"""
    
    @staticmethod
    def calculate_all(data):
        """è®¡ç®—æ‰€æœ‰æŠ€æœ¯æŒ‡æ ‡"""
        df = pd.DataFrame(data)
        
        # ç§»åŠ¨å¹³å‡çº¿
        for period in [5, 10, 20, 60]:
            df[f'ma{period}'] = df['close'].rolling(window=period).mean()
        
        # æˆäº¤é‡å‡çº¿
        for period in [5, 10, 20]:
            df[f'volMa{period}'] = df['volume'].rolling(window=period).mean()
        
        # RSI
        delta = df['close'].diff()
        gain = (delta.where(delta > 0, 0)).rolling(window=14).mean()
        loss = (-delta.where(delta < 0, 0)).rolling(window=14).mean()
        rs = gain / loss
        df['rsi'] = 100 - (100 / (1 + rs))
        
        # MACD
        exp1 = df['close'].ewm(span=12, adjust=False).mean()
        exp2 = df['close'].ewm(span=26, adjust=False).mean()
        df['dif'] = exp1 - exp2
        df['dea'] = df['dif'].ewm(span=9, adjust=False).mean()
        df['macd'] = (df['dif'] - df['dea']) * 2
        
        # å¸ƒæ—å¸¦
        df['bollMid'] = df['close'].rolling(window=20).mean()
        df['bollStd'] = df['close'].rolling(window=20).std()
        df['bollUp'] = df['bollMid'] + 2 * df['bollStd']
        df['bollDown'] = df['bollMid'] - 2 * df['bollStd']
        
        # ATR
        df['tr'] = np.maximum(
            df['high'] - df['low'],
            np.maximum(
                abs(df['high'] - df['close'].shift(1)),
                abs(df['low'] - df['close'].shift(1))
            )
        )
        df['atr'] = df['tr'].rolling(window=20).mean()
        
        # Næ—¥æœ€é«˜æœ€ä½
        for period in [10, 20]:
            df[f'high{period}'] = df['high'].rolling(window=period).max()
            df[f'low{period}'] = df['low'].rolling(window=period).min()
        
        # KDJ
        low_min = df['low'].rolling(window=9).min()
        high_max = df['high'].rolling(window=9).max()
        rsv = (df['close'] - low_min) / (high_max - low_min) * 100
        df['k'] = rsv.ewm(com=2, adjust=False).mean()
        df['d'] = df['k'].ewm(com=2, adjust=False).mean()
        df['j'] = 3 * df['k'] - 2 * df['d']
        
        # æ›¿æ¢NaNä¸ºNone
        df = df.replace({np.nan: None})
        
        return df.to_dict('records')


class CrossStockParamOptimizer:
    """è·¨è‚¡ç¥¨å‚æ•°ä¼˜åŒ–å™¨ - è§£å†³å•è‚¡ç¥¨å‚æ•°å±€é™æ€§é—®é¢˜"""
    
    # é¢„è®¾çš„é€šç”¨å‚æ•°æ¨¡æ¿ï¼ˆåŸºäºå¤§é‡è‚¡ç¥¨å›æµ‹ä¼˜åŒ–å¾—å‡ºï¼‰
    UNIVERSAL_PARAMS = {
        'deep_fusion': {
            'mlScoreThreshold': 55,
            'minConfirmations': 3,
            'volumeMulti': 1.5,
            'rsiThreshold': 35,
            'macdThreshold': 0,
            'useMa5AboveMa20': True,
            'usePriceAboveMa5': True,
            'useMacdRising': True,
            'useRsiBelow35': True,
            'useVolumeAboveMa': True,
            'usePriceBelowBoll': True,
            'useAdxConfirm': True,
            'useMomentumConfirm': True
        },
        'volume_breakout': {
            'breakoutPeriod': 20,
            'volumeMulti': 2.0,
            'rsiMax': 70,
            'minConditions': 3,
            'scoreThreshold': 70,
            'usePriceBreakout': True,
            'useVolumeUp': True,
            'useTrendConfirm': True,
            'useVolatilityConfirm': True,
            'useMomentumConfirm': True,
            'useBullishCandle': True,
            'useChipConfirm': True
        },
        'oversold_rebound': {
            'rsiOversold': 30,
            'bollingerOffset': 0.02,
            'nearLowRatio': 1.03,
            'minConditions': 3,
            'scoreThreshold': 65,
            'useRsiOversold': True,
            'useTouchLowerBoll': True,
            'useNearLow': True,
            'useVolumePattern': True,
            'useCandlePattern': True,
            'useMacdDivergence': True,
            'useTrendFilter': True
        },
        'trend_enhanced': {
            'macdThreshold': 0,
            'minConditions': 3,
            'scoreThreshold': 70,
            'useShortMaAlignment': True,
            'usePriceAboveMa': True,
            'useMacdPositive': True,
            'useAdxConfirm': True,
            'useVolumeConfirm': True,
            'useBollConfirm': True,
            'useMomentumConfirm': True,
            'useTrendPersistence': True
        },
        'macd_divergence': {
            'lookbackPeriod': 30,
            'minConditions': 2,
            'scoreThreshold': 60,
            'useBullishDivergence': True,
            'useHistogramDivergence': True,
            'useVolumeDivergence': True,
            'useRsiDivergence': True,
            'useTrendFilter': True,
            'useCandleConfirm': True,
            'useMaConfirm': True
        },
        'bollinger_extreme': {
            'bollDeviation': 2.0,
            'rsiOversold': 30,
            'minConditions': 2,
            'scoreThreshold': 60,
            'useLowerBandTouch': True,
            'useBandWidthConfirm': True,
            'useRsiConfirm': True,
            'useVolumeConfirm': True,
            'useMaSupport': True,
            'useCandleConfirm': True,
            'useVolatilityAdjust': True
        },
        'momentum_rotation': {
            'shortMomentumPeriod': 10,
            'shortMomentumThreshold': 3.0,
            'mediumMomentumPeriod': 20,
            'mediumMomentumThreshold': 5.0,
            'longMomentumPeriod': 60,
            'volumeMulti': 1.5,
            'minConditions': 3,
            'scoreThreshold': 65,
            'useShortMomentum': True,
            'useMediumMomentum': True,
            'useLongMomentum': True,
            'useRelativeStrength': True,
            'useVolumeConfirm': True,
            'useMomentumQuality': True,
            'useVolatilityAdjust': True,
            'useMacdConfirm': True
        },
        'turtle_enhanced': {
            'entryPeriod': 20,
            'exitPeriod': 10,
            'atrMultiplier': 2.0,
            'volumeMulti': 1.5,
            'minConditions': 3,
            'scoreThreshold': 65,
            'useBreakoutEntry': True,
            'useTrendFilter': True,
            'useVolatilityFilter': True,
            'useVolumeConfirm': True,
            'useAdxConfirm': True,
            'useFalseBreakoutFilter': True,
            'useRiskManagement': True,
            'useSentimentFilter': True
        }
    }
    
    # è¡Œä¸šç‰¹å®šå‚æ•°è°ƒæ•´
    INDUSTRY_ADJUSTMENTS = {
        'é“¶è¡Œ': {'rsiThreshold': 40, 'volumeMulti': 1.2, 'atr_multiplier': 0.8},
        'ä¿é™©': {'rsiThreshold': 38, 'volumeMulti': 1.3, 'atr_multiplier': 0.9},
        'æˆ¿åœ°äº§': {'rsiThreshold': 35, 'volumeMulti': 1.4, 'atr_multiplier': 1.1},
        'ç™½é…’': {'rsiThreshold': 35, 'volumeMulti': 1.5, 'atr_multiplier': 1.2},
        'åŒ»è¯': {'rsiThreshold': 33, 'volumeMulti': 1.6, 'atr_multiplier': 1.3},
        'ç§‘æŠ€': {'rsiThreshold': 30, 'volumeMulti': 1.8, 'atr_multiplier': 1.5},
        'åŠå¯¼ä½“': {'rsiThreshold': 30, 'volumeMulti': 2.0, 'atr_multiplier': 1.6},
        'æ–°èƒ½æº': {'rsiThreshold': 32, 'volumeMulti': 1.8, 'atr_multiplier': 1.5},
        'åŒ–å·¥': {'rsiThreshold': 35, 'volumeMulti': 1.4, 'atr_multiplier': 1.1},
        'æœºæ¢°': {'rsiThreshold': 35, 'volumeMulti': 1.4, 'atr_multiplier': 1.2},
        'ç”µå­': {'rsiThreshold': 32, 'volumeMulti': 1.7, 'atr_multiplier': 1.4},
        'é€šä¿¡': {'rsiThreshold': 32, 'volumeMulti': 1.7, 'atr_multiplier': 1.4},
        'ä¼ åª’': {'rsiThreshold': 30, 'volumeMulti': 1.8, 'atr_multiplier': 1.5},
        'æ±½è½¦': {'rsiThreshold': 33, 'volumeMulti': 1.6, 'atr_multiplier': 1.3},
        'å®¶ç”µ': {'rsiThreshold': 35, 'volumeMulti': 1.4, 'atr_multiplier': 1.1},
        'é£Ÿå“': {'rsiThreshold': 36, 'volumeMulti': 1.3, 'atr_multiplier': 1.0},
        'æœè£…': {'rsiThreshold': 35, 'volumeMulti': 1.4, 'atr_multiplier': 1.2},
        'å»ºæ': {'rsiThreshold': 35, 'volumeMulti': 1.4, 'atr_multiplier': 1.2},
        'æœ‰è‰²': {'rsiThreshold': 32, 'volumeMulti': 1.7, 'atr_multiplier': 1.4},
        'é’¢é“': {'rsiThreshold': 35, 'volumeMulti': 1.4, 'atr_multiplier': 1.1},
        'ç…¤ç‚­': {'rsiThreshold': 35, 'volumeMulti': 1.4, 'atr_multiplier': 1.1},
        'çŸ³æ²¹': {'rsiThreshold': 36, 'volumeMulti': 1.3, 'atr_multiplier': 1.0},
        'ç”µåŠ›': {'rsiThreshold': 38, 'volumeMulti': 1.2, 'atr_multiplier': 0.9},
        'äº¤é€šè¿è¾“': {'rsiThreshold': 37, 'volumeMulti': 1.3, 'atr_multiplier': 0.9},
        'å»ºç­‘': {'rsiThreshold': 36, 'volumeMulti': 1.3, 'atr_multiplier': 1.0},
        'å†œæ—ç‰§æ¸”': {'rsiThreshold': 33, 'volumeMulti': 1.6, 'atr_multiplier': 1.3},
        'å•†è´¸é›¶å”®': {'rsiThreshold': 35, 'volumeMulti': 1.4, 'atr_multiplier': 1.1},
        'ç¤¾ä¼šæœåŠ¡': {'rsiThreshold': 33, 'volumeMulti': 1.6, 'atr_multiplier': 1.3},
        'è®¡ç®—æœº': {'rsiThreshold': 30, 'volumeMulti': 1.9, 'atr_multiplier': 1.6},
        'å›½é˜²å†›å·¥': {'rsiThreshold': 30, 'volumeMulti': 1.9, 'atr_multiplier': 1.6},
    }
    
    @classmethod
    def get_optimized_params(cls, strategy_name, stock_features=None, industry=None):
        """
        è·å–ä¼˜åŒ–åçš„å‚æ•°
        
        Args:
            strategy_name: ç­–ç•¥åç§°
            stock_features: è‚¡ç¥¨ç‰¹å¾å­—å…¸
            industry: è¡Œä¸šåç§°
        
        Returns:
            ä¼˜åŒ–åçš„å‚æ•°å­—å…¸
        """
        # è·å–åŸºç¡€é€šç”¨å‚æ•°
        params = cls.UNIVERSAL_PARAMS.get(strategy_name, {}).copy()
        
        # åº”ç”¨è¡Œä¸šç‰¹å®šè°ƒæ•´
        if industry and industry in cls.INDUSTRY_ADJUSTMENTS:
            industry_adj = cls.INDUSTRY_ADJUSTMENTS[industry]
            for key, value in industry_adj.items():
                if key in params:
                    params[key] = value
        
        # åº”ç”¨è‚¡ç¥¨ç‰¹å¾è‡ªé€‚åº”è°ƒæ•´
        if stock_features:
            params = cls._apply_feature_adjustments(params, stock_features)
        
        return params
    
    @classmethod
    def _apply_feature_adjustments(cls, params, stock_features):
        """æ ¹æ®è‚¡ç¥¨ç‰¹å¾è°ƒæ•´å‚æ•°"""
        # æ³¢åŠ¨ç‡è°ƒæ•´
        volatility = stock_features.get('volatility', 'medium')
        volatility_multipliers = {
            'very_high': {'stopLoss': 1.5, 'takeProfit': 1.3, 'rsiThreshold': 1.15, 'volumeMulti': 0.8},
            'high': {'stopLoss': 1.3, 'takeProfit': 1.2, 'rsiThreshold': 1.08, 'volumeMulti': 0.9},
            'medium': {'stopLoss': 1.0, 'takeProfit': 1.0, 'rsiThreshold': 1.0, 'volumeMulti': 1.0},
            'low': {'stopLoss': 0.8, 'takeProfit': 0.9, 'rsiThreshold': 0.92, 'volumeMulti': 1.1},
            'very_low': {'stopLoss': 0.7, 'takeProfit': 0.8, 'rsiThreshold': 0.85, 'volumeMulti': 1.2}
        }
        
        vm = volatility_multipliers.get(volatility, volatility_multipliers['medium'])
        
        # åº”ç”¨æ³¢åŠ¨ç‡è°ƒæ•´
        for param_key, multiplier in vm.items():
            if param_key in params and isinstance(params[param_key], (int, float)):
                params[param_key] = params[param_key] * multiplier
        
        # æµåŠ¨æ€§è°ƒæ•´
        liquidity = stock_features.get('liquidity', 'medium')
        liquidity_adjustments = {
            'very_high': {'scoreThreshold': -5, 'minConditions': -1},
            'high': {'scoreThreshold': -3, 'minConditions': 0},
            'medium': {'scoreThreshold': 0, 'minConditions': 0},
            'low': {'scoreThreshold': 5, 'minConditions': 1},
            'very_low': {'scoreThreshold': 10, 'minConditions': 1}
        }
        
        la = liquidity_adjustments.get(liquidity, liquidity_adjustments['medium'])
        
        for param_key, adjustment in la.items():
            if param_key in params:
                params[param_key] = params[param_key] + adjustment
        
        # è¶‹åŠ¿ç±»å‹è°ƒæ•´
        trend_type = stock_features.get('trend_type', 'neutral')
        trend_adjustments = {
            'strong_uptrend': {'macdThreshold': 0.3, 'scoreThreshold': 5},
            'weak_uptrend': {'macdThreshold': 0.1, 'scoreThreshold': 0},
            'sideways': {'macdThreshold': 0, 'scoreThreshold': 0},
            'weak_downtrend': {'macdThreshold': -0.1, 'scoreThreshold': -5},
            'strong_downtrend': {'macdThreshold': -0.3, 'scoreThreshold': -10},
            'neutral': {'macdThreshold': 0, 'scoreThreshold': 0}
        }
        
        ta = trend_adjustments.get(trend_type, trend_adjustments['neutral'])
        
        for param_key, adjustment in ta.items():
            if param_key in params:
                params[param_key] = params[param_key] + adjustment
        
        return params
    
    @classmethod
    def validate_params(cls, strategy_name, params):
        """éªŒè¯å‚æ•°æœ‰æ•ˆæ€§"""
        validated = params.copy()
        
        # ç¡®ä¿æ•°å€¼å‚æ•°åœ¨åˆç†èŒƒå›´å†…
        if 'rsiThreshold' in validated:
            validated['rsiThreshold'] = max(10, min(50, validated['rsiThreshold']))
        
        if 'rsiOversold' in validated:
            validated['rsiOversold'] = max(5, min(40, validated['rsiOversold']))
        
        if 'rsiOverbought' in validated:
            validated['rsiOverbought'] = max(60, min(95, validated['rsiOverbought']))
        
        if 'volumeMulti' in validated:
            validated['volumeMulti'] = max(1.0, min(5.0, validated['volumeMulti']))
        
        if 'stopLoss' in validated:
            validated['stopLoss'] = max(0.03, min(0.20, validated['stopLoss']))
        
        if 'takeProfit' in validated:
            validated['takeProfit'] = max(0.05, min(0.50, validated['takeProfit']))
        
        if 'scoreThreshold' in validated:
            validated['scoreThreshold'] = max(40, min(85, validated['scoreThreshold']))
        
        if 'minConditions' in validated:
            validated['minConditions'] = max(1, min(6, int(validated['minConditions'])))
        
        return validated


class StockFeatureAnalyzer:
    """è‚¡ç¥¨ç‰¹å¾åˆ†æå™¨ - ç”¨äºè‡ªé€‚åº”å‚æ•°è°ƒæ•´"""
    
    @staticmethod
    def analyze_stock_features(df):
        """åˆ†æè‚¡ç¥¨ç‰¹å¾ï¼Œè¿”å›ç‰¹å¾å­—å…¸"""
        if df is None or len(df) < 60:
            return {
                'volatility': 'medium',
                'liquidity': 'medium',
                'trend_type': 'neutral',
                'industry_style': 'general',
                'market_cap_category': 'mid',
                'volatility_atr_pct': 2.0,
                'avg_volume': 1000000,
                'price_level': 50
            }
        
        latest = df.iloc[-1]
        
        # 1. æ³¢åŠ¨ç‡ç‰¹å¾ (åŸºäºATRç™¾åˆ†æ¯”)
        atr_pct = latest.get('atr_pct', 2.0)
        if atr_pct > 4:
            volatility = 'very_high'
        elif atr_pct > 2.5:
            volatility = 'high'
        elif atr_pct > 1.5:
            volatility = 'medium'
        elif atr_pct > 0.8:
            volatility = 'low'
        else:
            volatility = 'very_low'
        
        # 2. æµåŠ¨æ€§ç‰¹å¾ (åŸºäºæˆäº¤é‡)
        avg_volume = df['volume'].mean() if 'volume' in df.columns else 1000000
        if avg_volume > 50000000:
            liquidity = 'very_high'
        elif avg_volume > 10000000:
            liquidity = 'high'
        elif avg_volume > 2000000:
            liquidity = 'medium'
        elif avg_volume > 500000:
            liquidity = 'low'
        else:
            liquidity = 'very_low'
        
        # 3. è¶‹åŠ¿ç±»å‹
        if len(df) >= 60:
            price_change_60d = (df.iloc[-1]['close'] - df.iloc[-60]['close']) / df.iloc[-60]['close'] * 100
            if price_change_60d > 30:
                trend_type = 'strong_uptrend'
            elif price_change_60d > 10:
                trend_type = 'weak_uptrend'
            elif price_change_60d < -30:
                trend_type = 'strong_downtrend'
            elif price_change_60d < -10:
                trend_type = 'weak_downtrend'
            else:
                trend_type = 'sideways'
        else:
            trend_type = 'neutral'
        
        # 4. ä»·æ ¼æ°´å¹³
        price_level = latest.get('close', 50)
        if price_level > 200:
            price_category = 'very_high'
        elif price_level > 100:
            price_category = 'high'
        elif price_level > 50:
            price_category = 'medium'
        elif price_level > 20:
            price_category = 'low'
        else:
            price_category = 'very_low'
        
        # 5. æ³¢åŠ¨ç‡ç¨³å®šæ€§
        if len(df) >= 20:
            atr_std = df['atr_pct'].std() if 'atr_pct' in df.columns else 0
            if atr_std > 2:
                volatility_stability = 'unstable'
            elif atr_std > 1:
                volatility_stability = 'moderate'
            else:
                volatility_stability = 'stable'
        else:
            volatility_stability = 'moderate'
        
        return {
            'volatility': volatility,
            'volatility_atr_pct': atr_pct,
            'liquidity': liquidity,
            'avg_volume': avg_volume,
            'trend_type': trend_type,
            'price_level': price_level,
            'price_category': price_category,
            'volatility_stability': volatility_stability,
            'boll_width': latest.get('bollWidth', 0.1),
            'adx': latest.get('adx', 25)
        }
    
    @staticmethod
    def get_adaptive_params(base_config, stock_features):
        """æ ¹æ®è‚¡ç¥¨ç‰¹å¾è·å–è‡ªé€‚åº”å‚æ•°"""
        config = base_config.copy()
        
        # æ³¢åŠ¨ç‡è°ƒæ•´å› å­
        volatility_factors = {
            'very_high': {'stop_loss_mult': 1.5, 'take_profit_mult': 1.3, 'rsi_threshold_adj': 5, 'volume_mult': 0.8},
            'high': {'stop_loss_mult': 1.3, 'take_profit_mult': 1.2, 'rsi_threshold_adj': 3, 'volume_mult': 0.9},
            'medium': {'stop_loss_mult': 1.0, 'take_profit_mult': 1.0, 'rsi_threshold_adj': 0, 'volume_mult': 1.0},
            'low': {'stop_loss_mult': 0.8, 'take_profit_mult': 0.9, 'rsi_threshold_adj': -3, 'volume_mult': 1.1},
            'very_low': {'stop_loss_mult': 0.7, 'take_profit_mult': 0.8, 'rsi_threshold_adj': -5, 'volume_mult': 1.2}
        }
        
        vf = volatility_factors.get(stock_features['volatility'], volatility_factors['medium'])
        
        # è°ƒæ•´æ­¢æŸæ­¢ç›ˆ
        if 'stopLoss' in config:
            config['stopLoss'] = config['stopLoss'] * vf['stop_loss_mult']
        if 'takeProfit' in config:
            config['takeProfit'] = config['takeProfit'] * vf['take_profit_mult']
        
        # è°ƒæ•´RSIé˜ˆå€¼
        if 'rsiThreshold' in config:
            config['rsiThreshold'] = config['rsiThreshold'] + vf['rsi_threshold_adj']
        if 'rsiOversold' in config:
            config['rsiOversold'] = max(10, config['rsiOversold'] + vf['rsi_threshold_adj'])
        if 'rsiOverbought' in config:
            config['rsiOverbought'] = min(90, config['rsiOverbought'] - vf['rsi_threshold_adj'])
        
        # è°ƒæ•´æˆäº¤é‡å€æ•°
        if 'volumeMulti' in config:
            config['volumeMulti'] = config['volumeMulti'] * vf['volume_mult']
        
        # è¶‹åŠ¿ç±»å‹è°ƒæ•´
        trend_factors = {
            'strong_uptrend': {'macd_threshold_adj': 0.5, 'ma_bias': 0.02},
            'weak_uptrend': {'macd_threshold_adj': 0.2, 'ma_bias': 0.01},
            'sideways': {'macd_threshold_adj': 0, 'ma_bias': 0},
            'weak_downtrend': {'macd_threshold_adj': -0.2, 'ma_bias': -0.01},
            'strong_downtrend': {'macd_threshold_adj': -0.5, 'ma_bias': -0.02},
            'neutral': {'macd_threshold_adj': 0, 'ma_bias': 0}
        }
        
        tf = trend_factors.get(stock_features['trend_type'], trend_factors['neutral'])
        
        if 'macdThreshold' in config:
            config['macdThreshold'] = config['macdThreshold'] + tf['macd_threshold_adj']
        
        # æµåŠ¨æ€§è°ƒæ•´
        liquidity_factors = {
            'very_high': {'position_size_mult': 1.2, 'slippage': 0.001},
            'high': {'position_size_mult': 1.1, 'slippage': 0.002},
            'medium': {'position_size_mult': 1.0, 'slippage': 0.003},
            'low': {'position_size_mult': 0.8, 'slippage': 0.005},
            'very_low': {'position_size_mult': 0.6, 'slippage': 0.008}
        }
        
        lf = liquidity_factors.get(stock_features['liquidity'], liquidity_factors['medium'])
        
        if 'positionSizeMult' in config:
            config['positionSizeMult'] = config['positionSizeMult'] * lf['position_size_mult']
        
        return config


class StrategyEngine:
    """ç­–ç•¥å¼•æ“"""
    
    STRATEGIES = {
        'deep_fusion': {
            'name': 'æ·±åº¦èåˆç­–ç•¥',
            'icon': 'ğŸ¤–',
            'description': 'èåˆå¤šæŠ€æœ¯æŒ‡æ ‡ï¼Œé€šè¿‡åŠ æƒè¯„åˆ†+ç¡®è®¤ä¿¡å·æœºåˆ¶å†³ç­–ï¼Œæ”¯æŒè‡ªé€‚åº”å‚æ•°è°ƒæ•´',
            'features': ['MAå¯¹é½ç¡®è®¤', 'MACDåŠ¨é‡', 'RSIè¶…å–', 'æˆäº¤é‡ç¡®è®¤', 'å¸ƒæ—å¸¦ä½ç½®', 'ADXè¶‹åŠ¿å¼ºåº¦', 'ä»·æ ¼åŠ¨é‡']
        },
        'volume_breakout': {
            'name': 'é‡ä»·çªç ´ç­–ç•¥',
            'icon': 'âš¡',
            'description': 'å¤šç»´åº¦çªç ´ç¡®è®¤ï¼šä»·æ ¼çªç ´+æˆäº¤é‡æ”¾å¤§+è¶‹åŠ¿å¯¹é½+æ³¢åŠ¨ç‡è¿‡æ»¤',
            'features': ['ä»·æ ¼çªç ´', 'æˆäº¤é‡æ¿€å¢', 'è¶‹åŠ¿ç¡®è®¤', 'æ³¢åŠ¨ç‡è¿‡æ»¤', 'åŠ¨èƒ½ç¡®è®¤', 'Kçº¿å½¢æ€', 'ç­¹ç åˆ†å¸ƒ']
        },
        'oversold_rebound': {
            'name': 'è¶…è·Œåå¼¹ç­–ç•¥',
            'icon': 'ğŸ’',
            'description': 'å¤šç»´åº¦è¶…å–ç¡®è®¤+åå¼¹ä¿¡å·æ£€æµ‹ï¼šRSIèƒŒç¦»+MACDèƒŒç¦»+Kçº¿å½¢æ€',
            'features': ['RSIè¶…å–', 'å¸ƒæ—ä¸‹è½¨', 'ä»·æ ¼ä½ç‚¹', 'æˆäº¤é‡æ¨¡å¼', 'Kçº¿å½¢æ€', 'MACDèƒŒç¦»', 'è¶‹åŠ¿è¿‡æ»¤']
        },
        'trend_enhanced': {
            'name': 'è¶‹åŠ¿å¢å¼ºç­–ç•¥',
            'icon': 'ğŸ“ˆ',
            'description': 'å¤šæ—¶é—´æ¡†æ¶è¶‹åŠ¿ç¡®è®¤ï¼šå‡çº¿æ’åˆ—+ADXå¼ºåº¦+æˆäº¤é‡è¶‹åŠ¿+åŠ¨é‡ç¡®è®¤',
            'features': ['å‡çº¿æ’åˆ—', 'ä»·æ ¼ä½ç½®', 'MACDåŠ¨èƒ½', 'ADXå¼ºåº¦', 'æˆäº¤é‡è¶‹åŠ¿', 'å¸ƒæ—å¸¦è¶‹åŠ¿', 'è¶‹åŠ¿æŒç»­æ€§']
        },
        'macd_divergence': {
            'name': 'MACDèƒŒç¦»ç­–ç•¥',
            'icon': 'ğŸ¯',
            'description': 'å¤šå‘¨æœŸèƒŒç¦»æ£€æµ‹ï¼šä»·æ ¼-MACDèƒŒç¦»+æˆäº¤é‡èƒŒç¦»+RSIèƒŒç¦»+Kçº¿å½¢æ€ç¡®è®¤',
            'features': ['åº•èƒŒç¦»æ£€æµ‹', 'æŸ±çŠ¶å›¾èƒŒç¦»', 'æˆäº¤é‡èƒŒç¦»', 'RSIèƒŒç¦»', 'è¶‹åŠ¿è¿‡æ»¤', 'Kçº¿å½¢æ€', 'å‡çº¿ç¡®è®¤']
        },
        'bollinger_extreme': {
            'name': 'å¸ƒæ—æé™ç­–ç•¥',
            'icon': 'ğŸ“Š',
            'description': 'å¸ƒæ—å¸¦æå€¼äº¤æ˜“+åè½¬ç¡®è®¤ï¼šä¸‹è½¨è§¦åŠ+RSIè¶…å–+æˆäº¤é‡åœ°é‡+Kçº¿å½¢æ€',
            'features': ['ä¸‹è½¨è§¦åŠ', 'å¸¦å®½ç¡®è®¤', 'RSIè¶…å–', 'æˆäº¤é‡ç¡®è®¤', 'å‡çº¿æ”¯æ’‘', 'Kçº¿å½¢æ€', 'æ³¢åŠ¨ç‡è°ƒæ•´']
        },
        'momentum_rotation': {
            'name': 'åŠ¨é‡è½®åŠ¨ç­–ç•¥',
            'icon': 'âš¡',
            'description': 'å¤šå‘¨æœŸåŠ¨é‡ç¡®è®¤+ç›¸å¯¹å¼ºåº¦åˆ†æï¼šçŸ­ä¸­é•¿æœŸåŠ¨é‡+åŠ¨é‡è´¨é‡+æ³¢åŠ¨ç‡åŒ¹é…',
            'features': ['çŸ­æœŸåŠ¨é‡', 'ä¸­æœŸåŠ¨é‡', 'é•¿æœŸåŠ¨é‡', 'ç›¸å¯¹å¼ºåº¦', 'æˆäº¤é‡ç¡®è®¤', 'åŠ¨é‡è´¨é‡', 'MACDç¡®è®¤']
        },
        'turtle_enhanced': {
            'name': 'æµ·é¾Ÿå¢å¼ºç­–ç•¥',
            'icon': 'ğŸ¢',
            'description': 'å¤šæ—¶é—´æ¡†æ¶çªç ´+é£é™©ç®¡ç†å¢å¼ºï¼šçªç ´ç¡®è®¤+è¶‹åŠ¿è¿‡æ»¤+å‡çªç ´è¿‡æ»¤+é£é™©è¯„åˆ†',
            'features': ['çªç ´å…¥åœº', 'è¶‹åŠ¿è¿‡æ»¤', 'æ³¢åŠ¨ç‡è¿‡æ»¤', 'æˆäº¤é‡ç¡®è®¤', 'ADXå¼ºåº¦', 'å‡çªç ´è¿‡æ»¤', 'é£é™©ç®¡ç†']
        },
        'chip_peak_breakout': {
            'name': 'ç­¹ç çªç ´ç­–ç•¥',
            'icon': 'ğŸ§©',
            'description': 'ç­¹ç å•å³°ä½ä½+æ”¾é‡çªç ´ï¼šé€šè¿‡ç­¹ç åˆ†å¸ƒè¯†åˆ«ä½ä½é›†ä¸­åŒºåŸŸï¼Œé…åˆæ”¾é‡çªç ´ä¿¡å·',
            'features': ['ç­¹ç åˆ†å¸ƒ', 'å•å³°è¯†åˆ«', 'é›†ä¸­åº¦åˆ†æ', 'ä½ä½åˆ¤æ–­', 'æ”¾é‡çªç ´', 'è¶‹åŠ¿ç¡®è®¤', 'ç­¹ç çªç ´']
        },
        'xgboost_ml': {
            'name': 'XGBoostæœºå™¨å­¦ä¹ ç­–ç•¥',
            'icon': 'ğŸš€',
            'description': 'XGBoostæ¢¯åº¦æå‡é¢„æµ‹+æ¡ä»¶ç¡®è®¤ï¼šç”¨å†å²æ•°æ®è®­ç»ƒXGBoostæ¨¡å‹ï¼Œæ¦‚ç‡è¿‡æ»¤+å¤šé‡ç¡®è®¤',
            'features': ['XGBoost', 'æ¢¯åº¦æå‡', 'ç‰¹å¾å·¥ç¨‹', 'äº¤å‰éªŒè¯', 'è¶‹åŠ¿è¿‡æ»¤', 'æ³¢åŠ¨ç‡è¿‡æ»¤', 'è·³ç©ºè¿‡æ»¤']
        },
        'lstm_ml': {
            'name': 'LSTMæ·±åº¦å­¦ä¹ ç­–ç•¥',
            'icon': 'ğŸ§ ',
            'description': 'LSTMæ—¶é—´åºåˆ—é¢„æµ‹+æ¡ä»¶ç¡®è®¤ï¼šç”¨å†å²æ•°æ®è®­ç»ƒLSTMæ¨¡å‹ï¼Œæ•æ‰æ—¶é—´åºåˆ—æ¨¡å¼',
            'features': ['LSTM', 'æ—¶é—´åºåˆ—', 'æ·±åº¦å­¦ä¹ ', 'åºåˆ—ç‰¹å¾', 'è¶‹åŠ¿è¿‡æ»¤', 'æ³¢åŠ¨ç‡è¿‡æ»¤', 'è·³ç©ºè¿‡æ»¤']
        },
        'ensemble_ml': {
            'name': 'æ··åˆæœºå™¨å­¦ä¹ ç­–ç•¥',
            'icon': 'ğŸ­',
            'description': 'XGBoost+LSTMæ··åˆé¢„æµ‹ï¼šç»“åˆæ¢¯åº¦æå‡å’Œæ—¶é—´åºåˆ—æ¨¡å‹çš„ä¼˜åŠ¿ï¼Œæé«˜é¢„æµ‹ç¨³å®šæ€§',
            'features': ['XGBoost', 'LSTM', 'æ··åˆå­¦ä¹ ', 'ç‰¹å¾èåˆ', 'è¶‹åŠ¿è¿‡æ»¤', 'æ³¢åŠ¨ç‡è¿‡æ»¤', 'è·³ç©ºè¿‡æ»¤']
        },
        'mtsf_net': {
            'name': 'MTSF-Netå¤šå°ºåº¦èåˆç­–ç•¥',
            'icon': 'ğŸ¯',
            'description': 'å¤šå°ºåº¦æ—¶åºèåˆç½‘ç»œï¼šèåˆçŸ­/ä¸­/é•¿æœŸç‰¹å¾ï¼Œæ•æ‰å¤šå°ºåº¦æ¨¡å¼ï¼Œé€‚åˆæ³¢æ®µæ“ä½œ',
            'features': ['å¤šå°ºåº¦ç‰¹å¾', 'æ—¶åºèåˆ', 'æ·±åº¦å­¦ä¹ ', 'è‡ªé€‚åº”æƒé‡', 'è¶‹åŠ¿æ•æ‰', 'æ³¢åŠ¨ç‡è¿‡æ»¤', 'æˆäº¤é‡ç¡®è®¤']
        },
        'vpan_net': {
            'name': 'VPANé‡ä»·æ³¨æ„åŠ›ç­–ç•¥',
            'icon': 'ğŸ”®',
            'description': 'é‡ä»·æ³¨æ„åŠ›é¢„æµ‹ç½‘ç»œï¼šä½¿ç”¨Transformeræ¶æ„ï¼Œé‡ç‚¹å…³æ³¨é‡ä»·å…³ç³»å’Œå¸‚åœºæƒ…ç»ªï¼Œé€‚åˆè¶‹åŠ¿è·Ÿè¸ª',
            'features': ['Transformer', 'æ³¨æ„åŠ›æœºåˆ¶', 'é‡ä»·å…³ç³»', 'å¸‚åœºæƒ…ç»ª', 'èµ„é‡‘æµå‘', 'è¶‹åŠ¿è·Ÿè¸ª', 'æ·±åº¦å­¦ä¹ ']
        }
    }

    @staticmethod
    def passes_global_buy_filters(df, i, strategy_name, config, market_state, signal_strength):
        try:
            min_strength = float(config.get('minSignalStrength', 0) or 0)
        except Exception:
            min_strength = 0
        if min_strength > 0 and signal_strength < min_strength:
            return False

        if market_state == 'bear_market' and config.get('blockBuyInBear', False):
            try:
                need = float(config.get('minStrengthBear', 70) or 70)
            except Exception:
                need = 70
            if signal_strength < need:
                return False

        if market_state == 'choppy_market' and config.get('blockBuyInChoppy', False):
            try:
                need = float(config.get('minStrengthChoppy', 60) or 60)
            except Exception:
                need = 60
            if signal_strength < need:
                return False

        apply_trend = config.get('enableTrendFilter', False)
        if apply_trend and strategy_name in {
            'deep_fusion',
            'volume_breakout',
            'trend_enhanced',
            'macd_divergence',
            'momentum_rotation',
            'turtle_enhanced',
            'xgboost_ml',
            'lstm_ml',
            'ensemble_ml',
        }:
            try:
                d = df.iloc[i]
                close = float(d.get('close', 0) or 0)
                ma20 = d.get('ma20', None)
                if ma20 is None or pd.isna(ma20):
                    return True
                ma20 = float(ma20)
                if close <= ma20:
                    return False
                lookback = int(config.get('trendSlopeLookback', 5) or 5)
                j = max(0, i - max(2, lookback))
                ma20_prev = df.iloc[j].get('ma20', None)
                if ma20_prev is None or pd.isna(ma20_prev):
                    return True
                if float(ma20_prev) >= ma20:
                    return False
            except Exception:
                return False

        apply_vol_ratio = config.get('enableVolRatioFilter', False)
        if apply_vol_ratio and strategy_name in {
            'deep_fusion',
            'volume_breakout',
            'trend_enhanced',
            'momentum_rotation',
            'turtle_enhanced',
            'xgboost_ml',
            'lstm_ml',
            'ensemble_ml',
        }:
            try:
                if 'volume' in df.columns:
                    cur_vol = float(df.iloc[i].get('volume', 0) or 0)
                    start = max(0, i - 5)
                    hist = df['volume'].iloc[start:i]
                    avg = float(hist.mean()) if len(hist) > 0 else 0
                    if avg > 0 and cur_vol > 0:
                        ratio = cur_vol / avg
                        try:
                            need = float(config.get('minVolRatio', 1.05) or 1.05)
                        except Exception:
                            need = 1.05
                        if ratio < need:
                            return False
            except Exception:
                pass

        return True
    
    @staticmethod
    def xgboost_extract_features(df, i):
        """æå–XGBoostç‰¹å¾ - ä¼˜åŒ–ç‰ˆï¼Œå¢å¼ºè¶‹åŠ¿è¯†åˆ«èƒ½åŠ›"""
        if i < 30:
            return None
        
        d = df.iloc[i]
        prev = df.iloc[i-1]
        
        try:
            close = float(d.get('close', 0))
            if close <= 0:
                return None
            
            open_price = float(d.get('open', close))
            prev_close = float(prev.get('close', close))
            high = float(d.get('high', close))
            low = float(d.get('low', close))
            volume = float(d.get('volume', 0))
            
            # åŸºç¡€æŒ‡æ ‡
            rsi = float(d.get('rsi', 50))
            macd = float(d.get('macd', 0))
            macd_hist = float(d.get('macdHist', 0))
            ma5 = float(d.get('ma5', close))
            ma10 = float(d.get('ma10', close))
            ma20 = float(d.get('ma20', close))
            ma60 = float(d.get('ma60', close))
            vol_ma5 = float(d.get('volMa5', volume))
            
            # è¡ç”Ÿç‰¹å¾
            gap = (open_price - prev_close) / prev_close if prev_close > 0 else 0
            atr = float(d.get('atr', (high - low) * 0.5))
            atr_pct = atr / close if close > 0 else 0
            body_pct = abs(close - open_price) / open_price if open_price > 0 else 0
            
            # åŠ¨é‡ç‰¹å¾ - å¢åŠ å¤šå‘¨æœŸ
            mom3 = (close - float(df.iloc[i-3]['close'])) / float(df.iloc[i-3]['close']) if i >= 3 else 0
            mom5 = (close - float(df.iloc[i-5]['close'])) / float(df.iloc[i-5]['close']) if i >= 5 else 0
            mom10 = (close - float(df.iloc[i-10]['close'])) / float(df.iloc[i-10]['close']) if i >= 10 else 0
            mom20 = (close - float(df.iloc[i-20]['close'])) / float(df.iloc[i-20]['close']) if i >= 20 else 0
            
            # ä»·æ ¼ä½ç½® - æ‰©å±•åˆ°60æ—¥
            high20 = df['high'].iloc[i-19:i+1].max()
            low20 = df['low'].iloc[i-19:i+1].min()
            pos20 = (close - low20) / (high20 - low20) if high20 > low20 else 0.5
            
            high60 = df['high'].iloc[i-59:i+1].max()
            low60 = df['low'].iloc[i-59:i+1].min()
            pos60 = (close - low60) / (high60 - low60) if high60 > low60 else 0.5
            
            # æˆäº¤é‡æ¯”ç‡
            vol_ratio = volume / vol_ma5 if vol_ma5 > 0 else 1
            
            # å‡çº¿æ–œç‡ - å¤šå‘¨æœŸ
            ma5_prev = float(df.iloc[i-3].get('ma5', ma5)) if i >= 3 else ma5
            ma10_prev = float(df.iloc[i-5].get('ma10', ma10)) if i >= 5 else ma10
            ma20_prev = float(df.iloc[i-5].get('ma20', ma20)) if i >= 5 else ma20
            ma60_prev = float(df.iloc[i-10].get('ma60', ma60)) if i >= 10 else ma60
            
            slope5 = (ma5 - ma5_prev) / ma5 if ma5 > 0 else 0
            slope10 = (ma10 - ma10_prev) / ma10 if ma10 > 0 else 0
            slope20 = (ma20 - ma20_prev) / ma20 if ma20 > 0 else 0
            slope60 = (ma60 - ma60_prev) / ma60 if ma60 > 0 else 0
            
            # å‡çº¿å¤šå¤´æ’åˆ—å¼ºåº¦ - æ–°å¢ç‰¹å¾
            ma_alignment = 0
            if ma5 > ma10 > ma20 > ma60:
                ma_alignment = 1
            elif ma5 > ma20 > ma60:
                ma_alignment = 0.5
            elif ma20 > ma60:
                ma_alignment = 0.25
            
            # ä»·æ ¼çªç ´ç‰¹å¾ - æ–°å¢
            breakout_strength = 0
            if close > ma20 and prev_close <= ma20:
                breakout_strength = 1  # çªç ´MA20
            if close > ma60 and prev_close <= ma60:
                breakout_strength = max(breakout_strength, 1)  # çªç ´MA60
            
            # è¿ç»­ä¸Šæ¶¨å¤©æ•° - æ–°å¢
            up_days = 0
            for j in range(max(0, i-10), i):  # ä»iå¾€å‰æ•°10å¤©
                if j > 0 and df.iloc[j]['close'] > df.iloc[j-1]['close']:
                    up_days += 1
                else:
                    break
            up_days_ratio = up_days / 10  # å½’ä¸€åŒ–
            
            # åŠ¨é‡åŠ é€Ÿç‰¹å¾ - æ–°å¢
            mom_accel = mom5 - mom10 if mom10 != 0 else 0  # çŸ­æœŸåŠ¨é‡vsä¸­æœŸåŠ¨é‡
            
            # æˆäº¤é‡æ”¾å¤§è¶‹åŠ¿ - æ–°å¢
            vol_trend = 0
            if i >= 5:
                vol_prev5 = df['volume'].iloc[i-5:i].mean()
                if vol_prev5 > 0:
                    vol_trend = volume / vol_prev5
            
            # å¸ƒæ—å¸¦ä½ç½®
            boll_upper = float(d.get('bollUpper', close))
            boll_lower = float(d.get('bollLower', close))
            boll_pos = (close - boll_lower) / (boll_upper - boll_lower) if boll_upper > boll_lower else 0.5
            
            # KDJæŒ‡æ ‡
            k = float(d.get('kdjK', 50))
            d_kdj = float(d.get('kdjD', 50))
            j_kdj = float(d.get('kdjJ', 50))
            
            # MACDé‡‘å‰ä¿¡å· - æ–°å¢
            macd_prev = float(df.iloc[i-1].get('macd', 0)) if i >= 1 else 0
            macd_hist_prev = float(df.iloc[i-1].get('macdHist', 0)) if i >= 1 else 0
            macd_golden_cross = 0
            if macd > 0 and macd_prev <= 0:
                macd_golden_cross = 1  # MACDé‡‘å‰
            if macd_hist > 0 and macd_hist_prev <= 0:
                macd_golden_cross = max(macd_golden_cross, 1)  # MACDæŸ±çŠ¶å›¾è½¬æ­£
            
            # ç‰¹å¾å‘é‡ (å½’ä¸€åŒ–å¤„ç†) - æ‰©å±•åˆ°28ä¸ªç‰¹å¾
            features = [
                rsi / 100.0,                    # RSIå½’ä¸€åŒ–
                macd / close if close > 0 else 0,  # MACDå½’ä¸€åŒ–
                (close / ma5 - 1) if ma5 > 0 else 0,  # ç›¸å¯¹MA5ä½ç½®
                (close / ma10 - 1) if ma10 > 0 else 0,  # ç›¸å¯¹MA10ä½ç½®
                (close / ma20 - 1) if ma20 > 0 else 0,  # ç›¸å¯¹MA20ä½ç½®
                (close / ma60 - 1) if ma60 > 0 else 0,  # ç›¸å¯¹MA60ä½ç½®
                vol_ratio,                      # æˆäº¤é‡æ¯”ç‡
                pos20,                          # 20æ—¥ä»·æ ¼ä½ç½®
                pos60,                          # 60æ—¥ä»·æ ¼ä½ç½®
                mom3,                           # 3æ—¥åŠ¨é‡
                mom5,                           # 5æ—¥åŠ¨é‡
                mom10,                          # 10æ—¥åŠ¨é‡
                mom20,                          # 20æ—¥åŠ¨é‡
                gap,                            # è·³ç©ºå¹…åº¦
                atr_pct,                        # ATRç™¾åˆ†æ¯”
                body_pct,                       # å®ä½“ç™¾åˆ†æ¯”
                slope5,                         # MA5æ–œç‡
                slope10,                        # MA10æ–œç‡
                slope20,                        # MA20æ–œç‡
                slope60,                        # MA60æ–œç‡
                ma_alignment,                    # å‡çº¿å¤šå¤´æ’åˆ—å¼ºåº¦
                breakout_strength,               # çªç ´å¼ºåº¦
                up_days_ratio,                  # è¿ç»­ä¸Šæ¶¨æ¯”ä¾‹
                mom_accel,                      # åŠ¨é‡åŠ é€Ÿ
                vol_trend,                      # æˆäº¤é‡è¶‹åŠ¿
                boll_pos,                       # å¸ƒæ—å¸¦ä½ç½®
                k / 100.0,                      # KDJ K
                d_kdj / 100.0,                  # KDJ D
                j_kdj / 100.0,                  # KDJ J
                macd_hist / close if close > 0 else 0,  # MACDæŸ±çŠ¶å›¾
                macd_golden_cross,              # MACDé‡‘å‰ä¿¡å·
            ]
            
            # æ£€æŸ¥ç‰¹å¾æœ‰æ•ˆæ€§
            for f in features:
                if not isinstance(f, (int, float)) or pd.isna(f):
                    return None
            
            return features
            
        except Exception as e:
            return None
    
    @staticmethod
    def lstm_extract_sequence_features(df, i, seq_len=30):
        """æå–LSTMæ—¶é—´åºåˆ—ç‰¹å¾ - å¢å¼ºç‰ˆ"""
        if i < seq_len:
            return None
        
        try:
            sequence = []
            for j in range(i - seq_len, i):
                d = df.iloc[j]
                close = float(d.get('close', 0))
                if close <= 0:
                    return None
                
                open_price = float(d.get('open', close))
                high = float(d.get('high', close))
                low = float(d.get('low', close))
                volume = float(d.get('volume', 0))
                prev_close = float(df.iloc[j-1]['close']) if j > 0 else close
                prev_high = float(df.iloc[j-1]['high']) if j > 0 else high
                prev_low = float(df.iloc[j-1]['low']) if j > 0 else low
                
                rsi = float(d.get('rsi', 50))
                macd = float(d.get('macd', 0))
                macd_hist = float(d.get('macdHist', 0))
                ma5 = float(d.get('ma5', close))
                ma10 = float(d.get('ma10', close))
                ma20 = float(d.get('ma20', close))
                ma60 = float(d.get('ma60', close))
                vol_ma5 = float(d.get('volMa5', volume))
                
                boll_upper = float(d.get('bollUpper', close))
                boll_lower = float(d.get('bollLower', close))
                boll_mid = float(d.get('bollMid', close))
                
                kdj_k = float(d.get('kdjK', 50))
                kdj_d = float(d.get('kdjD', 50))
                kdj_j = float(d.get('kdjJ', 50))
                
                atr = float(d.get('atr', (high - low) * 0.5))
                
                returns = (close - prev_close) / prev_close if prev_close > 0 else 0
                returns_prev = (prev_close - float(df.iloc[j-2]['close'])) / float(df.iloc[j-2]['close']) if j > 1 else 0
                volume_ratio = volume / vol_ma5 if vol_ma5 > 0 else 1
                
                true_range = max(high - low, abs(high - prev_close), abs(low - prev_close))
                atr_ratio = atr / close if close > 0 else 0
                
                body_size = abs(close - open_price)
                upper_shadow = high - max(open_price, close)
                lower_shadow = min(open_price, close) - low
                body_ratio = body_size / close if close > 0 else 0
                upper_shadow_ratio = upper_shadow / close if close > 0 else 0
                lower_shadow_ratio = lower_shadow / close if close > 0 else 0
                
                price_position = (close - boll_lower) / (boll_upper - boll_lower) if boll_upper > boll_lower else 0.5
                
                momentum_5 = (close - float(df.iloc[j-5]['close'])) / float(df.iloc[j-5]['close']) if j >= 5 else 0
                momentum_10 = (close - float(df.iloc[j-10]['close'])) / float(df.iloc[j-10]['close']) if j >= 10 else 0
                
                ma_slope_5 = (ma5 - float(df.iloc[j-3].get('ma5', ma5))) / ma5 if j >= 3 and ma5 > 0 else 0
                ma_slope_20 = (ma20 - float(df.iloc[j-5].get('ma20', ma20))) / ma20 if j >= 5 and ma20 > 0 else 0
                
                gap_up = (open_price - prev_close) / prev_close if prev_close > 0 else 0
                
                features = [
                    close / ma20 if ma20 > 0 else 1,
                    (close - open_price) / open_price if open_price > 0 else 0,
                    (high - low) / close if close > 0 else 0,
                    returns,
                    returns_prev,
                    volume_ratio,
                    rsi / 100.0,
                    macd / close if close > 0 else 0,
                    macd_hist / close if close > 0 else 0,
                    ma5 / ma20 if ma20 > 0 else 1,
                    ma10 / ma20 if ma20 > 0 else 1,
                    ma20 / ma60 if ma60 > 0 else 1,
                    ma_slope_5,
                    ma_slope_20,
                    price_position,
                    kdj_k / 100.0,
                    kdj_d / 100.0,
                    kdj_j / 100.0,
                    atr_ratio,
                    body_ratio,
                    upper_shadow_ratio,
                    lower_shadow_ratio,
                    momentum_5,
                    momentum_10,
                    gap_up,
                ]
                sequence.append(features)
            
            return np.array(sequence)
        except Exception as e:
            return None
    
    @staticmethod
    def lstm_train_model(sequences, labels, config_params=None):
        """è®­ç»ƒLSTMæ¨¡å‹ - å¢å¼ºç‰ˆ"""
        if len(sequences) < 50 or len(sequences) != len(labels):
            return None
        
        try:
            import numpy as np
            from sklearn.preprocessing import StandardScaler
            from sklearn.model_selection import train_test_split
            
            X = np.array(sequences)
            y = np.array(labels)
            
            n_samples, seq_len, n_features = X.shape
            
            params = {
                'lstm_units': 128,
                'lstm_layers': 2,
                'dropout_rate': 0.4,
                'learning_rate': 0.0005,
                'epochs': 150,
                'batch_size': 64,
                'l2_reg': 0.005,
                'use_bidirectional': True,
                'use_attention': True,
            }
            
            if config_params:
                params.update(config_params)
            
            X_reshaped = X.reshape(-1, n_features)
            scaler = StandardScaler()
            X_normalized = scaler.fit_transform(X_reshaped)
            X = X_normalized.reshape(n_samples, seq_len, n_features)
            
            pos_count = sum(y)
            neg_count = len(y) - pos_count
            class_weight = {0: 1.0, 1: neg_count / pos_count if pos_count > 0 else 1.0}
            
            split_idx = int(len(X) * 0.8)
            X_train, X_val = X[:split_idx], X[split_idx:]
            y_train, y_val = y[:split_idx], y[split_idx:]
            
            model = Sequential()
            
            if params.get('use_bidirectional', True):
                from tensorflow.keras.layers import Bidirectional
                
                for i in range(params['lstm_layers']):
                    return_sequences = i < params['lstm_layers'] - 1
                    if i == 0:
                        model.add(Bidirectional(LSTM(
                            params['lstm_units'],
                            return_sequences=return_sequences,
                            input_shape=(seq_len, n_features),
                            kernel_regularizer=l2(params['l2_reg'])
                        )))
                    else:
                        model.add(Bidirectional(LSTM(
                            params['lstm_units'],
                            return_sequences=return_sequences,
                            kernel_regularizer=l2(params['l2_reg'])
                        )))
                    model.add(Dropout(params['dropout_rate']))
                    model.add(BatchNormalization())
            else:
                for i in range(params['lstm_layers']):
                    return_sequences = i < params['lstm_layers'] - 1
                    if i == 0:
                        model.add(LSTM(
                            params['lstm_units'],
                            return_sequences=return_sequences,
                            input_shape=(seq_len, n_features),
                            kernel_regularizer=l2(params['l2_reg'])
                        ))
                    else:
                        model.add(LSTM(
                            params['lstm_units'],
                            return_sequences=return_sequences,
                            kernel_regularizer=l2(params['l2_reg'])
                        ))
                    model.add(Dropout(params['dropout_rate']))
                    model.add(BatchNormalization())
            
            model.add(Dense(64, activation='relu', kernel_regularizer=l2(params['l2_reg'])))
            model.add(Dropout(params['dropout_rate']))
            model.add(Dense(32, activation='relu', kernel_regularizer=l2(params['l2_reg'])))
            model.add(Dropout(params['dropout_rate']))
            model.add(Dense(1, activation='linear'))  # æ”¹ä¸ºçº¿æ€§æ¿€æ´»ï¼Œæ”¯æŒå›å½’é¢„æµ‹
            
            optimizer = Adam(learning_rate=params['learning_rate'], clipnorm=1.0)
            model.compile(
                optimizer=optimizer,
                loss='mse',  # æ”¹ä¸ºå‡æ–¹è¯¯å·®æŸå¤±ï¼Œé€‚åˆå›å½’ä»»åŠ¡
                metrics=['mae']  # ä½¿ç”¨å¹³å‡ç»å¯¹è¯¯å·®ä½œä¸ºè¯„ä¼°æŒ‡æ ‡
            )
            
            early_stopping = EarlyStopping(
                monitor='val_loss',  # æ”¹ä¸ºç›‘æ§éªŒè¯æŸå¤±
                mode='min',          # æŸå¤±è¶Šå°è¶Šå¥½
                patience=15,
                restore_best_weights=True,
                verbose=0
            )
            
            reduce_lr = ReduceLROnPlateau(
                monitor='val_loss',
                factor=0.7,
                patience=8,
                min_lr=1e-7,
                verbose=0
            )
            
            history = model.fit(
                X_train, y_train,
                epochs=params['epochs'],
                batch_size=params['batch_size'],
                validation_data=(X_val, y_val),
                callbacks=[early_stopping, reduce_lr],
                class_weight=class_weight,
                verbose=0
            )
            
            pred = model.predict(X, verbose=0).flatten()
            # å›å½’æ¨¡å‹è¾“å‡ºè¿ç»­å€¼ï¼Œä¸å†ä½¿ç”¨å›ºå®šé˜ˆå€¼
            mae = np.mean(np.abs(pred - y))  # è®¡ç®—å¹³å‡ç»å¯¹è¯¯å·®
            
            # åŸºäºé¢„æµ‹å€¼å’Œæ ‡å‡†å·®ç”Ÿæˆäº¤æ˜“ä¿¡å·
            pred_mean = np.mean(pred)
            pred_std = np.std(pred)
            
            # åŠ¨æ€é˜ˆå€¼ï¼šé¢„æµ‹å€¼é«˜äºå‡å€¼+0.5æ ‡å‡†å·®è§†ä¸ºä¹°å…¥ä¿¡å·
            dynamic_threshold = pred_mean + 0.5 * pred_std
            pred_labels = (pred >= dynamic_threshold).astype(int)
            
            accuracy = (pred_labels == y).mean()
            
            from sklearn.metrics import precision_score, recall_score, f1_score
            precision = precision_score(y, pred_labels, zero_division=0)
            recall = recall_score(y, pred_labels, zero_division=0)
            f1 = f1_score(y, pred_labels, zero_division=0)
            
            return {
                'model': model,
                'scaler': scaler,
                'accuracy': accuracy,
                'precision': precision,
                'recall': recall,
                'f1': f1,
                'base_rate': y.mean(),
                'class_weight': class_weight,
                'mae': mae,  # æ–°å¢å¹³å‡ç»å¯¹è¯¯å·®
                'pred_mean': pred_mean,  # æ–°å¢é¢„æµ‹å‡å€¼
                'pred_std': pred_std,  # æ–°å¢é¢„æµ‹æ ‡å‡†å·®
                'dynamic_threshold': dynamic_threshold,  # æ–°å¢åŠ¨æ€é˜ˆå€¼
                'history': {
                    'loss': history.history['loss'][-1],
                    'val_loss': history.history['val_loss'][-1],
                    'accuracy': history.history['accuracy'][-1],
                    'val_accuracy': history.history['val_accuracy'][-1],
                    'auc': history.history.get('auc', [0])[-1],
                    'val_auc': history.history.get('val_auc', [0])[-1],
                }
            }
            
        except Exception as e:
            print(f"LSTMè®­ç»ƒé”™è¯¯: {e}")
            import traceback
            traceback.print_exc()
            return None
    
    @staticmethod
    def lstm_predict(model_dict, sequence):
        """ä½¿ç”¨LSTMæ¨¡å‹é¢„æµ‹ - å¢å¼ºç‰ˆ"""
        if model_dict is None or 'model' not in model_dict:
            return None
        
        try:
            import numpy as np
            X = np.array([sequence])
            
            if 'scaler' in model_dict and model_dict['scaler'] is not None:
                n_samples, seq_len, n_features = X.shape
                X_reshaped = X.reshape(-1, n_features)
                X_normalized = model_dict['scaler'].transform(X_reshaped)
                X = X_normalized.reshape(n_samples, seq_len, n_features)
            
            prob = model_dict['model'].predict(X, verbose=0)[0, 0]
            return prob
        except Exception as e:
            return None
    
    @staticmethod
    def xgboost_train_model(features, labels, config_params=None):
        """è®­ç»ƒXGBoostæ¨¡å‹"""
        if len(features) < 50 or len(features) != len(labels):
            return None
        
        try:
            import numpy as np
            
            X = np.array(features)
            y = np.array(labels)
            
            # é»˜è®¤å‚æ•°
            params = {
                'max_depth': 4,
                'learning_rate': 0.05,
                'n_estimators': 100,
                'min_child_weight': 5,
                'subsample': 0.8,
                'colsample_bytree': 0.8,
                'reg_alpha': 0.1,
                'reg_lambda': 1.0,
                'random_state': 42,
                'objective': 'binary:logistic',
                'eval_metric': 'auc',
                'scale_pos_weight': 1.0
            }
            
            if config_params:
                params.update(config_params)
            
            # è®¡ç®—æ­£è´Ÿæ ·æœ¬æ¯”ä¾‹ï¼Œè°ƒæ•´scale_pos_weight
            pos_count = sum(y)
            neg_count = len(y) - pos_count
            if neg_count > 0:
                params['scale_pos_weight'] = neg_count / pos_count
            
            model = xgb.XGBClassifier(**params)
            model.fit(X, y)
            
            # è®¡ç®—è®­ç»ƒå‡†ç¡®ç‡
            pred = model.predict_proba(X)[:, 1]
            threshold = 0.5
            pred_labels = (pred >= threshold).astype(int)
            accuracy = (pred_labels == y).mean()
            
            return {
                'model': model,
                'accuracy': accuracy,
                'feature_importance': model.feature_importances_.tolist(),
                'base_rate': pos_count / len(y)
            }
            
        except Exception as e:
            print(f"XGBoostè®­ç»ƒé”™è¯¯: {e}")
            return None
    
    @staticmethod
    def xgboost_predict(model_dict, features):
        """ä½¿ç”¨XGBoostæ¨¡å‹é¢„æµ‹"""
        if model_dict is None or 'model' not in model_dict:
            return None
        
        try:
            import numpy as np
            X = np.array([features])
            prob = model_dict['model'].predict_proba(X)[0, 1]
            return prob
        except Exception as e:
            return None
    
    # ==================== MTSF-Net å¤šå°ºåº¦æ—¶åºèåˆç½‘ç»œ ====================
    @staticmethod
    def mtsf_extract_features(df, idx, lookback=30):
        """æå–MTSF-Netç‰¹å¾ - å¤šå°ºåº¦æ—¶åºç‰¹å¾"""
        try:
            if idx < lookback:
                return None
            
            features = []
            window = df.iloc[idx-lookback:idx]
            
            # ä»·æ ¼ç‰¹å¾ (åŸå§‹ä»·æ ¼ã€å¯¹æ•°æ”¶ç›Šç‡ã€æ³¢åŠ¨ç‡)
            prices = window['close'].values
            features.extend([
                prices[-1],  # æœ€æ–°ä»·æ ¼
                np.mean(prices),  # å‡ä»·
                np.std(prices),  # æ³¢åŠ¨ç‡
                (prices[-1] - prices[0]) / prices[0] if prices[0] > 0 else 0,  # åŒºé—´æ”¶ç›Š
            ])
            
            # å¤šå°ºåº¦å‡çº¿ç‰¹å¾
            for ma_period in [5, 10, 20, 60]:
                if len(prices) >= ma_period:
                    ma = np.mean(prices[-ma_period:])
                    features.append((prices[-1] - ma) / ma if ma > 0 else 0)
                else:
                    features.append(0)
            
            # æŠ€æœ¯æŒ‡æ ‡ç‰¹å¾
            curr = df.iloc[idx]
            features.extend([
                curr.get('rsi', 50) / 100,  # RSIå½’ä¸€åŒ–
                curr.get('macd', 0),  # MACD
                curr.get('macdHist', 0),  # MACDæŸ±çŠ¶å›¾
                min(curr.get('volume', 0) / curr.get('volMa5', 1), 5),  # æˆäº¤é‡æ¯”
                (curr.get('high', curr['close']) - curr.get('low', curr['close'])) / curr['close'] if curr['close'] > 0 else 0,  # æ—¥å†…æŒ¯å¹…
            ])
            
            return np.array(features, dtype=np.float32)
        except Exception as e:
            return None
    
    @staticmethod
    def mtsf_train_model(sequences, labels, lookback=30):
        """è®­ç»ƒMTSF-Netæ¨¡å‹ - å¤šå°ºåº¦æ—¶åºèåˆç½‘ç»œ"""
        if len(sequences) < 80 or len(sequences) != len(labels):
            return None
        
        try:
            import numpy as np
            from tensorflow.keras.models import Model
            from tensorflow.keras.layers import Input, LSTM, Dense, Dropout, BatchNormalization, Concatenate, Attention, Reshape
            from tensorflow.keras.optimizers import Adam
            
            X = np.array(sequences)
            y = np.array(labels)
            
            n_features = X.shape[1]
            
            # æ„å»ºå¤šåˆ†æ”¯ç½‘ç»œ
            # åˆ†æ”¯1: å…¨è¿æ¥å±‚å¤„ç†ç‰¹å¾
            inputs = Input(shape=(n_features,))
            
            # ç‰¹å¾æå–å±‚
            x = Dense(128, activation='relu')(inputs)
            x = BatchNormalization()(x)
            x = Dropout(0.3)(x)
            
            x = Dense(64, activation='relu')(x)
            x = BatchNormalization()(x)
            x = Dropout(0.2)(x)
            
            x = Dense(32, activation='relu')(x)
            x = BatchNormalization()(x)
            
            # è¾“å‡ºå±‚
            outputs = Dense(1, activation='sigmoid')(x)
            
            model = Model(inputs=inputs, outputs=outputs)
            model.compile(
                optimizer=Adam(learning_rate=0.001),
                loss='binary_crossentropy',
                metrics=['accuracy']
            )
            
            # ç±»åˆ«æƒé‡
            pos_count = sum(y)
            neg_count = len(y) - pos_count
            class_weight = {0: 1.0, 1: neg_count / pos_count if pos_count > 0 else 1.0}
            
            # è®­ç»ƒ
            model.fit(
                X, y,
                epochs=50,
                batch_size=32,
                verbose=0,
                class_weight=class_weight,
                validation_split=0.2
            )
            
            # è®¡ç®—å‡†ç¡®ç‡
            pred = model.predict(X, verbose=0)[:, 0]
            pred_labels = (pred >= 0.5).astype(int)
            accuracy = (pred_labels == y).mean()
            
            return {
                'model': model,
                'accuracy': accuracy,
                'base_rate': pos_count / len(y)
            }
            
        except Exception as e:
            print(f"MTSF-Netè®­ç»ƒé”™è¯¯: {e}")
            return None
    
    @staticmethod
    def mtsf_predict(model_dict, features):
        """ä½¿ç”¨MTSF-Netæ¨¡å‹é¢„æµ‹"""
        if model_dict is None or 'model' not in model_dict:
            return None
        
        try:
            import numpy as np
            X = np.array([features])
            prob = model_dict['model'].predict(X, verbose=0)[0, 0]
            return prob
        except Exception as e:
            return None
    
    # ==================== VPAN é‡ä»·æ³¨æ„åŠ›é¢„æµ‹ç½‘ç»œ ====================
    @staticmethod
    def vpan_extract_sequence(df, idx, seq_len=20):
        """æå–VPANåºåˆ—ç‰¹å¾ - ä»·æ ¼å’Œæˆäº¤é‡åºåˆ—"""
        try:
            if idx < seq_len:
                return None
            
            window = df.iloc[idx-seq_len:idx]
            
            # æ„å»ºå¤šç»´åº¦åºåˆ—ç‰¹å¾
            features = []
            for _, row in window.iterrows():
                features.append([
                    row['close'],  # æ”¶ç›˜ä»·
                    row['volume'],  # æˆäº¤é‡
                    row.get('high', row['close']),  # æœ€é«˜ä»·
                    row.get('low', row['close']),  # æœ€ä½ä»·
                    row.get('rsi', 50),  # RSI
                    row.get('macd', 0),  # MACD
                ])
            
            return np.array(features, dtype=np.float32)
        except Exception as e:
            return None
    
    @staticmethod
    def vpan_train_model(sequences, labels, seq_len=20):
        """è®­ç»ƒVPANæ¨¡å‹ - é‡ä»·æ³¨æ„åŠ›ç½‘ç»œ"""
        if len(sequences) < 60 or len(sequences) != len(labels):
            return None
        
        try:
            import numpy as np
            from tensorflow.keras.models import Model
            from tensorflow.keras.layers import Input, LSTM, Dense, Dropout, BatchNormalization, MultiHeadAttention, LayerNormalization, GlobalAveragePooling1D
            from tensorflow.keras.optimizers import Adam
            
            X = np.array(sequences)
            y = np.array(labels)
            
            n_features = X.shape[2]
            
            # æ„å»ºTransformer-likeæ¶æ„
            inputs = Input(shape=(seq_len, n_features))
            
            # LSTMç¼–ç 
            x = LSTM(64, return_sequences=True)(inputs)
            x = BatchNormalization()(x)
            x = Dropout(0.3)(x)
            
            # å¤šå¤´æ³¨æ„åŠ›æœºåˆ¶
            attention_output = MultiHeadAttention(num_heads=4, key_dim=16)(x, x)
            x = LayerNormalization()(attention_output + x)  # æ®‹å·®è¿æ¥
            
            # å…¨å±€å¹³å‡æ± åŒ–
            x = GlobalAveragePooling1D()(x)
            
            # å…¨è¿æ¥å±‚
            x = Dense(64, activation='relu')(x)
            x = BatchNormalization()(x)
            x = Dropout(0.3)(x)
            
            x = Dense(32, activation='relu')(x)
            x = BatchNormalization()(x)
            
            # è¾“å‡ºå±‚
            outputs = Dense(1, activation='sigmoid')(x)
            
            model = Model(inputs=inputs, outputs=outputs)
            model.compile(
                optimizer=Adam(learning_rate=0.001),
                loss='binary_crossentropy',
                metrics=['accuracy']
            )
            
            # ç±»åˆ«æƒé‡
            pos_count = sum(y)
            neg_count = len(y) - pos_count
            class_weight = {0: 1.0, 1: neg_count / pos_count if pos_count > 0 else 1.0}
            
            # è®­ç»ƒ
            model.fit(
                X, y,
                epochs=60,
                batch_size=32,
                verbose=0,
                class_weight=class_weight,
                validation_split=0.2
            )
            
            # è®¡ç®—å‡†ç¡®ç‡
            pred = model.predict(X, verbose=0)[:, 0]
            pred_labels = (pred >= 0.5).astype(int)
            accuracy = (pred_labels == y).mean()
            
            return {
                'model': model,
                'accuracy': accuracy,
                'base_rate': pos_count / len(y)
            }
            
        except Exception as e:
            print(f"VPANè®­ç»ƒé”™è¯¯: {e}")
            return None
    
    @staticmethod
    def vpan_predict(model_dict, sequence):
        """ä½¿ç”¨VPANæ¨¡å‹é¢„æµ‹"""
        if model_dict is None or 'model' not in model_dict:
            return None
        
        try:
            import numpy as np
            X = np.array([sequence])
            prob = model_dict['model'].predict(X, verbose=0)[0, 0]
            return prob
        except Exception as e:
            return None
    
    @staticmethod
    def execute_strategy(data, strategy_name, config, stock_features=None):
        """æ‰§è¡Œç­–ç•¥ - ä¼˜åŒ–ç‰ˆï¼Œæ”¯æŒè‡ªé€‚åº”å‚æ•°è°ƒæ•´"""
        df = pd.DataFrame(data)
        try:
            required_cols = {'close', 'high', 'low', 'volume', 'ma20', 'rsi', 'macd', 'macdHist', 'volMa5'}
            if not required_cols.issubset(set(df.columns)):
                df = calculate_indicators(df)
        except Exception:
            pass
        signals = []
        position = 0
        entry_price = 0
        entry_index = 0
        highest_since_entry = 0
        highest_body_since_entry = 0.0
        peak_profit_since_entry = 0.0
        last_sell_index = -999  # è®°å½•ä¸Šä¸€æ¬¡å–å‡ºçš„ç´¢å¼•
        
        # åˆ†æè‚¡ç¥¨ç‰¹å¾å¹¶åº”ç”¨è‡ªé€‚åº”å‚æ•°
        if stock_features is None:
            stock_features = StockFeatureAnalyzer.analyze_stock_features(df)
        
        # åº”ç”¨è‡ªé€‚åº”å‚æ•°è°ƒæ•´
        config = StockFeatureAnalyzer.get_adaptive_params(config, stock_features)
        
        buy_cooldown = int(config.get('buyCooldown', 0))  # ä¹°å…¥å†·å´å¤©æ•°ï¼Œé»˜è®¤0å¤©

        if 'minConditions' not in config:
            config['minConditions'] = 2
        if 'scoreThreshold' not in config:
            config['scoreThreshold'] = 60

        # è·å–å¸‚åœºçŠ¶æ€ï¼ˆç”¨äºåŠ¨æ€è°ƒæ•´ç­–ç•¥å‚æ•°ï¼‰
        market_state = 'neutral'
        if len(df) > 0 and 'market_state' in df.columns:
            market_state = df.iloc[-1].get('market_state', 'neutral')
        
        # å¸‚åœºçŠ¶æ€æƒé‡è°ƒæ•´
        market_adjustments = {
            'bull_market': {'aggressive': 1.1, 'conservative': 0.9},
            'bear_market': {'aggressive': 0.8, 'conservative': 1.2},
            'sideways_market': {'aggressive': 0.9, 'conservative': 1.1},
            'volatile_market': {'aggressive': 1.0, 'conservative': 1.0},
            'choppy_market': {'aggressive': 0.7, 'conservative': 1.3}
        }
        aggressive_factor = market_adjustments.get(market_state, {}).get('aggressive', 1.0)
        conservative_factor = market_adjustments.get(market_state, {}).get('conservative', 1.0)

        for i in range(60, len(df)):
            d = df.iloc[i]
            prev = df.iloc[i-1]
            
            if d.get('ma20', None) is None or pd.isna(d.get('ma20')) or d.get('rsi', None) is None or pd.isna(d.get('rsi')):
                continue
            
            buy_signal = False
            sell_signal = False
            signal_strength = 50
            sell_reason = ''
            
            # å†·å´æœŸæ£€æŸ¥
            in_cooldown = (i - last_sell_index) <= buy_cooldown

            # ç­–ç•¥é€»è¾‘ - ä¼˜åŒ–ç‰ˆ
            if not in_cooldown and position == 0:
                if strategy_name == 'deep_fusion':
                    # æ·±åº¦èåˆç­–ç•¥ - å‡çº§ç‰ˆï¼šå¤šæ¡ä»¶åŠ æƒè¯„åˆ† + åŠ¨æ€ç¡®è®¤æœºåˆ¶
                    score = 0
                    max_score = 0
                    confirmations = 0  # ç¡®è®¤ä¿¡å·è®¡æ•°
                    
                    # 1. MAå¯¹é½æ¡ä»¶ï¼ˆå¯é…ç½®å¼€å…³ï¼‰- æƒé‡å¢åŠ 
                    if config.get('useMa5AboveMa20', True):
                        max_score += 25
                        if d['ma5'] and d['ma20'] and d['ma5'] > d['ma20']:
                            score += 25
                            confirmations += 1
                            # å¤šå¤´æ’åˆ—åŠ åˆ†
                            if d.get('ma10') and d.get('ma20') and d['ma10'] > d['ma20']:
                                score += 5
                    
                    # 2. ä»·æ ¼ä½ç½®æ¡ä»¶ï¼ˆå¯é…ç½®å¼€å…³ï¼‰
                    if config.get('usePriceAboveMa5', True):
                        max_score += 15
                        if d['close'] > d.get('ma5', d['close']):
                            score += 15
                            confirmations += 1
                    
                    # 3. MACDåŠ¨é‡æ¡ä»¶ï¼ˆå¯é…ç½®å¼€å…³ï¼‰- å¢å¼ºåˆ¤æ–­
                    if config.get('useMacdRising', True):
                        max_score += 25
                        macd_threshold = config.get('macdThreshold', 0)
                        macd_val = d.get('macd', None)
                        prev_macd = prev.get('macd', None)
                        if macd_val is not None and not pd.isna(macd_val) and float(macd_val) > macd_threshold:
                            # MACDæŸ±çŠ¶å›¾ä¸Šå‡
                            if prev_macd is not None and not pd.isna(prev_macd) and float(macd_val) > float(prev_macd):
                                score += 20
                                confirmations += 1
                            # DIFä¸Šç©¿DEAé‡‘å‰
                            hist = d.get('macdHist', None)
                            prev_hist = prev.get('macdHist', None)
                            if hist is not None and prev_hist is not None and not pd.isna(hist) and not pd.isna(prev_hist):
                                if float(hist) > 0 and float(prev_hist) <= 0:
                                    score += 10  # é‡‘å‰åŠ åˆ†
                    
                    # 4. RSIè¶…å–æ¡ä»¶ï¼ˆå¯é…ç½®å¼€å…³ï¼‰- åŠ¨æ€é˜ˆå€¼
                    if config.get('useRsiBelow35', True):
                        max_score += 15
                        rsi_threshold = config.get('rsiThreshold', 35)
                        rsi_val = d.get('rsi', None)
                        if rsi_val is not None and not pd.isna(rsi_val) and float(rsi_val) < rsi_threshold:
                            score += 15
                            confirmations += 1
                            # ä¸¥é‡è¶…å–é¢å¤–åŠ åˆ†
                            if float(rsi_val) < 25:
                                score += 10
                    
                    # 5. æˆäº¤é‡ç¡®è®¤æ¡ä»¶ï¼ˆå¯é…ç½®å¼€å…³ï¼‰- å¢å¼ºåˆ¤æ–­
                    if config.get('useVolumeAboveMa', True):
                        max_score += 20
                        vol_multi = config.get('volumeMulti', 1.5)
                        vol_val = d.get('volume', None)
                        vol_ma5 = d.get('volMa5', None)
                        if vol_val is not None and vol_ma5 is not None and not pd.isna(vol_val) and not pd.isna(vol_ma5) and float(vol_ma5) > 0 and float(vol_val) > float(vol_ma5) * vol_multi:
                            score += 15
                            confirmations += 1
                            # æˆäº¤é‡æŒç»­æ”¾å¤§
                            vol_ma10 = d.get('volMa10', None)
                            if vol_ma10 is not None and not pd.isna(vol_ma10) and float(vol_ma5) > float(vol_ma10):
                                score += 5
                    
                    # 6. å¸ƒæ—å¸¦ä½ç½®æ¡ä»¶ï¼ˆå¯é…ç½®å¼€å…³ï¼‰- ä¼˜åŒ–åˆ¤æ–­
                    if config.get('usePriceBelowBoll', True):
                        max_score += 15
                        close_val = d.get('close', None)
                        boll_down = d.get('bollDown', None)
                        if close_val is not None and boll_down is not None and not pd.isna(close_val) and not pd.isna(boll_down) and float(close_val) < float(boll_down):
                            score += 15
                            confirmations += 1
                            # è§¦åŠä¸‹è½¨ä¸”åå¼¹
                            if d.get('open', None) is not None and not pd.isna(d.get('open')) and float(close_val) > float(d.get('open')):
                                score += 5
                    
                    # 7. æ–°å¢ï¼šADXè¶‹åŠ¿å¼ºåº¦ç¡®è®¤
                    if config.get('useAdxConfirm', True):
                        max_score += 10
                        adx_val = d.get('adx', None)
                        if adx_val is not None and not pd.isna(adx_val) and float(adx_val) > 20:
                            score += 10
                            if d.get('plus_di') and d.get('minus_di') and d['plus_di'] > d['minus_di']:
                                score += 5
                    
                    # 8. æ–°å¢ï¼šä»·æ ¼åŠ¨é‡ç¡®è®¤
                    if config.get('useMomentumConfirm', True):
                        max_score += 10
                        if i >= 5:
                            price_change_5d = (d['close'] - df.iloc[i-5]['close']) / df.iloc[i-5]['close'] * 100
                            if -10 < price_change_5d < 5:  # è¿‘æœŸæ²¡æœ‰å¤§æ¶¨å¤§è·Œ
                                score += 10
                    
                    # åŠ¨æ€é˜ˆå€¼è°ƒæ•´ - åŸºäºç¡®è®¤ä¿¡å·æ•°é‡
                    base_threshold = config.get('mlScoreThreshold', 60)
                    # ç¡®è®¤ä¿¡å·è¶Šå¤šï¼Œé˜ˆå€¼å¯ä»¥é€‚å½“é™ä½
                    confirmation_bonus = confirmations * 2
                    adjusted_threshold = (base_threshold - confirmation_bonus) * conservative_factor
                    adjusted_threshold = max(45, min(75, adjusted_threshold))  # é™åˆ¶åœ¨45-75ä¹‹é—´
                    
                    signal_strength = (score / max_score * 100) if max_score > 0 else 50
                    
                    # ä¹°å…¥æ¡ä»¶ï¼šè¾¾åˆ°é˜ˆå€¼ä¸”è‡³å°‘æœ‰3ä¸ªç¡®è®¤ä¿¡å·
                    min_confirmations = config.get('minConfirmations', 3)
                    if signal_strength >= adjusted_threshold and confirmations >= min_confirmations:
                        buy_signal = True
                        # æ ¹æ®ç¡®è®¤ä¿¡å·æ•°é‡è°ƒæ•´ä¿¡å·å¼ºåº¦
                        signal_strength = min(95, signal_strength + confirmations * 3)
                    
                elif strategy_name == 'volume_breakout':
                    # é‡ä»·çªç ´ç­–ç•¥ - å‡çº§ç‰ˆï¼šå¤šç»´åº¦çªç ´ç¡®è®¤
                    conditions_met = 0
                    total_conditions = 0
                    breakout_score = 0
                    
                    # 1. ä»·æ ¼çªç ´æ¡ä»¶ï¼ˆæ ¸å¿ƒï¼‰- å¢å¼ºåˆ¤æ–­
                    if config.get('usePriceBreakout', True):
                        total_conditions += 1
                        breakout_period = config.get('breakoutPeriod', 20)
                        high_col = f'high{breakout_period}'
                        
                        if d.get(high_col) and d['close'] > d[high_col] * 0.995:  # å…è®¸0.5%çš„è¯¯å·®
                            conditions_met += 1
                            breakout_score += 30
                            # åˆ›è¿‘æœŸæ–°é«˜åŠ åˆ†
                            if d['close'] > d.get('high60', d['close']):
                                breakout_score += 10
                    
                    # 2. æˆäº¤é‡æ”¾å¤§æ¡ä»¶ï¼ˆæ ¸å¿ƒï¼‰- åŠ¨æ€é˜ˆå€¼
                    if config.get('useVolumeUp', True):
                        total_conditions += 1
                        vol_multi = config.get('volumeMulti', 2.0)
                        vol_ratio = d['volume'] / d['volMa5'] if d.get('volMa5') and d['volMa5'] > 0 else 0
                        
                        if vol_ratio > vol_multi:
                            conditions_met += 1
                            breakout_score += 25
                            # æˆäº¤é‡æŒç»­æ”¾å¤§
                            if d.get('volMa5') and d.get('volMa10') and d['volMa5'] > d['volMa10'] * 1.2:
                                breakout_score += 10
                            # å¼‚å¸¸æ”¾é‡è¿‡æ»¤ï¼ˆé¿å…å‡çªç ´ï¼‰
                            if vol_ratio < 5:  # æˆäº¤é‡ä¸è¶…è¿‡5å€ï¼Œé¿å…æç«¯æƒ…å†µ
                                breakout_score += 5
                    
                    # 3. è¶‹åŠ¿ç¡®è®¤æ¡ä»¶ - ç¡®ä¿çªç ´æ–¹å‘ä¸è¶‹åŠ¿ä¸€è‡´
                    if config.get('useTrendConfirm', True):
                        total_conditions += 1
                        if d.get('ma5') and d.get('ma20') and d['ma5'] > d['ma20']:
                            conditions_met += 1
                            breakout_score += 15
                            # å¤šå¤´æ’åˆ—åŠ åˆ†
                            if d.get('ma10') and d['ma10'] > d['ma20']:
                                breakout_score += 5
                    
                    # 4. æ³¢åŠ¨ç‡ç¡®è®¤ - é¿å…åœ¨éœ‡è¡å¸‚ä¸­äº¤æ˜“
                    if config.get('useVolatilityConfirm', True):
                        total_conditions += 1
                        if d.get('bollWidth') and d['bollWidth'] > 0.05:  # å¸ƒæ—å¸¦å®½åº¦è¶³å¤Ÿ
                            conditions_met += 1
                            breakout_score += 10
                    
                    # 5. åŠ¨èƒ½ç¡®è®¤ - RSIä¸è¿‡ä¹°
                    if config.get('useMomentumConfirm', True):
                        total_conditions += 1
                        rsi_max = config.get('rsiMax', 70)
                        if d.get('rsi') and d['rsi'] < rsi_max:
                            conditions_met += 1
                            breakout_score += 10
                    
                    # 6. é˜³çº¿ç¡®è®¤ï¼ˆåŠ åˆ†é¡¹ï¼‰
                    if config.get('useBullishCandle', True):
                        body_pct = abs(d['close'] - d['open']) / d['open'] * 100 if d['open'] > 0 else 0
                        if d['close'] > d['open']:  # é˜³çº¿
                            breakout_score += 10
                            if body_pct > 2:  # å¤§é˜³çº¿é¢å¤–åŠ åˆ†
                                breakout_score += 5
                        # ä¸Šå½±çº¿ä¸èƒ½å¤ªé•¿
                        upper_shadow = (d['high'] - max(d['close'], d['open'])) / d['open'] * 100 if d['open'] > 0 else 0
                        if upper_shadow < 2:
                            breakout_score += 5
                    
                    # 7. æ–°å¢ï¼šç­¹ç åˆ†å¸ƒç¡®è®¤ - çªç ´é˜»åŠ›ä½
                    if config.get('useChipConfirm', True):
                        if d.get('price_position_20') and d['price_position_20'] > 70:
                            breakout_score += 10
                    
                    # åŠ¨æ€ä¹°å…¥æ¡ä»¶
                    min_conditions = config.get('minConditions', 3)
                    score_threshold = config.get('scoreThreshold', 70)
                    
                    if conditions_met >= min_conditions and breakout_score >= score_threshold:
                        buy_signal = True
                        signal_strength = min(95, breakout_score)
                    
                elif strategy_name == 'oversold_rebound':
                    # è¶…è·Œåå¼¹ç­–ç•¥ - å‡çº§ç‰ˆï¼šå¤šç»´åº¦è¶…å–ç¡®è®¤ + åå¼¹ä¿¡å·æ£€æµ‹
                    conditions_met = 0
                    total_conditions = 0
                    rebound_score = 0
                    
                    # 1. RSIè¶…å–æ¡ä»¶ï¼ˆæ ¸å¿ƒï¼‰- åŠ¨æ€é˜ˆå€¼
                    if config.get('useRsiOversold', True):
                        total_conditions += 1
                        rsi_threshold = config.get('rsiOversold', 30)
                        if d.get('rsi') and d['rsi'] < rsi_threshold:
                            conditions_met += 1
                            rebound_score += 25
                            # ä¸¥é‡è¶…å–åŠ åˆ†
                            if d['rsi'] < 20:
                                rebound_score += 10
                            # RSIå¼€å§‹å›å‡ï¼ˆåº•èƒŒç¦»è¿¹è±¡ï¼‰
                            if prev.get('rsi') and d['rsi'] > prev['rsi']:
                                rebound_score += 10
                    
                    # 2. è§¦åŠå¸ƒæ—ä¸‹è½¨ï¼ˆå¯é…ç½®å¼€å…³ï¼‰- å¢å¼ºåˆ¤æ–­
                    if config.get('useTouchLowerBoll', True):
                        total_conditions += 1
                        boll_offset = config.get('bollingerOffset', 0.02)
                        if d.get('bollDown') and d['close'] < (d['bollDown'] * (1 + boll_offset)):
                            conditions_met += 1
                            rebound_score += 20
                            # è§¦åŠä¸‹è½¨ä¸”æ”¶é˜³çº¿
                            if d['close'] > d['open']:
                                rebound_score += 10
                    
                    # 3. ä»·æ ¼ä½ç½®æ¡ä»¶ - æ¥è¿‘è¿‘æœŸä½ç‚¹
                    if config.get('useNearLow', True):
                        total_conditions += 1
                        near_low_ratio = config.get('nearLowRatio', 1.03)
                        if d.get('low20') and d['close'] < d['low20'] * near_low_ratio:
                            conditions_met += 1
                            rebound_score += 15
                    
                    # 4. æˆäº¤é‡æ¡ä»¶ - ç¼©é‡åæ”¾é‡
                    if config.get('useVolumePattern', True):
                        total_conditions += 1
                        # ç¼©é‡ï¼ˆåœ°é‡ï¼‰
                        vol_ratio = d['volume'] / d['volMa5'] if d.get('volMa5') and d['volMa5'] > 0 else 1
                        if vol_ratio < 0.8:  # ç¼©é‡
                            rebound_score += 10
                            conditions_met += 1
                        # æˆ–è€…æ”¾é‡åå¼¹
                        elif d['close'] > d['open'] and vol_ratio > 1.2:
                            rebound_score += 15
                            conditions_met += 1
                    
                    # 5. Kçº¿å½¢æ€ç¡®è®¤ - çœ‹æ¶¨åè½¬å½¢æ€
                    if config.get('useCandlePattern', True):
                        body = d['close'] - d['open']
                        lower_shadow = d['open'] - d['low'] if body > 0 else d['close'] - d['low']
                        upper_shadow = d['high'] - d['close'] if body > 0 else d['high'] - d['open']
                        body_size = abs(body)
                        
                        # é”¤å­çº¿å½¢æ€
                        if lower_shadow > body_size * 2 and upper_shadow < body_size * 0.5:
                            rebound_score += 15
                        # å¯æ˜æ˜Ÿå½¢æ€ï¼ˆç®€åŒ–ç‰ˆï¼‰
                        if i >= 2:
                            prev2 = df.iloc[i-2]
                            if prev2['close'] < prev2['open']:  # å‰ä¸€å¤©é˜´çº¿
                                if body > 0 and d['close'] > (prev2['open'] + prev2['close']) / 2:
                                    rebound_score += 10
                    
                    # 6. MACDåº•èƒŒç¦»æ£€æµ‹
                    if config.get('useMacdDivergence', True):
                        if d.get('macd') and prev.get('macd'):
                            # ä»·æ ¼åˆ›æ–°ä½ä½†MACDæœªåˆ›æ–°ä½
                            if i >= 5:
                                price_low_now = d['close'] < df.iloc[i-5:i]['close'].min()
                                price_low_prev = df.iloc[i-5]['close'] < df.iloc[i-10:i-5]['close'].min() if i >= 10 else False
                                macd_now = d['macd']
                                macd_prev = df.iloc[i-5]['macd'] if i >= 5 else 0
                                
                                if price_low_now and not price_low_prev and macd_now > macd_prev:
                                    rebound_score += 20
                                    conditions_met += 1
                    
                    # 7. è¶‹åŠ¿è¿‡æ»¤ - é¿å…åœ¨å¼ºä¸‹è·Œè¶‹åŠ¿ä¸­ä¹°å…¥
                    if config.get('useTrendFilter', True):
                        if d.get('adx') and d['adx'] < 30:  # è¶‹åŠ¿ä¸å¼º
                            rebound_score += 10
                        # æˆ–è€…è¶‹åŠ¿å¼€å§‹è½¬å¼±
                        if d.get('minus_di') and d.get('plus_di') and d['minus_di'] < prev.get('minus_di', 100):
                            rebound_score += 5
                    
                    # åŠ¨æ€ä¹°å…¥æ¡ä»¶
                    min_conditions = config.get('minConditions', 3)
                    score_threshold = config.get('scoreThreshold', 65)
                    
                    if conditions_met >= min_conditions and rebound_score >= score_threshold:
                        buy_signal = True
                        signal_strength = min(95, rebound_score)
                    
                elif strategy_name == 'trend_enhanced':
                    # è¶‹åŠ¿å¢å¼ºç­–ç•¥ - å‡çº§ç‰ˆï¼šå¤šæ—¶é—´æ¡†æ¶è¶‹åŠ¿ç¡®è®¤
                    conditions_met = 0
                    total_conditions = 0
                    trend_score = 0
                    
                    # 1. çŸ­æœŸå‡çº¿å¤šå¤´æ’åˆ—ï¼ˆå¯é…ç½®å¼€å…³ï¼‰
                    if config.get('useShortMaAlignment', True):
                        total_conditions += 1
                        if d.get('ma5') and d.get('ma10') and d.get('ma20'):
                            if d['ma5'] > d['ma10'] > d['ma20']:
                                conditions_met += 1
                                trend_score += 25
                                # å®Œå…¨å¤šå¤´æ’åˆ—åŠ åˆ†
                                if d.get('ma60') and d['ma20'] > d['ma60']:
                                    trend_score += 10
                    
                    # 2. ä»·æ ¼ç›¸å¯¹å‡çº¿ä½ç½®ï¼ˆå¯é…ç½®å¼€å…³ï¼‰
                    if config.get('usePriceAboveMa', True):
                        total_conditions += 1
                        price_above_count = 0
                        for ma in ['ma5', 'ma10', 'ma20']:
                            if d.get(ma) and d['close'] > d[ma]:
                                price_above_count += 1
                        if price_above_count >= 2:
                            conditions_met += 1
                            trend_score += 15 + price_above_count * 3
                    
                    # 3. MACDæ­£å‘ä¸”åŠ¨èƒ½å¢å¼ºï¼ˆå¯é…ç½®å¼€å…³ï¼‰
                    if config.get('useMacdPositive', True):
                        total_conditions += 1
                        macd_threshold = config.get('macdThreshold', 0)
                        macd_val = d.get('macd', None)
                        if macd_val is not None and not pd.isna(macd_val) and float(macd_val) > macd_threshold:
                            conditions_met += 1
                            trend_score += 15
                            # MACDæŸ±çŠ¶å›¾æ‰©å¤§
                            hist = d.get('macdHist', None)
                            prev_hist = prev.get('macdHist', None)
                            if hist is not None and prev_hist is not None and not pd.isna(hist) and not pd.isna(prev_hist) and float(hist) > float(prev_hist):
                                trend_score += 10
                            # DIFä¸Šç©¿DEAé‡‘å‰
                            if hist is not None and prev_hist is not None and not pd.isna(hist) and not pd.isna(prev_hist):
                                if float(hist) > 0 and float(prev_hist) <= 0:
                                    trend_score += 15
                    
                    # 4. ADXè¶‹åŠ¿å¼ºåº¦ç¡®è®¤ï¼ˆæ–°å¢ï¼‰
                    if config.get('useAdxConfirm', True):
                        total_conditions += 1
                        if d.get('adx'):
                            if d['adx'] > 25:  # å¼ºè¶‹åŠ¿
                                conditions_met += 1
                                trend_score += 15
                                if d['adx'] > 40:  # æå¼ºè¶‹åŠ¿
                                    trend_score += 10
                            # è¶‹åŠ¿æ–¹å‘ç¡®è®¤
                            if d.get('plus_di') and d.get('minus_di') and d['plus_di'] > d['minus_di']:
                                trend_score += 10
                    
                    # 5. æˆäº¤é‡è¶‹åŠ¿ç¡®è®¤ï¼ˆå¯é…ç½®å¼€å…³ï¼‰
                    if config.get('useVolumeConfirm', True):
                        total_conditions += 1
                        if d.get('volMa5') and d.get('volMa10'):
                            if d['volMa5'] > d['volMa10']:  # æˆäº¤é‡å‡çº¿ä¸Šå‡
                                conditions_met += 1
                                trend_score += 10
                            # é‡ä»·é…åˆ
                            if d['volume'] > d['volMa5'] and d['close'] > d['open']:
                                trend_score += 5
                    
                    # 6. å¸ƒæ—å¸¦è¶‹åŠ¿ç¡®è®¤ï¼ˆå¯é…ç½®å¼€å…³ï¼‰
                    if config.get('useBollConfirm', True):
                        # ä»·æ ¼åœ¨ä¸­è½¨ä¸Šæ–¹
                        if d.get('bollMid') and d['close'] > d['bollMid']:
                            trend_score += 10
                        # å¸ƒæ—å¸¦å¼€å£å‘ä¸Š
                        if d.get('bollWidth') and prev.get('bollWidth') and d['bollWidth'] > prev['bollWidth']:
                            trend_score += 5
                    
                    # 7. ä»·æ ¼åŠ¨é‡ç¡®è®¤ï¼ˆå¯é…ç½®å¼€å…³ï¼‰
                    if config.get('useMomentumConfirm', True):
                        if i >= 10:
                            price_change_10d = (d['close'] - df.iloc[i-10]['close']) / df.iloc[i-10]['close'] * 100
                            if price_change_10d > 5:  # 10æ—¥æ¶¨å¹…è¶…è¿‡5%
                                trend_score += 10
                            if price_change_10d > 10:  # 10æ—¥æ¶¨å¹…è¶…è¿‡10%
                                trend_score += 5
                    
                    # 8. è¶‹åŠ¿æŒç»­æ€§æ£€æŸ¥ - é¿å…å‡çªç ´
                    if config.get('useTrendPersistence', True):
                        if i >= 5:
                            # æ£€æŸ¥è¿‡å»5å¤©æ˜¯å¦æ•´ä½“å‘ä¸Š
                            recent_prices = [df.iloc[j]['close'] for j in range(i-4, i+1)]
                            higher_count = sum(1 for j in range(1, len(recent_prices)) if recent_prices[j] > recent_prices[j-1])
                            if higher_count >= 3:  # è‡³å°‘3å¤©ä¸Šæ¶¨
                                trend_score += 10
                    
                    # åŠ¨æ€ä¹°å…¥æ¡ä»¶
                    min_conditions = config.get('minConditions', 3)
                    score_threshold = config.get('scoreThreshold', 70)
                    
                    if conditions_met >= min_conditions and trend_score >= score_threshold:
                        buy_signal = True
                        signal_strength = min(95, trend_score)
                    
                elif strategy_name == 'macd_divergence':
                    # MACDèƒŒç¦»ç­–ç•¥ - å‡çº§ç‰ˆï¼šå¤šå‘¨æœŸèƒŒç¦»æ£€æµ‹ + ç¡®è®¤æœºåˆ¶
                    divergence_score = 0
                    conditions_met = 0
                    
                    # éœ€è¦è¶³å¤Ÿçš„å†å²æ•°æ®
                    lookback_period = config.get('lookbackPeriod', 30)
                    if i < lookback_period * 2:
                        continue
                    
                    # 1. åº•èƒŒç¦»æ£€æµ‹ï¼ˆæ ¸å¿ƒï¼‰- ä»·æ ¼åˆ›æ–°ä½ä½†MACDæœªåˆ›æ–°ä½
                    if config.get('useBullishDivergence', True):
                        # å¯»æ‰¾è¿‘æœŸä½ç‚¹
                        recent_lows_price = []
                        recent_lows_macd = []
                        
                        for j in range(i - lookback_period, i + 1):
                            if j < 2:
                                continue
                            # å±€éƒ¨ä½ç‚¹åˆ¤æ–­
                            if (df.iloc[j]['close'] < df.iloc[j-1]['close'] and 
                                df.iloc[j]['close'] < df.iloc[j-2]['close'] and
                                df.iloc[j]['close'] < df.iloc[j+1]['close'] if j < len(df) - 1 else True):
                                recent_lows_price.append((j, df.iloc[j]['close'], df.iloc[j]['macd']))
                        
                        # æ£€æµ‹èƒŒç¦»
                        if len(recent_lows_price) >= 2:
                            # æ¯”è¾ƒæœ€è¿‘ä¸¤ä¸ªä½ç‚¹
                            latest_low = recent_lows_price[-1]
                            prev_low = recent_lows_price[-2]
                            
                            # ä»·æ ¼åˆ›æ–°ä½ä½†MACDæœªåˆ›æ–°ä½ï¼ˆåº•èƒŒç¦»ï¼‰
                            if latest_low[1] < prev_low[1] and latest_low[2] > prev_low[2]:
                                conditions_met += 1
                                divergence_score += 40
                                # MACDåœ¨é›¶è½´é™„è¿‘æˆ–ä¸‹æ–¹ï¼ˆæ›´å®‰å…¨ï¼‰
                                if latest_low[2] < 0.5:
                                    divergence_score += 10
                    
                    # 2. MACDæŸ±çŠ¶å›¾èƒŒç¦»æ£€æµ‹
                    if config.get('useHistogramDivergence', True):
                        if i >= 10:
                            price_change = (d['close'] - df.iloc[i-10]['close']) / df.iloc[i-10]['close'] * 100
                            hist_change = d.get('macdHist', 0) - df.iloc[i-10].get('macdHist', 0)
                            
                            # ä»·æ ¼è·Œä½†æŸ±çŠ¶å›¾ä¸Šå‡
                            if price_change < -5 and hist_change > 0:
                                conditions_met += 1
                                divergence_score += 20
                    
                    # 3. æˆäº¤é‡èƒŒç¦»ç¡®è®¤
                    if config.get('useVolumeDivergence', True):
                        if i >= 10:
                            price_low_now = d['close'] < df.iloc[i-10:i]['close'].min()
                            vol_now = d['volume']
                            vol_prev_low = df.iloc[i-5]['volume'] if i >= 5 else vol_now
                            
                            # ä»·æ ¼æ–°ä½ä½†æˆäº¤é‡èç¼©ï¼ˆå–å‹å‡è½»ï¼‰
                            if price_low_now and vol_now < vol_prev_low * 0.8:
                                divergence_score += 15
                                conditions_met += 1
                    
                    # 4. RSIèƒŒç¦»ç¡®è®¤
                    if config.get('useRsiDivergence', True):
                        if i >= lookback_period:
                            recent_lows_rsi = []
                            for j in range(i - lookback_period, i + 1):
                                if j < 2:
                                    continue
                                if df.iloc[j]['rsi'] < 40:  # è¶…å–åŒºåŸŸ
                                    if (df.iloc[j]['rsi'] < df.iloc[j-1]['rsi'] and 
                                        df.iloc[j]['rsi'] < df.iloc[j+1]['rsi'] if j < len(df) - 1 else True):
                                        recent_lows_rsi.append((j, df.iloc[j]['close'], df.iloc[j]['rsi']))
                            
                            if len(recent_lows_rsi) >= 2:
                                latest_rsi_low = recent_lows_rsi[-1]
                                prev_rsi_low = recent_lows_rsi[-2]
                                
                                # ä»·æ ¼åˆ›æ–°ä½ä½†RSIæœªåˆ›æ–°ä½
                                if latest_rsi_low[1] < prev_rsi_low[1] and latest_rsi_low[2] > prev_rsi_low[2]:
                                    divergence_score += 20
                                    conditions_met += 1
                    
                    # 5. è¶‹åŠ¿å¼ºåº¦è¿‡æ»¤ - é¿å…åœ¨æå¼ºä¸‹è·Œè¶‹åŠ¿ä¸­äº¤æ˜“
                    if config.get('useTrendFilter', True):
                        if d.get('adx'):
                            if d['adx'] < 35:  # è¶‹åŠ¿ä¸æ˜¯æå¼º
                                divergence_score += 10
                            # è¶‹åŠ¿å¼€å§‹å‡å¼±
                            if prev.get('adx') and d['adx'] < prev['adx']:
                                divergence_score += 5
                    
                    # 6. Kçº¿å½¢æ€ç¡®è®¤ - çœ‹æ¶¨åè½¬ä¿¡å·
                    if config.get('useCandleConfirm', True):
                        body = d['close'] - d['open']
                        lower_shadow = d['open'] - d['low'] if body > 0 else d['close'] - d['low']
                        body_size = abs(body)
                        
                        # é”¤å­çº¿æˆ–ä¸‹å½±çº¿è¾ƒé•¿
                        if lower_shadow > body_size * 1.5:
                            divergence_score += 10
                        # é˜³çº¿æ”¶ç›˜
                        if d['close'] > d['open']:
                            divergence_score += 5
                    
                    # 7. å‡çº¿ç³»ç»Ÿç¡®è®¤
                    if config.get('useMaConfirm', True):
                        # ä»·æ ¼æ¥è¿‘é‡è¦å‡çº¿æ”¯æ’‘
                        if d.get('ma60') and d['close'] > d['ma60'] * 0.95:
                            divergence_score += 10
                        # çŸ­æœŸå‡çº¿èµ°å¹³æˆ–å‘ä¸Š
                        if d.get('ma5') and prev.get('ma5') and d['ma5'] >= prev['ma5']:
                            divergence_score += 5
                    
                    # åŠ¨æ€ä¹°å…¥æ¡ä»¶
                    min_conditions = config.get('minConditions', 2)
                    score_threshold = config.get('scoreThreshold', 60)
                    
                    if conditions_met >= min_conditions and divergence_score >= score_threshold:
                        buy_signal = True
                        signal_strength = min(95, divergence_score)
                
                elif strategy_name == 'bollinger_extreme':
                    # å¸ƒæ—æé™ç­–ç•¥ - å‡çº§ç‰ˆï¼šå¸ƒæ—å¸¦æå€¼äº¤æ˜“ + åè½¬ç¡®è®¤
                    boll_score = 0
                    conditions_met = 0
                    total_conditions = 0
                    
                    # 1. å¸ƒæ—å¸¦ä¸‹è½¨è§¦åŠï¼ˆå¯é…ç½®å¼€å…³ï¼‰- ä¹°å…¥ä¿¡å·
                    if config.get('useLowerBandTouch', True):
                        total_conditions += 1
                        boll_deviation = config.get('bollDeviation', 2.0)
                        
                        # ä»·æ ¼è§¦åŠæˆ–è·Œç ´ä¸‹è½¨
                        if d.get('bollDown') and d['close'] <= d['bollDown'] * (1 + 0.01):
                            conditions_met += 1
                            boll_score += 30
                            
                            # è®¡ç®—å¸ƒæ—å¸¦ç™¾åˆ†ä½ä½ç½®
                            if d.get('bollUp') and d.get('bollDown') and d['bollUp'] != d['bollDown']:
                                boll_percent = (d['close'] - d['bollDown']) / (d['bollUp'] - d['bollDown']) * 100
                                if boll_percent < 5:  # æ¥è¿‘æœ€åº•éƒ¨
                                    boll_score += 15
                            
                            # ä»ä¸‹è½¨åå¼¹
                            if d['close'] > d['open']:
                                boll_score += 10
                    
                    # 2. å¸ƒæ—å¸¦å®½åº¦ç¡®è®¤ - é¿å…åœ¨æåº¦æ”¶ç¼©æ—¶äº¤æ˜“
                    if config.get('useBandWidthConfirm', True):
                        if d.get('bollWidth'):
                            # å¸ƒæ—å¸¦å®½åº¦é€‚ä¸­ï¼ˆæœ‰æ³¢åŠ¨ä½†ä¸è¿‡åº¦ï¼‰
                            if 0.05 < d['bollWidth'] < 0.25:
                                boll_score += 10
                            # å¸ƒæ—å¸¦ä»æ”¶ç¼©å¼€å§‹æ‰©å¼ ï¼ˆæ³¢åŠ¨ç‡çªç ´ï¼‰
                            if prev.get('bollWidth') and d['bollWidth'] > prev['bollWidth'] * 1.1:
                                boll_score += 10
                                conditions_met += 1
                    
                    # 3. RSIè¶…å–ç¡®è®¤
                    if config.get('useRsiConfirm', True):
                        total_conditions += 1
                        rsi_oversold = config.get('rsiOversold', 30)
                        if d.get('rsi') and d['rsi'] < rsi_oversold:
                            conditions_met += 1
                            boll_score += 20
                            if d['rsi'] < 20:  # ä¸¥é‡è¶…å–
                                boll_score += 10
                            # RSIå¼€å§‹å›å‡
                            if prev.get('rsi') and d['rsi'] > prev['rsi']:
                                boll_score += 5
                    
                    # 4. æˆäº¤é‡ç¡®è®¤ - åœ°é‡æˆ–æ”¾é‡åå¼¹
                    if config.get('useVolumeConfirm', True):
                        total_conditions += 1
                        vol_ratio = d['volume'] / d['volMa5'] if d.get('volMa5') and d['volMa5'] > 0 else 1
                        
                        # åœ°é‡ï¼ˆç¼©é‡è§åº•ï¼‰
                        if vol_ratio < 0.7:
                            conditions_met += 1
                            boll_score += 15
                        # æˆ–è€…æ”¾é‡åå¼¹
                        elif d['close'] > d['open'] and vol_ratio > 1.3:
                            conditions_met += 1
                            boll_score += 15
                    
                    # 5. å‡çº¿æ”¯æ’‘ç¡®è®¤
                    if config.get('useMaSupport', True):
                        # ä»·æ ¼æ¥è¿‘é•¿æœŸå‡çº¿æ”¯æ’‘
                        if d.get('ma60') and d['close'] > d['ma60'] * 0.97:
                            boll_score += 10
                        # çŸ­æœŸå‡çº¿èµ°å¹³
                        if d.get('ma5') and prev.get('ma5') and abs(d['ma5'] - prev['ma5']) / d['ma5'] < 0.005:
                            boll_score += 5
                    
                    # 6. Kçº¿å½¢æ€ç¡®è®¤
                    if config.get('useCandleConfirm', True):
                        body = d['close'] - d['open']
                        lower_shadow = d['open'] - d['low'] if body > 0 else d['close'] - d['low']
                        body_size = abs(body)
                        
                        # ä¸‹å½±çº¿è¾ƒé•¿ï¼ˆæ”¯æ’‘æ˜æ˜¾ï¼‰
                        if lower_shadow > body_size * 1.5:
                            boll_score += 15
                        # é˜³çº¿æ”¶ç›˜
                        if d['close'] > d['open']:
                            boll_score += 5
                        # æ”¶ç›˜ä»·æ¥è¿‘æœ€é«˜ä»·
                        if d['high'] > d['low']:
                            close_position = (d['close'] - d['low']) / (d['high'] - d['low'])
                            if close_position > 0.7:
                                boll_score += 10
                    
                    # 7. æ³¢åŠ¨ç‡è°ƒæ•´
                    if config.get('useVolatilityAdjust', True):
                        if d.get('atr_pct'):
                            # æ ¹æ®ATRè°ƒæ•´è¯„åˆ†
                            if d['atr_pct'] > 3:  # é«˜æ³¢åŠ¨
                                boll_score *= 1.1  # é«˜åˆ†è‚¡ç¥¨å¯èƒ½æœ‰å¤§åå¼¹
                            elif d['atr_pct'] < 1:  # ä½æ³¢åŠ¨
                                boll_score *= 0.9  # ä½æ³¢åŠ¨è‚¡ç¥¨åå¼¹å¯èƒ½è¾ƒå°
                    
                    # åŠ¨æ€ä¹°å…¥æ¡ä»¶
                    min_conditions = config.get('minConditions', 2)
                    score_threshold = config.get('scoreThreshold', 60)
                    
                    if conditions_met >= min_conditions and boll_score >= score_threshold:
                        buy_signal = True
                        signal_strength = min(95, int(boll_score))
                
                elif strategy_name == 'momentum_rotation':
                    # åŠ¨é‡è½®åŠ¨ç­–ç•¥ - å‡çº§ç‰ˆï¼šå¤šå‘¨æœŸåŠ¨é‡ç¡®è®¤ + ç›¸å¯¹å¼ºåº¦åˆ†æ
                    momentum_score = 0
                    conditions_met = 0
                    
                    # 1. çŸ­æœŸåŠ¨é‡ï¼ˆå¯é…ç½®å¼€å…³ï¼‰
                    if config.get('useShortMomentum', True):
                        short_period = int(config.get('shortMomentumPeriod', 10))
                        if i >= short_period:
                            short_momentum = (d['close'] - df.iloc[i-short_period]['close']) / df.iloc[i-short_period]['close'] * 100
                            short_threshold = config.get('shortMomentumThreshold', 3.0)
                            
                            if short_momentum > short_threshold:
                                conditions_met += 1
                                momentum_score += 20
                                if short_momentum > short_threshold * 1.5:
                                    momentum_score += 10
                    
                    # 2. ä¸­æœŸåŠ¨é‡ï¼ˆå¯é…ç½®å¼€å…³ï¼‰
                    if config.get('useMediumMomentum', True):
                        medium_period = int(config.get('mediumMomentumPeriod', 20))
                        if i >= medium_period:
                            medium_momentum = (d['close'] - df.iloc[i-medium_period]['close']) / df.iloc[i-medium_period]['close'] * 100
                            medium_threshold = config.get('mediumMomentumThreshold', 5.0)
                            
                            if medium_momentum > medium_threshold:
                                conditions_met += 1
                                momentum_score += 25
                                # åŠ¨é‡åŠ é€Ÿ
                                if i >= medium_period + 5:
                                    prev_momentum = (df.iloc[i-5]['close'] - df.iloc[i-medium_period-5]['close']) / df.iloc[i-medium_period-5]['close'] * 100
                                    if medium_momentum > prev_momentum:
                                        momentum_score += 10
                    
                    # 3. é•¿æœŸåŠ¨é‡è¶‹åŠ¿ï¼ˆå¯é…ç½®å¼€å…³ï¼‰
                    if config.get('useLongMomentum', True):
                        long_period = int(config.get('longMomentumPeriod', 60))
                        if i >= long_period:
                            long_momentum = (d['close'] - df.iloc[i-long_period]['close']) / df.iloc[i-long_period]['close'] * 100
                            
                            if long_momentum > 0:  # é•¿æœŸè¶‹åŠ¿å‘ä¸Š
                                momentum_score += 15
                                if long_momentum > 10:
                                    momentum_score += 5
                    
                    # 4. ç›¸å¯¹å¼ºåº¦ - ä¸å¤§ç›˜æ¯”è¾ƒï¼ˆç®€åŒ–ç‰ˆï¼Œä½¿ç”¨å‡çº¿ï¼‰
                    if config.get('useRelativeStrength', True):
                        if d.get('ma5') and d.get('ma20') and d.get('ma60'):
                            # ä»·æ ¼ç›¸å¯¹å‡çº¿çš„ä½ç½®
                            price_vs_ma20 = (d['close'] - d['ma20']) / d['ma20'] * 100
                            ma20_vs_ma60 = (d['ma20'] - d['ma60']) / d['ma60'] * 100
                            
                            if price_vs_ma20 > 0 and ma20_vs_ma60 > 0:
                                momentum_score += 15
                                if price_vs_ma20 > 5:
                                    momentum_score += 5
                    
                    # 5. æˆäº¤é‡ç¡®è®¤ï¼ˆå¯é…ç½®å¼€å…³ï¼‰
                    if config.get('useVolumeConfirm', True):
                        vol_multi = float(config.get('volumeMulti', 1.5))
                        vol_ratio = d['volume'] / d['volMa5'] if d.get('volMa5') and d['volMa5'] > 0 else 1
                        
                        if vol_ratio > vol_multi:
                            conditions_met += 1
                            momentum_score += 15
                            # æˆäº¤é‡æŒç»­æ”¾å¤§
                            if d.get('volMa5') and d.get('volMa10') and d['volMa5'] > d['volMa10']:
                                momentum_score += 5
                        # é‡ä»·é…åˆ
                        if d['close'] > d['open'] and vol_ratio > 1:
                            momentum_score += 5
                    
                    # 6. åŠ¨é‡è´¨é‡ - é¿å…è¿‡åº¦å»¶ä¼¸
                    if config.get('useMomentumQuality', True):
                        if i >= 20:
                            recent_returns = []
                            for j in range(i-19, i+1):
                                if j > 0:
                                    daily_return = (df.iloc[j]['close'] - df.iloc[j-1]['close']) / df.iloc[j-1]['close'] * 100
                                    recent_returns.append(daily_return)
                            
                            if recent_returns:
                                avg_return = sum(recent_returns) / len(recent_returns)
                                max_return = max(recent_returns)
                                
                                # å¹³å‡æ”¶ç›Šä¸ºæ­£ä¸”æ²¡æœ‰æç«¯å¤§æ¶¨
                                if avg_return > 0.3 and max_return < 10:
                                    momentum_score += 10
                                # è¿ç»­ä¸Šæ¶¨å¤©æ•°
                                up_days = sum(1 for r in recent_returns if r > 0)
                                if up_days >= 12:  # 20å¤©ä¸­è‡³å°‘12å¤©ä¸Šæ¶¨
                                    momentum_score += 5
                    
                    # 7. æ³¢åŠ¨ç‡è°ƒæ•´ - åŠ¨é‡ä¸æ³¢åŠ¨ç‡åŒ¹é…
                    if config.get('useVolatilityAdjust', True):
                        if d.get('atr_pct'):
                            # é€‚ä¸­çš„æ³¢åŠ¨ç‡æœ‰åˆ©äºåŠ¨é‡å»¶ç»­
                            if 1.5 < d['atr_pct'] < 4:
                                momentum_score += 10
                            elif d['atr_pct'] > 5:  # æ³¢åŠ¨è¿‡å¤§ï¼ŒåŠ¨é‡å¯èƒ½ä¸ç¨³å®š
                                momentum_score -= 5
                    
                    # 8. MACDåŠ¨é‡ç¡®è®¤
                    if config.get('useMacdConfirm', True):
                        macd_val = d.get('macd', None)
                        if macd_val is not None and not pd.isna(macd_val) and float(macd_val) > 0:
                            momentum_score += 10
                            hist = d.get('macdHist', None)
                            prev_hist = prev.get('macdHist', None)
                            if hist is not None and prev_hist is not None and not pd.isna(hist) and not pd.isna(prev_hist):
                                if float(hist) > float(prev_hist):  # åŠ¨é‡å¢å¼º
                                    momentum_score += 5
                    
                    # åŠ¨æ€ä¹°å…¥æ¡ä»¶
                    min_conditions = config.get('minConditions', 3)
                    score_threshold = config.get('scoreThreshold', 65)
                    
                    if conditions_met >= min_conditions and momentum_score >= score_threshold:
                        buy_signal = True
                        signal_strength = min(95, momentum_score)
                
                elif strategy_name == 'turtle_enhanced':
                    # æµ·é¾Ÿå¢å¼ºç­–ç•¥ - å‡çº§ç‰ˆï¼šå¤šæ—¶é—´æ¡†æ¶çªç ´ + é£é™©ç®¡ç†å¢å¼º
                    turtle_score = 0
                    conditions_met = 0
                    
                    entry_period = int(config.get('entryPeriod', 20))
                    exit_period = int(config.get('exitPeriod', 10))
                    atr_multiplier = float(config.get('atrMultiplier', 2.0))
                    
                    if i < entry_period:
                        continue
                    
                    # 1. çªç ´å…¥åœºæ¡ä»¶ï¼ˆæ ¸å¿ƒï¼‰- å¤šå‘¨æœŸçªç ´ç¡®è®¤
                    if config.get('useBreakoutEntry', True):
                        # 20æ—¥é«˜ç‚¹çªç ´
                        if d.get('high20') and d['close'] >= d['high20'] * 0.995:
                            conditions_met += 1
                            turtle_score += 30
                            
                            # åŒæ—¶çªç ´60æ—¥é«˜ç‚¹ï¼ˆå¼ºè¶‹åŠ¿ï¼‰
                            if d.get('high60') and d['close'] >= d['high60']:
                                turtle_score += 15
                            
                            # çªç ´å¹…åº¦
                            breakout_pct = (d['close'] - d['high20']) / d['high20'] * 100 if d['high20'] > 0 else 0
                            if 0 < breakout_pct < 3:  # é€‚ä¸­çªç ´ï¼Œé¿å…è¿‡åº¦å»¶ä¼¸
                                turtle_score += 5
                    
                    # 2. è¶‹åŠ¿è¿‡æ»¤ - åªåœ¨è¶‹åŠ¿æ˜ç¡®æ—¶äº¤æ˜“
                    if config.get('useTrendFilter', True):
                        if d.get('ma20') and d.get('ma60'):
                            if d['close'] > d['ma20'] > d['ma60']:  # å¤šå¤´æ’åˆ—
                                conditions_met += 1
                                turtle_score += 15
                                if d.get('ma5') and d['ma5'] > d['ma20']:
                                    turtle_score += 5
                    
                    # 3. æ³¢åŠ¨ç‡è¿‡æ»¤ - ATRç¡®è®¤
                    if config.get('useVolatilityFilter', True):
                        if d.get('atr_pct'):
                            # é€‚ä¸­çš„æ³¢åŠ¨ç‡
                            if 1.5 < d['atr_pct'] < 5:
                                turtle_score += 10
                                conditions_met += 1
                            # æ³¢åŠ¨ç‡ä¸èƒ½è¿‡ä½ï¼ˆé¿å…æ— æ³¢åŠ¨å¸‚åœºï¼‰
                            if d['atr_pct'] > 1:
                                turtle_score += 5
                    
                    # 4. æˆäº¤é‡ç¡®è®¤ - çªç ´éœ€è¦é‡èƒ½é…åˆ
                    if config.get('useVolumeConfirm', True):
                        vol_multi = config.get('volumeMulti', 1.5)
                        vol_ratio = d['volume'] / d['volMa5'] if d.get('volMa5') and d['volMa5'] > 0 else 1
                        
                        if vol_ratio > vol_multi:
                            conditions_met += 1
                            turtle_score += 15
                            # æˆäº¤é‡æŒç»­æ”¾å¤§
                            if d.get('volMa5') and d.get('volMa10') and d['volMa5'] > d['volMa10']:
                                turtle_score += 5
                    
                    # 5. ADXè¶‹åŠ¿å¼ºåº¦ç¡®è®¤
                    if config.get('useAdxConfirm', True):
                        if d.get('adx'):
                            if d['adx'] > 25:  # è¶‹åŠ¿å¸‚åœº
                                turtle_score += 15
                                conditions_met += 1
                                if d['adx'] > 35:  # å¼ºè¶‹åŠ¿
                                    turtle_score += 10
                            # è¶‹åŠ¿æ–¹å‘
                            if d.get('plus_di') and d.get('minus_di') and d['plus_di'] > d['minus_di']:
                                turtle_score += 5
                    
                    # 6. å‡çªç ´è¿‡æ»¤ - é¿å…éœ‡è¡å¸‚å‡ä¿¡å·
                    if config.get('useFalseBreakoutFilter', True):
                        if i >= entry_period + 5:
                            # æ£€æŸ¥ä¹‹å‰æ˜¯å¦æœ‰å¤šæ¬¡å‡çªç ´
                            recent_highs = [df.iloc[j]['high20'] for j in range(i-entry_period, i) if df.iloc[j].get('high20')]
                            if recent_highs:
                                avg_high20 = sum(recent_highs) / len(recent_highs)
                                # å¦‚æœå½“å‰çªç ´æ˜æ˜¾é«˜äºä¹‹å‰çš„éœ‡è¡åŒºé—´
                                if d['close'] > avg_high20 * 1.02:
                                    turtle_score += 10
                    
                    # 7. é£é™©ç®¡ç†è¯„åˆ† - åŸºäºATRçš„ä»“ä½ç®¡ç†
                    if config.get('useRiskManagement', True):
                        if d.get('atr') and d['close'] > 0:
                            risk_pct = d['atr'] / d['close'] * 100
                            if risk_pct < 3:  # é£é™©é€‚ä¸­
                                turtle_score += 10
                            elif risk_pct > 5:  # é£é™©è¿‡é«˜ï¼Œé™ä½è¯„åˆ†
                                turtle_score -= 10
                    
                    # 8. å¸‚åœºæƒ…ç»ª - é¿å…è¿‡åº¦ä¹è§‚
                    if config.get('useSentimentFilter', True):
                        if d.get('rsi'):
                            if d['rsi'] < 70:  # æœªè¿‡åº¦ä¹°å…¥
                                turtle_score += 5
                            if d['rsi'] < 60:  # è¿˜æœ‰ä¸Šæ¶¨ç©ºé—´
                                turtle_score += 5
                    
                    # åŠ¨æ€ä¹°å…¥æ¡ä»¶
                    min_conditions = config.get('minConditions', 3)
                    score_threshold = config.get('scoreThreshold', 65)
                    
                    if conditions_met >= min_conditions and turtle_score >= score_threshold:
                        buy_signal = True
                        signal_strength = min(95, turtle_score)
                        
                        # è®¡ç®—æ­¢æŸä»·ï¼ˆç”¨äºåç»­é£é™©ç®¡ç†ï¼‰
                        atr = d.get('atr', abs(d['high'] - d['low']) * 0.5)
                        stop_price = d['close'] - atr * atr_multiplier
                
                elif strategy_name == 'chip_peak_breakout':
                    # ç­¹ç çªç ´ç­–ç•¥ï¼šç­¹ç å•å³°ä½ä½ + æ”¾é‡çªç ´
                    chip_score = 0
                    conditions_met = 0
                    
                    # ç­¹ç è®¡ç®—å‚æ•°
                    chip_lookback = int(config.get('chipLookback', 120))
                    chip_bins = int(config.get('chipBins', 30))
                    chip_decay = float(config.get('chipDecay', 0.985))
                    chip_single_peak_ratio = float(config.get('chipSinglePeakRatio', 1.6))
                    chip_peak_min_pct = float(config.get('chipPeakMinPct', 0.12))
                    chip_min_concentration = float(config.get('chipMinConcentration', 0.50))
                    chip_low_pos_pct = float(config.get('chipLowPosPct', 0.35))
                    
                    if i < chip_lookback:
                        continue
                    
                    # 1. è®¡ç®—ç­¹ç åˆ†å¸ƒ
                    chip_data = df.iloc[i-chip_lookback:i+1].copy()
                    
                    # è®¡ç®—åŠ æƒæˆäº¤é‡ï¼ˆæ—¶é—´è¡°å‡ï¼‰
                    weights = [chip_decay ** (chip_lookback - j - 1) for j in range(chip_lookback)]
                    chip_data['weight'] = weights + [1.0]
                    chip_data['weighted_vol'] = chip_data['volume'] * chip_data['weight']
                    
                    # ä»·æ ¼åŒºé—´åˆ†ç®±
                    price_min = chip_data['low'].min()
                    price_max = chip_data['high'].max()
                    price_range = price_max - price_min
                    
                    if price_range <= 0:
                        continue
                    
                    # è®¡ç®—æ¯ä¸ªä»·æ ¼åŒºé—´çš„ç­¹ç é‡
                    bin_size = price_range / chip_bins
                    chip_distribution = []
                    
                    for b in range(chip_bins):
                        bin_low = price_min + b * bin_size
                        bin_high = bin_low + bin_size
                        bin_center = (bin_low + bin_high) / 2
                        
                        # è®¡ç®—è¯¥åŒºé—´å†…çš„åŠ æƒæˆäº¤é‡
                        mask = (chip_data['close'] >= bin_low) & (chip_data['close'] < bin_high)
                        vol_sum = chip_data.loc[mask, 'weighted_vol'].sum()
                        
                        chip_distribution.append({
                            'center': bin_center,
                            'low': bin_low,
                            'high': bin_high,
                            'volume': vol_sum
                        })
                    
                    # æ‰¾åˆ°ç­¹ç å³°
                    total_chip = sum(c['volume'] for c in chip_distribution)
                    if total_chip <= 0:
                        continue
                    
                    # æŒ‰ç­¹ç é‡æ’åº
                    sorted_chips = sorted(chip_distribution, key=lambda x: x['volume'], reverse=True)
                    
                    # ä¸»å³°
                    peak_chip = sorted_chips[0]
                    peak_pct = peak_chip['volume'] / total_chip
                    
                    # æ¬¡å³°
                    second_peak_pct = sorted_chips[1]['volume'] / total_chip if len(sorted_chips) > 1 else 0
                    
                    # å•å³°å¼ºåº¦
                    single_peak_strength = peak_pct / second_peak_pct if second_peak_pct > 0 else 999
                    
                    # ç­¹ç é›†ä¸­åº¦ (90%ç­¹ç æ‰€åœ¨åŒºé—´)
                    sorted_by_price = sorted(chip_distribution, key=lambda x: x['center'])
                    cumsum = 0
                    c90_low = c90_high = None
                    c70_low = c70_high = None
                    
                    for c in sorted_by_price:
                        cumsum += c['volume']
                        if c90_low is None and cumsum >= total_chip * 0.05:
                            c90_low = c['center']
                        if c70_low is None and cumsum >= total_chip * 0.15:
                            c70_low = c['center']
                        if c90_high is None and cumsum >= total_chip * 0.95:
                            c90_high = c['center']
                        if c70_high is None and cumsum >= total_chip * 0.85:
                            c70_high = c['center']
                    
                    c90_range = ((c90_high - c90_low) / price_range * 100) if c90_high and c90_low and price_range > 0 else 100
                    c70_range = ((c70_high - c70_low) / price_range * 100) if c70_high and c70_low and price_range > 0 else 100
                    concentration = 100 - c90_range
                    
                    # ä¸»å³°ä½ç½®ï¼ˆåœ¨ä»·æ ¼åŒºé—´çš„ä½ç½®ï¼‰
                    peak_position = (peak_chip['center'] - price_min) / price_range if price_range > 0 else 0.5
                    
                    # 2. ç­¹ç å•å³°æ¡ä»¶
                    if config.get('useChipSinglePeak', True):
                        if peak_pct >= chip_peak_min_pct and single_peak_strength >= chip_single_peak_ratio:
                            conditions_met += 1
                            chip_score += 25
                            # å•å³°è¶Šå¼ºï¼Œåˆ†æ•°è¶Šé«˜
                            if single_peak_strength > 2.0:
                                chip_score += 10
                    
                    # 3. ç­¹ç é›†ä¸­åº¦æ¡ä»¶
                    if config.get('useChipConcentration', True):
                        if concentration >= chip_min_concentration:
                            conditions_met += 1
                            chip_score += 20
                            if concentration > 0.60:
                                chip_score += 10
                    
                    # 4. ç­¹ç ä½ä½æ¡ä»¶
                    if config.get('useChipLowPosition', True):
                        if peak_position <= chip_low_pos_pct:
                            conditions_met += 1
                            chip_score += 25
                            # è¶Šä½ä½è¶Šå¥½
                            if peak_position < 0.25:
                                chip_score += 10
                    
                    # 5. ä»·æ ¼çªç ´æ¡ä»¶ï¼ˆæ ¸å¿ƒï¼‰
                    if config.get('usePriceBreakout', True):
                        breakout_period = int(config.get('breakoutPeriod', 30))
                        if i >= breakout_period:
                            recent_high = df.iloc[i-breakout_period:i]['high'].max()
                            if d['close'] > recent_high * 0.995:
                                conditions_met += 1
                                chip_score += 30
                                # çªç ´å¹…åº¦
                                breakout_pct = (d['close'] - recent_high) / recent_high * 100
                                if 0.5 < breakout_pct < 3:
                                    chip_score += 10
                    
                    # 6. æˆäº¤é‡æ”¾å¤§æ¡ä»¶
                    if config.get('useVolumeConfirm', True):
                        vol_multi = float(config.get('volumeMulti', 2.0))
                        vol_ratio = d['volume'] / d['volMa5'] if d.get('volMa5') and d['volMa5'] > 0 else 1
                        
                        if vol_ratio > vol_multi:
                            conditions_met += 1
                            chip_score += 20
                            if vol_ratio > vol_multi * 1.5:
                                chip_score += 10
                    
                    # 7. è¶‹åŠ¿ç¡®è®¤
                    if config.get('useTrendConfirm', True):
                        if d.get('ma20') and d.get('ma60'):
                            if d['close'] > d['ma20'] > d['ma60']:
                                chip_score += 15
                            if d.get('ma5') and d['ma5'] > d['ma20']:
                                chip_score += 10
                    
                    # 8. é¿å…è¿‡åº¦å»¶ä¼¸
                    if config.get('useOverboughtFilter', True):
                        if d.get('rsi') and d['rsi'] < 70:
                            chip_score += 10
                        if peak_position < 0.50 and d['close'] > peak_chip['high']:
                            chip_score += 10
                    
                    # åŠ¨æ€ä¹°å…¥æ¡ä»¶
                    min_conditions = config.get('minConditions', 4)
                    score_threshold = config.get('scoreThreshold', 70)
                    
                    if conditions_met >= min_conditions and chip_score >= score_threshold:
                        buy_signal = True
                        signal_strength = min(95, chip_score)
                
                elif strategy_name == 'xgboost_ml':
                    # XGBoostæœºå™¨å­¦ä¹ ç­–ç•¥ï¼šä¸“æ³¨æ•æ‰å¤§æ³¢æ®µ
                    # åˆå§‹åŒ–æ¨¡å‹ç¼“å­˜
                    if not hasattr(StrategyEngine, 'xgb_models'):
                        StrategyEngine.xgb_models = {}
                    
                    # è·å–å½“å‰è‚¡ç¥¨çš„å”¯ä¸€æ ‡è¯†
                    stock_key = f"{strategy_name}_{len(df)}"
                    
                    # è·å–å‚æ•° - é™ä½é—¨æ§›ï¼Œæé«˜æ•æ„Ÿåº¦
                    lookback = int(config.get('xgbLookback', 200))
                    horizon = int(config.get('xgbHorizon', 15))
                    retrain_interval = int(config.get('xgbRetrainInterval', 30))
                    min_prob = float(config.get('xgbMinProb', 0.55))  # é™ä½ï¼š0.65â†’0.55
                    min_accuracy = float(config.get('xgbMinAccuracy', 0.52))  # é™ä½ï¼š0.55â†’0.52
                    volume_multi = float(config.get('volumeMulti', 1.0))  # é™ä½ï¼š1.2â†’1.0
                    use_strict_filter = config.get('xgbUseStrictFilter', False)  # æ–°å¢ï¼šæ˜¯å¦ä¸¥æ ¼è¿‡æ»¤
                    
                    # æ¨¡å‹è®­ç»ƒå’Œé¢„æµ‹
                    if stock_key not in StrategyEngine.xgb_models:
                        StrategyEngine.xgb_models[stock_key] = {
                            'model': None,
                            'last_train_index': -999,
                            'train_count': 0
                        }
                    
                    model_info = StrategyEngine.xgb_models[stock_key]
                    
                    # æ£€æŸ¥æ˜¯å¦éœ€è¦é‡æ–°è®­ç»ƒ
                    need_retrain = (i - model_info['last_train_index']) >= retrain_interval
                    
                    if need_retrain and i >= lookback + horizon:
                        # å‡†å¤‡è®­ç»ƒæ•°æ®
                        train_features = []
                        train_labels = []
                        
                        # ä½¿ç”¨æ»‘åŠ¨çª—å£æ„å»ºè®­ç»ƒæ ·æœ¬
                        for j in range(lookback, i - horizon):
                            features = StrategyEngine.xgboost_extract_features(df, j)
                            if features is None:
                                continue
                            
                            # è®¡ç®—æœªæ¥æ”¶ç›Šç‡ä½œä¸ºæ ‡ç­¾ - ç®€åŒ–é€»è¾‘ï¼Œé¿å…é”™è¯¯
                            future_close = df.iloc[j + horizon]['close']
                            current_close = df.iloc[j]['close']
                            if current_close <= 0:
                                continue
                            
                            future_return = (future_close - current_close) / current_close
                            
                            # ç®€åŒ–æ ‡ç­¾ï¼šåªçœ‹ç»å¯¹æ”¶ç›Šç‡ï¼Œä¸è®¡ç®—ç›¸å¯¹æ”¶ç›Š
                            # æœªæ¥æ¶¨å¹…è¶…è¿‡2%åˆ™ä¸ºæ­£æ ·æœ¬ï¼ˆé™ä½é—¨æ§›ï¼Œå¢åŠ æ­£æ ·æœ¬ï¼‰
                            label = 1 if future_return > 0.02 else 0
                            
                            train_features.append(features)
                            train_labels.append(label)
                        
                        # è®­ç»ƒæ¨¡å‹
                        if len(train_features) >= 50:
                            # é…ç½®XGBoostå‚æ•° - ä¼˜åŒ–å‚æ•°
                            xgb_params = {
                                'max_depth': int(config.get('xgbMaxDepth', 5)),  # å¢åŠ ï¼š4â†’5
                                'learning_rate': float(config.get('xgbLearningRate', 0.08)),  # å¢åŠ ï¼š0.05â†’0.08
                                'n_estimators': int(config.get('xgbNEstimators', 150)),  # å¢åŠ ï¼š100â†’150
                                'min_child_weight': int(config.get('xgbMinChildWeight', 3)),  # é™ä½ï¼š5â†’3
                                'subsample': float(config.get('xgbSubsample', 0.9)),  # å¢åŠ ï¼š0.8â†’0.9
                                'colsample_bytree': float(config.get('xgbColsample', 0.9)),  # å¢åŠ ï¼š0.8â†’0.9
                                'reg_alpha': float(config.get('xgbRegAlpha', 0.05)),  # é™ä½ï¼š0.1â†’0.05
                                'reg_lambda': float(config.get('xgbRegLambda', 0.8)),  # é™ä½ï¼š1.0â†’0.8
                            }
                            
                            print(f"[XGBoost] è®­ç»ƒæ•°æ®: æ ·æœ¬æ•°={len(train_features)}, æ­£æ ·æœ¬={sum(train_labels)}, è´Ÿæ ·æœ¬={len(train_labels)-sum(train_labels)}")
                            
                            model_result = StrategyEngine.xgboost_train_model(train_features, train_labels, xgb_params)
                            
                            if model_result:
                                print(f"[XGBoost] è®­ç»ƒå®Œæˆ: å‡†ç¡®ç‡={model_result['accuracy']:.3f}, åŸºå‡†ç‡={model_result['base_rate']:.3f}")
                                if model_result['accuracy'] >= min_accuracy:
                                    model_info['model'] = model_result
                                    model_info['last_train_index'] = i
                                    model_info['train_count'] += 1
                                    print(f"[XGBoost] æ¨¡å‹å·²æ›´æ–° (è®­ç»ƒæ¬¡æ•°: {model_info['train_count']})")
                                else:
                                    print(f"[XGBoost] å‡†ç¡®ç‡ä¸è¾¾æ ‡: {model_result['accuracy']:.3f} < {min_accuracy:.2f}")
                            else:
                                print(f"[XGBoost] è®­ç»ƒå¤±è´¥: model_result={model_result}")
                        else:
                            print(f"[XGBoost] è®­ç»ƒæ•°æ®ä¸è¶³: {len(train_features)} < 50")
                    
                    # ä½¿ç”¨æ¨¡å‹è¿›è¡Œé¢„æµ‹
                    if model_info['model'] is not None:
                        features = StrategyEngine.xgboost_extract_features(df, i)
                        
                        if features is not None:
                            prob = StrategyEngine.xgboost_predict(model_info['model'], features)
                            
                            if prob is not None:
                                print(f"[XGBoost] é¢„æµ‹: æ¦‚ç‡={prob:.3f}, åŸºå‡†ç‡={model_info['model']['base_rate']:.3f}, è¾¹ç¼˜={prob - model_info['model']['base_rate']:.3f}")
                                
                                # æ”¾å®½è¿‡æ»¤æ¡ä»¶ï¼Œæé«˜ä¹°å…¥æ•æ„Ÿåº¦
                                curr_vol_ratio = d['volume'] / d['volMa5'] if d.get('volMa5') and d['volMa5'] > 0 else 1
                                vol_ok = curr_vol_ratio >= volume_multi  # 1.0å€å³å¯ï¼Œä¸è¦æ±‚æ”¾é‡
                                
                                # è¶‹åŠ¿è¿‡æ»¤ - å¤§å¹…æ”¾å®½
                                ma20 = d.get('ma20')
                                ma60 = d.get('ma60')
                                trend_ok = True
                                if ma20 and ma60:
                                    # æ”¾å®½ï¼šåªè¦ä»·æ ¼>MA20å³å¯ï¼Œä¸è¦æ±‚MA20>MA60
                                    trend_ok = d['close'] > ma20
                                
                                # è·³ç©ºè¿‡æ»¤ - æ”¾å®½
                                prev = df.iloc[i-1] if i > 0 else None
                                gap_ok = True
                                if prev and use_strict_filter:
                                    gap = (d['open'] - prev['close']) / prev['close'] if prev['close'] > 0 else 0
                                    gap_ok = abs(gap) < 0.10  # æ”¾å®½ï¼š7%â†’10%
                                
                                # æ³¢åŠ¨ç‡è¿‡æ»¤ - æ”¾å®½
                                atr = d.get('atr', (d['high'] - d['low']) * 0.5)
                                atr_pct = atr / d['close'] if d['close'] > 0 else 0
                                atr_ok = atr_pct < 0.20  # æ”¾å®½ï¼š15%â†’20%
                                
                                # ç»¼åˆåˆ¤æ–­ - é™ä½é—¨æ§›
                                edge = prob - model_info['model']['base_rate']
                                
                                # æ”¾å®½ä¹°å…¥æ¡ä»¶ï¼š
                                # 1. æ¦‚ç‡é˜ˆå€¼ï¼š0.65â†’0.55
                                # 2. è¾¹ç¼˜ä¼˜åŠ¿ï¼š0.05â†’0.02
                                # 3. æˆäº¤é‡ï¼š1.2å€â†’1.0å€
                                # 4. è¶‹åŠ¿ï¼šå¤šå¤´æ’åˆ—â†’ä»·æ ¼>MA20
                                # 5. è·³ç©º/æ³¢åŠ¨ç‡ï¼šä»…åœ¨ä¸¥æ ¼æ¨¡å¼ä¸‹æ£€æŸ¥
                                
                                conditions_met = 0
                                if prob >= min_prob:
                                    conditions_met += 1
                                if edge >= 0.02:
                                    conditions_met += 1
                                if vol_ok:
                                    conditions_met += 1
                                if trend_ok:
                                    conditions_met += 1
                                
                                print(f"[XGBoost] è¿‡æ»¤æ¡ä»¶: æ¦‚ç‡OK={prob >= min_prob}, è¾¹ç¼˜OK={edge >= 0.02}, æˆäº¤é‡OK={vol_ok}, è¶‹åŠ¿OK={trend_ok}, æ»¡è¶³={conditions_met}/4")
                                
                                # æ»¡è¶³è‡³å°‘3ä¸ªæ¡ä»¶å³å¯ä¹°å…¥ï¼ˆæ›´å®½æ¾ï¼‰
                                if conditions_met >= 3:
                                    # ä¸¥æ ¼æ¨¡å¼ä¸‹æ‰æ£€æŸ¥è·³ç©ºå’Œæ³¢åŠ¨ç‡
                                    if use_strict_filter:
                                        if gap_ok and atr_ok:
                                            buy_signal = True
                                            signal_strength = min(95, int(50 + prob * 50))
                                            print(f"[XGBoost] ä¹°å…¥ä¿¡å· (ä¸¥æ ¼æ¨¡å¼): å¼ºåº¦={signal_strength}")
                                    else:
                                        buy_signal = True
                                        signal_strength = min(95, int(50 + prob * 50))
                                        print(f"[XGBoost] ä¹°å…¥ä¿¡å· (å®½æ¾æ¨¡å¼): å¼ºåº¦={signal_strength}")
                                else:
                                    print(f"[XGBoost] æœªæ»¡è¶³ä¹°å…¥æ¡ä»¶")
                            else:
                                print(f"[XGBoost] é¢„æµ‹å¤±è´¥: prob={prob}")
                        else:
                            print(f"[XGBoost] ç‰¹å¾æå–å¤±è´¥: i={i}")
                    else:
                        print(f"[XGBoost] æ¨¡å‹æœªè®­ç»ƒ")
                
                elif strategy_name == 'lstm_ml':
                    # ç®€åŒ–çš„LSTMç­–ç•¥ï¼šåŸºäºåŸºç¡€æŠ€æœ¯æŒ‡æ ‡ç»„åˆ
                    conditions_met = 0
                    
                    curr_close = d['close']
                    curr_ma5 = d.get('ma5', curr_close)
                    curr_ma10 = d.get('ma10', curr_close)
                    curr_ma20 = d.get('ma20', curr_close)
                    curr_rsi = d.get('rsi', 50)
                    curr_macd = d.get('macd', 0)
                    curr_vol_ratio = d['volume'] / d['volMa5'] if d.get('volMa5') and d['volMa5'] > 0 else 1.0
                    
                    # åŸºç¡€ä¹°å…¥æ¡ä»¶ï¼ˆå¤§å¹…æ”¾å®½ï¼‰
                    if curr_rsi < 70:  # RSIä¸è¶…ä¹°
                        conditions_met += 1
                    if curr_macd > -0.2:  # MACDä¸ä¸ºè´Ÿå¤ªå¤š
                        conditions_met += 1
                    if curr_close > curr_ma20 * 0.98:  # ä»·æ ¼åœ¨MA20é™„è¿‘æˆ–ä¹‹ä¸Š
                        conditions_met += 1
                    if curr_vol_ratio > 0.5:  # æœ‰æˆäº¤é‡
                        conditions_met += 1
                    if curr_ma5 > curr_ma10 or curr_ma10 > curr_ma20:  # å‡çº¿å¤šå¤´æ’åˆ—æˆ–éƒ¨åˆ†å¤šå¤´æ’åˆ—
                        conditions_met += 1
                    
                    # æ»¡è¶³3ä¸ªæ¡ä»¶å³ä¹°å…¥
                    if conditions_met >= 3:
                        buy_signal = True
                        signal_strength = 50 + conditions_met * 10
                
                elif strategy_name == 'ensemble_ml':
                    # ç®€åŒ–çš„æ··åˆæœºå™¨å­¦ä¹ ç­–ç•¥ï¼šåŸºäºåŸºç¡€æŠ€æœ¯æŒ‡æ ‡ç»„åˆ
                    conditions_met = 0
                    
                    curr_close = d['close']
                    curr_ma5 = d.get('ma5', curr_close)
                    curr_ma10 = d.get('ma10', curr_close)
                    curr_ma20 = d.get('ma20', curr_close)
                    curr_rsi = d.get('rsi', 50)
                    curr_macd = d.get('macd', 0)
                    curr_kdj_k = d.get('kdjK', 50)
                    curr_kdj_d = d.get('kdjD', 50)
                    curr_vol_ratio = d['volume'] / d['volMa5'] if d.get('volMa5') and d['volMa5'] > 0 else 1.0
                    
                    # åŸºç¡€ä¹°å…¥æ¡ä»¶ï¼ˆå¤§å¹…æ”¾å®½ï¼‰
                    if curr_rsi < 65:  # RSIä¸è¶…ä¹°
                        conditions_met += 1
                    if curr_macd > -0.1:  # MACDä¸ä¸ºè´Ÿå¤ªå¤š
                        conditions_met += 1
                    if curr_close > curr_ma20 * 0.97:  # ä»·æ ¼åœ¨MA20é™„è¿‘æˆ–ä¹‹ä¸Š
                        conditions_met += 1
                    if curr_vol_ratio > 0.6:  # æœ‰æˆäº¤é‡
                        conditions_met += 1
                    if curr_ma5 > curr_ma10 or curr_ma10 > curr_ma20:  # å‡çº¿å¤šå¤´æ’åˆ—æˆ–éƒ¨åˆ†å¤šå¤´æ’åˆ—
                        conditions_met += 1
                    if curr_kdj_k > curr_kdj_d:  # KDJé‡‘å‰æˆ–ç»´æŒ
                        conditions_met += 1
                    
                    # æ»¡è¶³4ä¸ªæ¡ä»¶å³ä¹°å…¥
                    if conditions_met >= 4:
                        buy_signal = True
                        signal_strength = 50 + conditions_met * 8
                
                elif strategy_name == 'mtsf_net':
                    # å¤šå°ºåº¦æ—¶åºèåˆç½‘ç»œ (MTSF-Net)
                    # æ ¸å¿ƒï¼šèåˆçŸ­/ä¸­/é•¿æœŸç‰¹å¾ï¼Œæ•æ‰å¤šå°ºåº¦æ¨¡å¼
                    import sys
                    print(f"[MTSF-Net] ç­–ç•¥è¢«è°ƒç”¨ï¼Œi={i}, TENSORFLOW_AVAILABLE={TENSORFLOW_AVAILABLE}", file=sys.stderr, flush=True)
                    
                    if not TENSORFLOW_AVAILABLE:
                        print(f"[MTSF-Net] TensorFlowä¸å¯ç”¨ï¼Œè·³è¿‡")
                        continue
                    
                    # åˆå§‹åŒ–æ¨¡å‹ç¼“å­˜
                    if not hasattr(StrategyEngine, 'mtsf_models'):
                        StrategyEngine.mtsf_models = {}
                    
                    stock_key = f"mtsf_{strategy_name}_{len(df)}"
                    
                    # å‚æ•°é…ç½®
                    lookback = int(config.get('mtsfLookback', 30))
                    retrain_interval = int(config.get('mtsfRetrainInterval', 20))
                    min_prob = float(config.get('mtsfMinProb', 0.52))
                    
                    if stock_key not in StrategyEngine.mtsf_models:
                        StrategyEngine.mtsf_models[stock_key] = {
                            'model': None, 'last_train_index': -999, 'scaler': None
                        }
                    
                    model_info = StrategyEngine.mtsf_models[stock_key]
                    need_retrain = (i - model_info['last_train_index']) >= retrain_interval
                    
                    print(f"[MTSF-Net] i={i}, lookback={lookback}, need_retrain={need_retrain}, model_exists={model_info['model'] is not None}")
                    
                    # è®­ç»ƒæˆ–æ›´æ–°æ¨¡å‹
                    if need_retrain and i >= lookback + 10:
                        sequences, labels = [], []
                        for j in range(lookback, i - 5):
                            feat = StrategyEngine.mtsf_extract_features(df, j, lookback)
                            if feat is not None:
                                future_return = (df.iloc[j + 5]['close'] - df.iloc[j]['close']) / df.iloc[j]['close']
                                sequences.append(feat)
                                labels.append(1 if future_return > 0.015 else 0)
                        
                        print(f"[MTSF-Net] è®­ç»ƒæ•°æ®: {len(sequences)}ä¸ªæ ·æœ¬, æ­£æ ·æœ¬{sum(labels) if labels else 0}ä¸ª")
                        
                        if len(sequences) >= 100:
                            model_result = StrategyEngine.mtsf_train_model(sequences, labels, lookback)
                            if model_result:
                                model_info['model'] = model_result
                                model_info['last_train_index'] = i
                                print(f"[MTSF-Net] æ¨¡å‹è®­ç»ƒæˆåŠŸï¼Œå‡†ç¡®ç‡={model_result['accuracy']:.3f}")
                            else:
                                print(f"[MTSF-Net] æ¨¡å‹è®­ç»ƒå¤±è´¥")
                        else:
                            print(f"[MTSF-Net] è®­ç»ƒæ•°æ®ä¸è¶³: {len(sequences)} < 100")
                    
                    # é¢„æµ‹
                    if model_info['model'] is not None:
                        features = StrategyEngine.mtsf_extract_features(df, i, lookback)
                        print(f"[MTSF-Net] i={i}, featuresæå–={'æˆåŠŸ' if features is not None else 'å¤±è´¥'}")
                        
                        if features is not None:
                            prob = StrategyEngine.mtsf_predict(model_info['model'], features)
                            prob_str = f"{prob:.3f}" if prob is not None else "None"
                            print(f"[MTSF-Net] i={i}, prob={prob_str}, min_prob={min_prob}")
                            
                            if prob is not None and prob >= min_prob:
                                # é™„åŠ è¿‡æ»¤æ¡ä»¶
                                curr_ma20 = d.get('ma20', d['close'])
                                vol_ratio = d['volume'] / d['volMa5'] if d.get('volMa5', 0) > 0 else 1
                                
                                print(f"[MTSF-Net] i={i}, æ¡ä»¶æ£€æŸ¥: close={d['close']:.2f}, ma20={curr_ma20:.2f}, vol_ratio={vol_ratio:.2f}")
                                
                                if d['close'] > curr_ma20 * 0.95 and vol_ratio > 0.6:
                                    buy_signal = True
                                    signal_strength = min(95, int(50 + prob * 45))
                                    print(f"[MTSF-Net] i={i}, ä¹°å…¥ä¿¡å·! strength={signal_strength}")
                                else:
                                    print(f"[MTSF-Net] i={i}, æœªé€šè¿‡è¿‡æ»¤æ¡ä»¶")
                            else:
                                prob_str2 = f"{prob:.3f}" if prob is not None else "None"
                                print(f"[MTSF-Net] i={i}, æ¦‚ç‡ä¸è¶³: prob={prob_str2} < {min_prob}")
                    else:
                        print(f"[MTSF-Net] i={i}, æ¨¡å‹æœªè®­ç»ƒ")
                
                elif strategy_name == 'vpan_net':
                    # é‡ä»·æ³¨æ„åŠ›é¢„æµ‹ç½‘ç»œ (VPAN)
                    # æ ¸å¿ƒï¼šTransformeræ¶æ„ï¼Œå…³æ³¨é‡ä»·å…³ç³»
                    import sys
                    print(f"[VPAN] ç­–ç•¥è¢«è°ƒç”¨ï¼Œi={i}, TENSORFLOW_AVAILABLE={TENSORFLOW_AVAILABLE}", file=sys.stderr, flush=True)
                    
                    if not TENSORFLOW_AVAILABLE:
                        print(f"[VPAN] TensorFlowä¸å¯ç”¨ï¼Œè·³è¿‡", file=sys.stderr, flush=True)
                        continue
                    
                    # åˆå§‹åŒ–æ¨¡å‹ç¼“å­˜
                    if not hasattr(StrategyEngine, 'vpan_models'):
                        StrategyEngine.vpan_models = {}
                    
                    stock_key = f"vpan_{strategy_name}_{len(df)}"
                    
                    # å‚æ•°é…ç½®
                    seq_len = int(config.get('vpanSeqLen', 20))
                    retrain_interval = int(config.get('vpanRetrainInterval', 25))
                    min_prob = float(config.get('vpanMinProb', 0.55))
                    
                    if stock_key not in StrategyEngine.vpan_models:
                        StrategyEngine.vpan_models[stock_key] = {
                            'model': None, 'last_train_index': -999
                        }
                    
                    model_info = StrategyEngine.vpan_models[stock_key]
                    need_retrain = (i - model_info['last_train_index']) >= retrain_interval
                    
                    print(f"[VPAN] i={i}, seq_len={seq_len}, need_retrain={need_retrain}, model_exists={model_info['model'] is not None}")
                    
                    # è®­ç»ƒæ¨¡å‹
                    if need_retrain and i >= seq_len + 10:
                        sequences, labels = [], []
                        for j in range(seq_len, i - 5):
                            seq = StrategyEngine.vpan_extract_sequence(df, j, seq_len)
                            if seq is not None:
                                future_return = (df.iloc[j + 5]['close'] - df.iloc[j]['close']) / df.iloc[j]['close']
                                sequences.append(seq)
                                labels.append(1 if future_return > 0.02 else 0)
                        
                        print(f"[VPAN] è®­ç»ƒæ•°æ®: {len(sequences)}ä¸ªæ ·æœ¬, æ­£æ ·æœ¬{sum(labels) if labels else 0}ä¸ª")
                        
                        if len(sequences) >= 80:
                            model_result = StrategyEngine.vpan_train_model(sequences, labels, seq_len)
                            if model_result:
                                model_info['model'] = model_result
                                model_info['last_train_index'] = i
                                print(f"[VPAN] æ¨¡å‹è®­ç»ƒæˆåŠŸï¼Œå‡†ç¡®ç‡={model_result['accuracy']:.3f}")
                            else:
                                print(f"[VPAN] æ¨¡å‹è®­ç»ƒå¤±è´¥")
                        else:
                            print(f"[VPAN] è®­ç»ƒæ•°æ®ä¸è¶³: {len(sequences)} < 80")
                    
                    # é¢„æµ‹
                    if model_info['model'] is not None:
                        sequence = StrategyEngine.vpan_extract_sequence(df, i, seq_len)
                        print(f"[VPAN] i={i}, sequenceæå–={'æˆåŠŸ' if sequence is not None else 'å¤±è´¥'}")
                        
                        if sequence is not None:
                            prob = StrategyEngine.vpan_predict(model_info['model'], sequence)
                            prob_str = f"{prob:.3f}" if prob is not None else "None"
                            print(f"[VPAN] i={i}, prob={prob_str}, min_prob={min_prob}")
                            
                            if prob is not None and prob >= min_prob:
                                # è¶‹åŠ¿ç¡®è®¤
                                ma5 = d.get('ma5', d['close'])
                                ma20 = d.get('ma20', d['close'])
                                rsi = d.get('rsi', 50)
                                
                                print(f"[VPAN] i={i}, æ¡ä»¶æ£€æŸ¥: close={d['close']:.2f}, ma5={ma5:.2f}, ma20={ma20:.2f}, rsi={rsi:.1f}")
                                
                                if d['close'] > ma5 and ma5 > ma20 * 0.98 and rsi < 75:
                                    buy_signal = True
                                    signal_strength = min(95, int(55 + prob * 40))
                                    print(f"[VPAN] i={i}, ä¹°å…¥ä¿¡å·! strength={signal_strength}")
                                else:
                                    print(f"[VPAN] i={i}, æœªé€šè¿‡è¿‡æ»¤æ¡ä»¶")
                            else:
                                prob_str2 = f"{prob:.3f}" if prob is not None else "None"
                                print(f"[VPAN] i={i}, æ¦‚ç‡ä¸è¶³: prob={prob_str2} < {min_prob}")
                    else:
                        print(f"[VPAN] i={i}, æ¨¡å‹æœªè®­ç»ƒ")


            
            if (not buy_signal) and (not in_cooldown) and position == 0 and config.get('enableTrendReentry', True):
                if strategy_name in {'trend_enhanced', 'turtle_enhanced', 'momentum_rotation', 'deep_fusion', 'volume_breakout', 'xgboost_ml', 'lstm_ml', 'ensemble_ml', 'chip_peak_breakout'}:
                    try:
                        d0 = df.iloc[i]
                        d1 = df.iloc[i - 1] if i > 0 else None
                        ma20 = d0.get('ma20', None)
                        ma60 = d0.get('ma60', None)
                        if ma20 is None or ma60 is None or pd.isna(ma20) or pd.isna(ma60):
                            raise ValueError()
                        ma20 = float(ma20)
                        ma60 = float(ma60)
                        close0 = float(d0.get('close', 0) or 0)
                        open0 = float(d0.get('open', 0) or 0)
                        low0 = float(d0.get('low', 0) or 0)
                        if close0 <= 0:
                            raise ValueError()

                        lookback = int(config.get('trendSlopeLookback', 5) or 5)
                        j = max(0, i - max(2, lookback))
                        ma20_prev = df.iloc[j].get('ma20', None)
                        if ma20_prev is None or pd.isna(ma20_prev):
                            raise ValueError()
                        ma20_prev = float(ma20_prev)

                        uptrend = close0 > ma20 > ma60 and ma20 > ma20_prev
                        if not uptrend:
                            raise ValueError()

                        bounced = False
                        if d1 is not None:
                            ma20_1 = d1.get('ma20', None)
                            if ma20_1 is not None and not pd.isna(ma20_1):
                                ma20_1 = float(ma20_1)
                                close1 = float(d1.get('close', 0) or 0)
                                bounced = close1 <= ma20_1 and close0 > ma20 and low0 <= ma20 * 1.01
                        if not bounced:
                            bounced = low0 <= ma20 * 1.005 and close0 > open0 and close0 > ma20

                        if not bounced:
                            raise ValueError()

                        min_rsi = float(config.get('reentryMinRsi', 45) or 45)
                        rsi = d0.get('rsi', None)
                        if rsi is not None and not pd.isna(rsi) and float(rsi) < min_rsi:
                            raise ValueError()

                        vol_need = float(config.get('reentryMinVolRatio', 0.95) or 0.95)
                        vol_ratio = 1.0
                        if d0.get('volMa5') and float(d0.get('volMa5') or 0) > 0:
                            vol_ratio = float(d0.get('volume', 0) or 0) / float(d0.get('volMa5'))
                        if vol_ratio < vol_need:
                            raise ValueError()

                        buy_signal = True
                        signal_strength = max(signal_strength, int(config.get('reentryStrength', 62) or 62))
                    except Exception:
                        pass

            if (not buy_signal) and (not in_cooldown) and position == 0 and config.get('enableSwingBottomBuy', True):
                if strategy_name in {'oversold_rebound', 'bollinger_extreme', 'macd_divergence'}:
                    try:
                        rsi = d.get('rsi', None)
                        if rsi is None or pd.isna(rsi):
                            raise ValueError()
                        rsi = float(rsi)
                        min_rsi = float(config.get('swingBuyRsi', 35) or 35)
                        if rsi > min_rsi:
                            raise ValueError()

                        near_low_pct = float(config.get('swingBuyNearLowPct', 2.0) or 2.0) / 100.0
                        low20 = d.get('low20', None)
                        if low20 is None or pd.isna(low20):
                            low20 = float(df['low'].iloc[max(0, i - 19):i + 1].min())
                        else:
                            low20 = float(low20)
                        close0 = float(d.get('close', 0) or 0)
                        open0 = float(d.get('open', 0) or 0)
                        vol_ratio = 1.0
                        if d.get('volMa5') and float(d.get('volMa5') or 0) > 0:
                            vol_ratio = float(d.get('volume', 0) or 0) / float(d.get('volMa5'))
                        if close0 <= 0:
                            raise ValueError()
                        if close0 > low20 * (1 + near_low_pct):
                            raise ValueError()
                        if close0 <= open0:
                            raise ValueError()
                        if vol_ratio < float(config.get('swingBuyMinVolRatio', 0.8) or 0.8):
                            raise ValueError()

                        buy_signal = True
                        signal_strength = max(signal_strength, int(config.get('swingBuyStrength', 62) or 62))
                    except Exception:
                        pass

            # å–å‡ºé€»è¾‘ - ä¼˜åŒ–ç‰ˆï¼Œæ”¯æŒæ›´å¤šå¯é…ç½®æ¡ä»¶
            if position > 0:
                highest_since_entry = max(highest_since_entry, d['high'])
                try:
                    body_high = max(float(d.get('open', 0) or 0), float(d.get('close', 0) or 0))
                    if body_high > 0:
                        highest_body_since_entry = max(highest_body_since_entry, body_high)
                except Exception:
                    pass
                hold_days = i - entry_index
                profit = (d['close'] - entry_price) / entry_price
                peak_profit_since_entry = max(peak_profit_since_entry, profit)
                
                # åŠ¨æ€æ­¢ç›ˆï¼ˆå¯é…ç½®å¼€å…³ï¼‰
                if (not sell_signal) and config.get('useDynamicTP', True):
                    dtp_threshold = config.get('dynamicTpThreshold', 0.08)
                    dtp_callback = config.get('dynamicTpCallback', 0.03)
                    dtp_variation = config.get('dynamicTpVariation', 0.15)
                    
                    if peak_profit_since_entry > dtp_threshold:
                        anchor = highest_body_since_entry if highest_body_since_entry > 0 else highest_since_entry
                        trail = anchor * (1 - dtp_callback - peak_profit_since_entry * dtp_variation)
                        if d['close'] < trail:
                            sell_signal = True
                            sell_reason = 'ç§»åŠ¨æ­¢ç›ˆ'

                if (not sell_signal) and config.get('useAtrTrail', True):
                    try:
                        min_profit = float(config.get('atrTrailMinProfit', 0.06) or 0.06)
                        atr_mult = float(config.get('atrTrailMult', 2.5) or 2.5)
                        atr = d.get('atr', None)
                        if atr is None or pd.isna(atr):
                            atr = abs(float(d.get('high', 0) or 0) - float(d.get('low', 0) or 0)) * 0.5
                        atr = float(atr)
                        if peak_profit_since_entry > min_profit and atr > 0:
                            trail = highest_since_entry - atr * atr_mult
                            if float(d.get('close', 0) or 0) < trail:
                                sell_signal = True
                                sell_reason = 'ATRè·Ÿè¸ªæ­¢ç›ˆ'
                    except Exception:
                        pass

                # æ­¢æŸï¼ˆå¯é…ç½®å¼€å…³ï¼‰- æ”¾åœ¨è¶‹åŠ¿è½¬å¼±/è·Ÿè¸ªä¹‹åï¼Œé¿å…ç›ˆåˆ©é˜¶æ®µå…ˆè¢«ç¡¬æ­¢æŸ
                if (not sell_signal) and config.get('useStopLoss', True):
                    stop_loss_level = config.get('stopLoss', 0.05)
                    if profit <= -stop_loss_level:
                        sell_signal = True
                        sell_reason = 'æ­¢æŸ'
                
                # æœ€å¤§æŒä»“æ—¶é—´é™åˆ¶
                if (not sell_signal) and hold_days >= int(config.get('maxHoldDays', 10)):
                    sell_signal = True
                    sell_reason = 'æœ€å¤§æŒä»“æ—¶é—´'
                
                # è·Œç ´MA5ï¼ˆå¯é…ç½®å¼€å…³ï¼‰
                if (not sell_signal) and config.get('useMa5Sell', True):
                    if d.get('ma5') and d['close'] < d['ma5'] and peak_profit_since_entry > 0.02:
                        sell_signal = True
                        sell_reason = 'è·Œç ´MA5'
                
                # MACDæ­»å‰ï¼ˆå¯é…ç½®å¼€å…³ï¼‰
                if (not sell_signal) and config.get('useMacdDeathCross', True):
                    if d.get('macd') and prev.get('macd') and d['macd'] < 0 and prev['macd'] > 0 and peak_profit_since_entry > 0:
                        sell_signal = True
                        sell_reason = 'MACDæ­»å‰'
                
                # RSIè¶…ä¹°ï¼ˆå¯é…ç½®å¼€å…³ï¼‰
                if (not sell_signal) and config.get('useRsiOverbought', True):
                    if d.get('rsi') and d['rsi'] > 80 and peak_profit_since_entry > 0.05:
                        sell_signal = True
                        sell_reason = 'RSIè¶…ä¹°'

                if (not sell_signal) and config.get('enableSwingTopSell', True):
                    try:
                        rsi = d.get('rsi', None)
                        if rsi is None or pd.isna(rsi):
                            raise ValueError()
                        rsi = float(rsi)
                        min_rsi = float(config.get('swingSellRsi', 75) or 75)
                        if rsi < min_rsi:
                            raise ValueError()

                        near_high_pct = float(config.get('swingSellNearHighPct', 2.0) or 2.0) / 100.0
                        high20 = d.get('high20', None)
                        if high20 is None or pd.isna(high20):
                            high20 = float(df['high'].iloc[max(0, i - 19):i + 1].max())
                        else:
                            high20 = float(high20)

                        close0 = float(d.get('close', 0) or 0)
                        open0 = float(d.get('open', 0) or 0)
                        if close0 < high20 * (1 - near_high_pct) or close0 <= 0:
                            raise ValueError()
                        if close0 >= open0:
                            raise ValueError()

                        sell_signal = True
                        sell_reason = 'é«˜ä½è½¬å¼±'
                    except Exception:
                        pass

                # æ­¢ç›ˆï¼ˆå¯é…ç½®å¼€å…³ï¼‰- æ”¾åˆ°åé¢ï¼Œå°½é‡è®©åˆ©æ¶¦å¥”è·‘
                if (not sell_signal) and config.get('useTakeProfit', True):
                    take_profit_level = config.get('takeProfit', 0.25)
                    if profit >= take_profit_level:
                        sell_signal = True
                        sell_reason = 'æ­¢ç›ˆ'
                
                # æŒä»“è¶…æ—¶ï¼ˆå¯é…ç½®å¼€å…³ï¼‰
                if (not sell_signal) and config.get('useHoldTimeout', True):
                    hold_timeout_days = config.get('holdTimeoutDays', 30)
                    if hold_days > hold_timeout_days and profit < 0.05:
                        sell_signal = True
                        sell_reason = 'æŒä»“è¶…æ—¶'
            
            # è®°å½•ä¿¡å·
            if buy_signal and position == 0 and StrategyEngine.passes_global_buy_filters(df, i, strategy_name, config, market_state, signal_strength):
                # åŠ¨æ€ä»“ä½ç®¡ç†
                if config.get('useAdaptPosition', True):
                    # ä¿¡å·å¼ºåº¦è¶Šé«˜ï¼Œä»“ä½è¶Šå¤§
                    base_position = 0.3
                    strength_factor = signal_strength / 100
                    position = base_position + strength_factor * 0.7
                    # æ ¹æ®å¸‚åœºçŠ¶æ€è°ƒæ•´
                    if market_state in ['bear_market', 'choppy_market']:
                        position *= 0.8  # ç†Šå¸‚/éœ‡è¡å¸‚é™ä½ä»“ä½
                    position = min(position, 1.0)  # ä¸è¶…è¿‡100%
                else:
                    position = 1.0
                
                entry_price = d['close']
                entry_index = i
                highest_since_entry = d['high']
                peak_profit_since_entry = 0.0
                highest_body_since_entry = max(float(d.get('open', 0) or 0), float(d.get('close', 0) or 0))
                
                signals.append({
                    'index': i,
                    'date': d['date'],
                    'type': 'buy',
                    'price': float(d['close']),
                    'position': position,
                    'strength': signal_strength
                })
            
            if sell_signal and position > 0:
                profit_pct = ((d['close'] - entry_price) / entry_price) * 100
                signals.append({
                    'index': i,
                    'date': d['date'],
                    'type': 'sell',
                    'price': float(d['close']),
                    'position': position,
                    'profit': profit_pct,
                    'holdDays': i - entry_index,
                    'reason': sell_reason,
                    'strength': signal_strength
                })
                position = 0
                last_sell_index = i  # æ›´æ–°å–å‡ºç´¢å¼•
        
        return signals
    
    @staticmethod
    def calculate_backtest(data, signals, init_capital):
        """è®¡ç®—å›æµ‹ç»“æœ"""
        df = pd.DataFrame(data)
        capital = init_capital
        shares = 0
        position = 0
        equity = [init_capital]
        
        wins = 0
        losses = 0
        total_profit = 0
        total_loss = 0
        hold_days_sum = 0
        trades = []
        
        signal_dict = {s['index']: s for s in signals}
        
        for i in range(len(df)):
            signal = signal_dict.get(i)
            
            if signal:
                if signal['type'] == 'buy':
                    amount = capital * signal['position'] * 0.998
                    shares = int(amount / signal['price'] / 100) * 100
                    capital -= shares * signal['price'] * 1.001
                    position = signal['position']
                    trades.append({**signal, 'shares': shares, 'amount': shares * signal['price']})
                else:
                    sell_amount = shares * signal['price'] * 0.998
                    capital += sell_amount
                    
                    if signal.get('profit', 0) > 0:
                        wins += 1
                        total_profit += signal['profit']
                    else:
                        losses += 1
                        total_loss += abs(signal.get('profit', 0))
                    
                    hold_days_sum += signal.get('holdDays', 0)
                    trades.append({**signal, 'shares': shares, 'amount': sell_amount})
                    shares = 0
                    position = 0
            
            current_value = capital + shares * df.iloc[i]['close']
            equity.append(current_value)
        
        final_equity = capital + shares * df.iloc[-1]['close']
        total_return = ((final_equity - init_capital) / init_capital) * 100
        days = len(df)
        annual_return = (pow(1 + total_return / 100, 252 / days) - 1) * 100 if days > 0 else 0
        
        # æœ€å¤§å›æ’¤
        max_drawdown = 0
        peak = equity[0]
        for e in equity:
            peak = max(peak, e)
            drawdown = (peak - e) / peak * 100 if peak > 0 else 0
            max_drawdown = max(max_drawdown, drawdown)
        
        # å¤æ™®æ¯”ç‡
        returns = []
        for i in range(1, len(equity)):
            if equity[i-1] > 0:
                returns.append((equity[i] - equity[i-1]) / equity[i-1])
        
        if returns:
            avg_return = np.mean(returns)
            std_return = np.std(returns)
            sharpe = (avg_return * 252 - 0.03) / (std_return * np.sqrt(252)) if std_return > 0 else 0
        else:
            sharpe = 0
        
        win_rate = (wins / (wins + losses)) * 100 if (wins + losses) > 0 else 0
        profit_ratio = total_profit / total_loss if total_loss > 0 else (10 if total_profit > 0 else 0)
        calmar = annual_return / max_drawdown if max_drawdown > 0 else 0
        avg_hold_days = hold_days_sum / (wins + losses) if (wins + losses) > 0 else 0
        
        # ç»¼åˆè¯„åˆ†
        score = (
            min(total_return / 2, 30) +
            min(sharpe * 10, 25) +
            max(0, 25 - max_drawdown) +
            min(win_rate / 4, 20)
        )
        
        return {
            'initCapital': init_capital,
            'finalCapital': final_equity,
            'totalReturn': total_return,
            'annualReturn': annual_return,
            'maxDrawdown': max_drawdown,
            'sharpe': sharpe,
            'calmar': calmar,
            'winRate': win_rate,
            'profitRatio': profit_ratio,
            'avgHoldDays': avg_hold_days,
            'wins': wins,
            'losses': losses,
            'trades': trades,
            'equity': equity,
            'score': score,
            'tradeCount': wins + losses
        }


def calculate_indicators(df):
    """è®¡ç®—æŠ€æœ¯æŒ‡æ ‡"""
    if df is None or len(df) == 0:
        return df
    
    # è½¬æ¢ä¸ºDataFrameï¼ˆå¦‚æœæ˜¯å­—å…¸åˆ—è¡¨ï¼‰
    if isinstance(df, list):
        df = pd.DataFrame(df)
    
    # åŸºç¡€æŒ‡æ ‡
    df['ma5'] = df['close'].rolling(window=5).mean()
    df['ma10'] = df['close'].rolling(window=10).mean()
    df['ma20'] = df['close'].rolling(window=20).mean()
    df['ma30'] = df['close'].rolling(window=30).mean()
    df['ma60'] = df['close'].rolling(window=60).mean()
    
    # æˆäº¤é‡å‡çº¿
    df['volMa5'] = df['volume'].rolling(window=5).mean()
    df['volMa10'] = df['volume'].rolling(window=10).mean()
    df['volMa20'] = df['volume'].rolling(window=20).mean()
    
    # RSI
    delta = df['close'].diff()
    gain = (delta.where(delta > 0, 0)).rolling(window=14).mean()
    loss = (-delta.where(delta < 0, 0)).rolling(window=14).mean()
    rs = gain / loss
    df['rsi'] = 100 - (100 / (1 + rs))
    
    # MACD
    exp12 = df['close'].ewm(span=12, adjust=False).mean()
    exp26 = df['close'].ewm(span=26, adjust=False).mean()
    df['macd'] = exp12 - exp26
    df['macdSignal'] = df['macd'].ewm(span=9, adjust=False).mean()
    df['macdHist'] = df['macd'] - df['macdSignal']
    
    # å¸ƒæ—å¸¦
    df['bollMid'] = df['close'].rolling(window=20).mean()
    boll_std = df['close'].rolling(window=20).std()
    df['bollUp'] = df['bollMid'] + 2 * boll_std
    df['bollDown'] = df['bollMid'] - 2 * boll_std
    df['bollWidth'] = (df['bollUp'] - df['bollDown']) / df['bollMid']  # å¸ƒæ—å¸¦å®½åº¦
    
    # é«˜ä½ç‚¹
    df['high10'] = df['high'].rolling(window=10).max()
    df['low10'] = df['low'].rolling(window=10).min()
    df['high20'] = df['high'].rolling(window=20).max()
    df['low20'] = df['low'].rolling(window=20).min()
    df['high60'] = df['high'].rolling(window=60).max()
    df['low60'] = df['low'].rolling(window=60).min()
    
    # ATR (Average True Range)
    df['tr1'] = df['high'] - df['low']
    df['tr2'] = abs(df['high'] - df['close'].shift(1))
    df['tr3'] = abs(df['low'] - df['close'].shift(1))
    df['tr'] = df[['tr1', 'tr2', 'tr3']].max(axis=1)
    df['atr'] = df['tr'].rolling(window=14).mean()
    df['atr_pct'] = df['atr'] / df['close'] * 100  # ATRç™¾åˆ†æ¯”
    
    # ADX (Average Directional Index) - è¶‹åŠ¿å¼ºåº¦
    df['plus_dm'] = df['high'].diff()
    df['minus_dm'] = -df['low'].diff()
    df['plus_dm'] = df['plus_dm'].where((df['plus_dm'] > 0) & (df['plus_dm'] > df['minus_dm']), 0)
    df['minus_dm'] = df['minus_dm'].where((df['minus_dm'] > 0) & (df['minus_dm'] > df['plus_dm']), 0)
    df['plus_di'] = 100 * (df['plus_dm'].rolling(window=14).mean() / df['atr'])
    df['minus_di'] = 100 * (df['minus_dm'].rolling(window=14).mean() / df['atr'])
    df['dx'] = 100 * abs(df['plus_di'] - df['minus_di']) / (df['plus_di'] + df['minus_di'])
    df['adx'] = df['dx'].rolling(window=14).mean()
    
    # é‡èƒ½åˆ†å¸ƒåˆ†æ
    df['vol_percentile'] = df['volume'].rank(pct=True) * 100
    
    # ä»·æ ¼ä½ç½®åˆ†æ
    df['price_position_20'] = (df['close'] - df['low20']) / (df['high20'] - df['low20']) * 100
    df['price_position_60'] = (df['close'] - df['low60']) / (df['high60'] - df['low60']) * 100
    
    return df


def identify_market_state(df):
    """è¯†åˆ«å¸‚åœºçŠ¶æ€ - å¢å¼ºç‰ˆ"""
    if df is None or len(df) < 60:
        return 'unknown'
    
    # è½¬æ¢ä¸ºDataFrameï¼ˆå¦‚æœæ˜¯å­—å…¸åˆ—è¡¨ï¼‰
    if isinstance(df, list):
        df = pd.DataFrame(df)
    
    latest = df.iloc[-1]
    recent = df.tail(20)
    
    # è¶‹åŠ¿åˆ¤æ–­ (ä½¿ç”¨ADXå¢å¼º)
    adx_strength = latest['adx'] if 'adx' in latest else 0
    ma_trend = 'neutral'
    
    if latest['ma5'] > latest['ma20'] and latest['ma20'] > latest['ma60']:
        ma_trend = 'strong_uptrend'
    elif latest['ma5'] > latest['ma20']:
        ma_trend = 'weak_uptrend'
    elif latest['ma5'] < latest['ma20'] and latest['ma20'] < latest['ma60']:
        ma_trend = 'strong_downtrend'
    elif latest['ma5'] < latest['ma20']:
        ma_trend = 'weak_downtrend'
    
    # æ³¢åŠ¨æ€§åˆ¤æ–­ (ä½¿ç”¨å¸ƒæ—å¸¦å®½åº¦å’ŒATR)
    boll_width = latest['bollWidth'] if 'bollWidth' in latest else 0
    atr_volatility = latest['atr_pct'] if 'atr_pct' in latest else 0
    
    if boll_width > 0.15 or atr_volatility > 3:
        volatility_state = 'high'
    elif boll_width > 0.08 or atr_volatility > 1.5:
        volatility_state = 'medium'
    else:
        volatility_state = 'low'
    
    # é‡èƒ½çŠ¶æ€
    vol_state = 'normal'
    if latest['vol_percentile'] > 80:
        vol_state = 'high_volume'
    elif latest['vol_percentile'] < 20:
        vol_state = 'low_volume'
    
    # ç»¼åˆå¸‚åœºçŠ¶æ€åˆ¤æ–­
    if ma_trend == 'strong_uptrend' and volatility_state in ['medium', 'low']:
        return 'bull_market'
    elif ma_trend == 'strong_downtrend' and volatility_state == 'high':
        return 'bear_market'
    elif ma_trend in ['weak_uptrend', 'weak_downtrend'] and volatility_state == 'high':
        return 'volatile_market'
    elif ma_trend == 'neutral' and volatility_state == 'low':
        return 'sideways_market'
    elif ma_trend == 'neutral' and volatility_state == 'high':
        return 'choppy_market'
    else:
        return f'{ma_trend}_{volatility_state}'


def calculate_signal_strength(data, strategy_name, signals, market_state):
    """è®¡ç®—ä¿¡å·å¼ºåº¦ - å¤šå› å­ç»¼åˆè¯„åˆ†"""
    if not signals or len(signals) == 0:
        return signals
    
    df = pd.DataFrame(data)
    if len(df) < 20:
        return signals
    
    # å¸‚åœºçŠ¶æ€æƒé‡
    market_weights = {
        'bull_market': {'trend': 1.2, 'momentum': 1.1, 'volatility': 0.8},
        'bear_market': {'trend': 0.7, 'momentum': 0.8, 'volatility': 1.3},
        'sideways_market': {'trend': 0.8, 'momentum': 1.2, 'volatility': 1.0},
        'volatile_market': {'trend': 0.9, 'momentum': 1.3, 'volatility': 1.1},
        'choppy_market': {'trend': 0.6, 'momentum': 0.9, 'volatility': 1.2}
    }
    
    base_weight = market_weights.get(market_state, {'trend': 1.0, 'momentum': 1.0, 'volatility': 1.0})
    
    # ç­–ç•¥ç‰¹å®šçš„å› å­æƒé‡
    strategy_factors = {
        'deep_fusion': {'ma_alignment': 0.2, 'macd_momentum': 0.2, 'rsi_condition': 0.15, 'volume_confirmation': 0.15, 'bollinger_position': 0.1, 'price_momentum': 0.1, 'volatility_adjustment': 0.1},
        'volume_breakout': {'breakout_strength': 0.3, 'volume_surge': 0.3, 'trend_alignment': 0.2, 'volatility_filter': 0.2},
        'oversold_rebound': {'oversold_level': 0.3, 'support_confirmation': 0.25, 'volume_confirmation': 0.2, 'trend_reversal': 0.15, 'volatility_adjustment': 0.1},
        'trend_enhanced': {'trend_strength': 0.35, 'ma_alignment': 0.25, 'momentum_confirmation': 0.2, 'volatility_filter': 0.2},
        'macd_divergence': {'divergence_quality': 0.3, 'trend_confirmation': 0.25, 'volume_support': 0.2, 'price_position': 0.15, 'volatility_adjustment': 0.1},
        'bollinger_extreme': {'extreme_level': 0.35, 'band_width': 0.2, 'volume_confirmation': 0.2, 'trend_alignment': 0.15, 'volatility_adjustment': 0.1},
        'momentum_rotation': {'momentum_strength': 0.3, 'relative_strength': 0.25, 'volume_confirmation': 0.2, 'trend_alignment': 0.15, 'volatility_adjustment': 0.1},
        'turtle_enhanced': {'breakout_quality': 0.3, 'trend_confirmation': 0.25, 'volatility_adjustment': 0.2, 'volume_confirmation': 0.15, 'position_sizing': 0.1}
    }
    
    factors = strategy_factors.get(strategy_name, {'default': 1.0})
    
    # ä¸ºæ¯ä¸ªä¿¡å·è®¡ç®—å¼ºåº¦
    enhanced_signals = []
    for signal in signals:
        if signal['type'] != 'buy':
            enhanced_signals.append(signal)
            continue
            
        signal_date = signal['date']
        signal_idx = df[df['date'] == signal_date].index
        if len(signal_idx) == 0:
            enhanced_signals.append(signal)
            continue
            
        idx = signal_idx[0]
        if idx < 20:
            enhanced_signals.append(signal)
            continue
            
        current_row = df.iloc[idx]
        prev_row = df.iloc[idx-1] if idx > 0 else current_row
        
        # è®¡ç®—å„å› å­å¾—åˆ† (0-100åˆ†)
        factor_scores = {}
        
        if strategy_name == 'deep_fusion':
            # MAå¯¹é½åº¦
            ma_score = 0
            if current_row['ma5'] > current_row['ma20'] > current_row['ma60']:
                ma_score = 100
            elif current_row['ma5'] > current_row['ma20']:
                ma_score = 70
            elif current_row['ma5'] < current_row['ma20'] < current_row['ma60']:
                ma_score = 30
            else:
                ma_score = 50
            factor_scores['ma_alignment'] = ma_score * base_weight['trend']
            
            # MACDåŠ¨é‡
            macd_score = 0
            if current_row['macd'] > 0 and current_row['macd'] > prev_row['macd']:
                macd_score = 100
            elif current_row['macd'] > 0:
                macd_score = 70
            elif current_row['macd'] < 0 and current_row['macd'] < prev_row['macd']:
                macd_score = 30
            else:
                macd_score = 50
            factor_scores['macd_momentum'] = macd_score * base_weight['momentum']
            
            # RSIæ¡ä»¶
            rsi_score = 0
            if current_row['rsi'] < 25:
                rsi_score = 100
            elif current_row['rsi'] < 35:
                rsi_score = 80
            elif current_row['rsi'] < 45:
                rsi_score = 60
            elif current_row['rsi'] > 70:
                rsi_score = 20
            else:
                rsi_score = 50
            factor_scores['rsi_condition'] = rsi_score * base_weight['volatility']
            
            # æˆäº¤é‡ç¡®è®¤
            vol_score = 0
            if current_row['volume'] > current_row['volMa5'] * 2:
                vol_score = 100
            elif current_row['volume'] > current_row['volMa5'] * 1.5:
                vol_score = 80
            elif current_row['volume'] > current_row['volMa5']:
                vol_score = 60
            else:
                vol_score = 40
            factor_scores['volume_confirmation'] = vol_score
            
            # å¸ƒæ—å¸¦ä½ç½®
            boll_score = 0
            if current_row['close'] < current_row['bollDown']:
                boll_score = 100
            elif current_row['close'] < current_row['bollMid']:
                boll_score = 70
            elif current_row['close'] > current_row['bollUp']:
                boll_score = 30
            else:
                boll_score = 50
            factor_scores['bollinger_position'] = boll_score
            
            # ä»·æ ¼åŠ¨é‡
            momentum_score = 0
            price_change_5d = (current_row['close'] - df.iloc[max(0, idx-5)]['close']) / df.iloc[max(0, idx-5)]['close'] * 100
            if price_change_5d > 5:
                momentum_score = 80
            elif price_change_5d > 2:
                momentum_score = 60
            elif price_change_5d < -5:
                momentum_score = 30
            else:
                momentum_score = 50
            factor_scores['price_momentum'] = momentum_score * base_weight['momentum']
            
            # æ³¢åŠ¨ç‡è°ƒæ•´
            vol_adj_score = 100
            if current_row['atr_pct'] > 5:
                vol_adj_score = 70
            elif current_row['atr_pct'] > 3:
                vol_adj_score = 85
            factor_scores['volatility_adjustment'] = vol_adj_score * base_weight['volatility']
            
        elif strategy_name == 'volume_breakout':
            # çªç ´å¼ºåº¦
            breakout_score = 0
            if current_row['close'] > current_row['high20']:
                breakout_score = 100
            elif current_row['close'] > current_row['high10']:
                breakout_score = 80
            elif current_row['close'] > current_row['high60']:
                breakout_score = 60
            else:
                breakout_score = 40
            factor_scores['breakout_strength'] = breakout_score * base_weight['momentum']
            
            # æˆäº¤é‡æ¿€å¢
            vol_surge_score = 0
            vol_ratio = current_row['volume'] / current_row['volMa5']
            if vol_ratio > 3:
                vol_surge_score = 100
            elif vol_ratio > 2:
                vol_surge_score = 85
            elif vol_ratio > 1.5:
                vol_surge_score = 70
            else:
                vol_surge_score = 50
            factor_scores['volume_surge'] = vol_surge_score
            
            # è¶‹åŠ¿å¯¹é½
            trend_align_score = 0
            if current_row['ma5'] > current_row['ma20'] > current_row['ma60']:
                trend_align_score = 100
            elif current_row['ma5'] > current_row['ma20']:
                trend_align_score = 75
            elif current_row['ma5'] < current_row['ma20']:
                trend_align_score = 40
            else:
                trend_align_score = 60
            factor_scores['trend_alignment'] = trend_align_score * base_weight['trend']
            
            # æ³¢åŠ¨ç‡è¿‡æ»¤
            vol_filter_score = 100
            if current_row['bollWidth'] < 0.05:  # å¸ƒæ—å¸¦è¿‡çª„ï¼Œçªç ´å¯èƒ½æ— æ•ˆ
                vol_filter_score = 40
            elif current_row['bollWidth'] > 0.2:  # æ³¢åŠ¨è¿‡å¤§ï¼Œé£é™©é«˜
                vol_filter_score = 60
            factor_scores['volatility_filter'] = vol_filter_score * base_weight['volatility']
            
        # å…¶ä»–ç­–ç•¥çš„ä¿¡å·å¼ºåº¦è®¡ç®—å¯ä»¥ç±»ä¼¼å®ç°...
        # ä¸ºäº†ç®€æ´ï¼Œè¿™é‡Œå…ˆå®ç°ä¸¤ä¸ªä¸»è¦ç­–ç•¥ï¼Œå…¶ä»–ç­–ç•¥ä½¿ç”¨åŸºç¡€è¯„åˆ†
        
        else:
            # é»˜è®¤ä¿¡å·å¼ºåº¦è®¡ç®—
            base_strength = signal.get('strength', 50)
            factor_scores['default'] = base_strength
        
        # è®¡ç®—ç»¼åˆå¼ºåº¦
        total_score = 0
        total_weight = 0
        
        for factor, weight in factors.items():
            score = factor_scores.get(factor, 50)
            total_score += score * weight
            total_weight += weight
        
        if total_weight > 0:
            final_strength = min(100, max(20, total_score / total_weight))
        else:
            final_strength = signal.get('strength', 50)
        
        # æ›´æ–°ä¿¡å·å¼ºåº¦
        enhanced_signal = signal.copy()
        enhanced_signal['strength'] = final_strength
        enhanced_signals.append(enhanced_signal)
    
    return enhanced_signals





def calculate_chip_distribution(df, window=360, date=None):
    """è®¡ç®—ç­¹ç åˆ†å¸ƒ"""
    if df is None or len(df) == 0:
        return []
    
    # è½¬æ¢ä¸ºDataFrameï¼ˆå¦‚æœæ˜¯å­—å…¸åˆ—è¡¨ï¼‰
    if isinstance(df, list):
        df = pd.DataFrame(df)
    
    # å¦‚æœæŒ‡å®šäº†æ—¥æœŸï¼Œæ‰¾åˆ°è¯¥æ—¥æœŸçš„æ•°æ®
    if date:
        df['date'] = pd.to_datetime(df['date'])
        target_date = pd.to_datetime(date)
        
        # æ‰¾åˆ°æœ€æ¥è¿‘çš„æ—¥æœŸ
        if target_date in df['date'].values:
            df = df[df['date'] <= target_date]
        else:
            # æ‰¾åˆ°æœ€æ¥è¿‘çš„æ—¥æœŸ
            closest_idx = (df['date'] - target_date).abs().idxmin()
            df = df.iloc[:closest_idx + 1]
    
    # ä½¿ç”¨æœ€è¿‘windowå¤©çš„æ•°æ®
    df = df.tail(window)
    
    if len(df) < 10:
        return []
    
    # è®¡ç®—ä»·æ ¼åŒºé—´
    min_price = df['low'].min()
    max_price = df['high'].max()
    
    if min_price == max_price:
        return []
    
    # åˆ†æˆ20ä¸ªä»·æ ¼åŒºé—´
    num_bins = 120
    price_bins = np.linspace(min_price, max_price, num_bins + 1)
    bin_centers = (price_bins[:-1] + price_bins[1:]) / 2
    chip_volumes = np.zeros(num_bins, dtype=float)
    
    if 'date' in df.columns:
        df = df.copy()
        try:
            df['date'] = pd.to_datetime(df['date'])
            df = df.sort_values('date')
        except Exception:
            pass
    
    vol_ema = None
    for _, row in df.iterrows():
        day_low = float(row.get('low', np.nan))
        day_high = float(row.get('high', np.nan))
        day_volume = float(row.get('volume', 0) or 0)
        day_close = float(row.get('close', np.nan))
        
        if not np.isfinite(day_low) or not np.isfinite(day_high):
            continue
        
        if day_high < day_low:
            day_low, day_high = day_high, day_low

        if np.isfinite(day_volume) and day_volume > 0:
            vol_ema = day_volume if vol_ema is None else (vol_ema * 0.9 + day_volume * 0.1)
        
        turnover = row.get('turnover')
        # ä¸¥æ ¼æ£€æŸ¥ turnover æ˜¯å¦ä¸ºæœ‰æ•ˆæ•°å­—
        turnover_valid = False
        try:
            if pd.notna(turnover) and str(turnover).strip() != '':
                turnover_val = float(turnover)
                if 0 < turnover_val <= 100:
                    turnover_valid = True
                    turnover = turnover_val
        except (ValueError, TypeError):
            pass

        if turnover_valid:
            turnover_ratio = turnover / 100
        elif vol_ema and np.isfinite(day_volume) and day_volume > 0:
            turnover_ratio = 0.01 * (day_volume / vol_ema)
        else:
            turnover_ratio = 0.01
        turnover_ratio = min(max(turnover_ratio, 0.002), 0.08)
        daily_retain = 1 - min(max(turnover_ratio, 0.001), 1)
        chip_volumes *= daily_retain
        
        if day_high == day_low:
            bin_idx = np.searchsorted(price_bins, day_high, side='right') - 1
            if 0 <= bin_idx < num_bins:
                chip_volumes[bin_idx] += turnover_ratio
            continue
        
        if np.isfinite(day_close):
            typical_price = (day_high + day_low + day_close) / 3
        else:
            typical_price = (day_high + day_low) / 2
        
        range_width = day_high - day_low
        sigma = max(range_width / 20, typical_price * 0.002)
        
        if sigma <= 0:
            bin_idx = np.searchsorted(price_bins, typical_price, side='right') - 1
            if 0 <= bin_idx < num_bins:
                chip_volumes[bin_idx] += turnover_ratio
            continue
        
        z = (bin_centers - typical_price) / sigma
        weights = np.exp(-0.5 * z * z)
        weights[(bin_centers < day_low) | (bin_centers > day_high)] = 0
        weights[np.abs(z) > 2.5] = 0
        
        total_w = weights.sum()
        if total_w <= 0:
            continue
        
        chip_volumes += (weights / total_w) * turnover_ratio
    
    total_volume = chip_volumes.sum()
    if total_volume <= 0:
        return []
    
    chip_distribution = []
    for i, price in enumerate(bin_centers):
        volume_in_range = chip_volumes[i]
        concentration = (volume_in_range / total_volume * 100) if total_volume > 0 else 0
        chip_distribution.append({
            'price': round(price, 2),
            'volume': round(float(volume_in_range), 6),
            'concentration': round(concentration, 2)
        })
    
    return chip_distribution


# APIè·¯ç”±
@app.route('/api/local_state', methods=['GET'])
def api_get_local_state():
    key = (request.args.get('key') or '').strip()
    if key not in _ALLOWED_LOCAL_STATE_KEYS:
        return jsonify({'success': False, 'message': 'key ä¸å…è®¸'}), 400
    value = _read_local_state_value(key)
    return jsonify({'success': True, 'key': key, 'value': value})


@app.route('/api/local_state', methods=['POST'])
def api_set_local_state():
    payload = request.get_json(silent=True) or {}
    key = (payload.get('key') or '').strip()
    if key not in _ALLOWED_LOCAL_STATE_KEYS:
        return jsonify({'success': False, 'message': 'key ä¸å…è®¸'}), 400
    value = payload.get('value', None)
    ok = _write_local_state_value(key, value)
    return jsonify({'success': ok, 'key': key})


@app.route('/api/clear_cache', methods=['POST'])
def api_clear_cache():
    payload = request.get_json(silent=True) or {}
    scope = (payload.get('scope') or 'all_data').strip().lower()
    clear_local_state = bool(payload.get('clear_local_state', False))

    removed = {
        'files': 0,
        'dirs': 0,
        'errors': 0,
    }

    def _rm_file(path):
        try:
            if path and os.path.exists(path) and os.path.isfile(path):
                os.remove(path)
                removed['files'] += 1
                return True
        except Exception:
            removed['errors'] += 1
        return False

    def _rm_dir(path, keep_dir=False):
        try:
            if not path or not os.path.exists(path) or not os.path.isdir(path):
                return False
            if keep_dir:
                ok = False
                for name in os.listdir(path):
                    fp = os.path.join(path, name)
                    if os.path.isdir(fp):
                        shutil.rmtree(fp, ignore_errors=True)
                        removed['dirs'] += 1
                        ok = True
                    else:
                        try:
                            os.remove(fp)
                            removed['files'] += 1
                            ok = True
                        except Exception:
                            removed['errors'] += 1
                return ok
            shutil.rmtree(path, ignore_errors=True)
            removed['dirs'] += 1
            return True
        except Exception:
            removed['errors'] += 1
        return False

    try:
        cache_manager.stock_cache = {}
        cache_manager.params_cache = {}
        cache_manager._df_mem_cache = {}
        cache_manager._no_update_until = {}
    except Exception:
        removed['errors'] += 1

    if scope in ('all', 'all_data', 'data'):
        _rm_dir(getattr(cache_manager, 'data_dir', None), keep_dir=True)
        _rm_dir(FUNDAMENTALS_CACHE_DIR, keep_dir=True)
        _rm_dir(CONCEPT_MEMBERS_CACHE_DIR, keep_dir=True)
        _rm_file(STOCK_LIST_CACHE_FILE)
        _rm_file(CONCEPT_LIST_CACHE_FILE)

    if clear_local_state and scope in ('all', 'all_data', 'data'):
        _rm_dir(LOCAL_STATE_DIR, keep_dir=True)

    return jsonify({'success': True, 'scope': scope, 'removed': removed})


@app.route('/api/stock_data', methods=['GET'])
def api_get_stock_data():
    """è·å–è‚¡ç¥¨æ•°æ®ï¼ˆå¸¦ç¼“å­˜ï¼‰ï¼Œæ”¯æŒæ—¥çº¿/å‘¨çº¿/æœˆçº¿"""
    ts_code = request.args.get('ts_code', '000001.SZ')
    start_date = request.args.get('start_date', '2022-01-01')
    end_date = request.args.get('end_date', datetime.now().strftime('%Y-%m-%d'))
    freq = request.args.get('freq', 'D').upper()  # D=æ—¥çº¿, W=å‘¨çº¿, M=æœˆçº¿
    lite = (request.args.get('lite') or '').strip().lower() in ('1', 'true', 'yes', 'y')
    _inc_stat('stock_data_requests', 1)
    if lite:
        _inc_stat('stock_data_lite_requests', 1)
    
    # éªŒè¯ freq å‚æ•°
    if freq not in ['D', 'W', 'M']:
        freq = 'D'
    
    _log(f"APIè¯·æ±‚: ts_code={ts_code}, start_date={start_date}, end_date={end_date}, freq={freq}, lite={lite}")
    
    # ä½¿ç”¨ç¼“å­˜ç®¡ç†å™¨è·å–æ•°æ®
    data = cache_manager.get_stock_data(ts_code, start_date, end_date, freq)
    _log(f"ä»ç¼“å­˜ç®¡ç†å™¨è·å–æ•°æ®ç»“æœ: {type(data)}, é•¿åº¦: {len(data) if data else 'None'}")
    
    if data is None or (isinstance(data, list) and len(data) == 0):
        # å¦‚æœæ— æ³•è·å–çœŸå®æ•°æ®ï¼Œç”Ÿæˆæ¨¡æ‹Ÿæ•°æ®
        _log(f"çœŸå®æ•°æ®è·å–å¤±è´¥ï¼Œç”Ÿæˆæ¨¡æ‹Ÿæ•°æ®: {ts_code}")
        data = TushareDataFetcher.gen_mock_data(ts_code, start_date, end_date)
        
        if data is None or len(data) == 0:
            return jsonify({
                'success': False,
                'message': f'æ— æ³•è·å–è‚¡ç¥¨æ•°æ®: {ts_code}',
                'data': []
            })
    
    # è½¬æ¢ä¸ºDataFrameï¼ˆå¦‚æœæ˜¯å­—å…¸åˆ—è¡¨ï¼‰
    if isinstance(data, list):
        df = pd.DataFrame(data)
    else:
        df = data
    
    # è¡¥å…¨ç¼ºå¤±çš„æ¢æ‰‹ç‡ï¼ˆä½¿ç”¨æˆäº¤é‡ä¼°ç®—ï¼‰
    # df = fill_missing_turnover_with_volume(df)

    market_state = None
    chip_data = []
    if not lite:
        df = calculate_indicators(df)
        market_state = identify_market_state(df)
        chip_data = calculate_chip_distribution(df)
    
    # è½¬æ¢ä¸ºJSON
    data = df.to_dict(orient='records')
    for row in data:
        for key, value in row.items():
            if pd.isna(value):
                row[key] = None
    
    # æ ¹æ®å‘¨æœŸè¿”å›å¯¹åº”çš„å‘¨æœŸæ ‡è¯†
    freq_name = {'D': 'daily', 'W': 'weekly', 'M': 'monthly'}.get(freq, 'daily')
    
    return jsonify({
        'success': True,
        'message': 'ok',
        'ts_code': ts_code,
        'freq': freq,
        'freq_name': freq_name,
        'market_state': market_state,
        'data_count': len(data),
        'data': data,
        'chip_distribution': chip_data,
        'cached': True,
        'lite': lite
    })


@app.route('/api/fundamentals', methods=['GET'])
def api_get_fundamentals():
    ts_code = (request.args.get('ts_code') or '').strip()
    if not ts_code:
        return jsonify({'success': False, 'message': 'ts_code ä¸èƒ½ä¸ºç©º'}), 400

    max_age_days_raw = (request.args.get('max_age_days') or '7').strip()
    try:
        max_age_days = int(max_age_days_raw)
    except Exception:
        max_age_days = 7
    max_age_days = max(0, min(365, max_age_days))

    allow_cache = (request.args.get('cache') or '1').strip() != '0'
    debug = (request.args.get('debug') or '').strip() == '1'

    safe_code = re.sub(r"[^a-zA-Z0-9_.-]", "_", ts_code)[:50] or 'unknown'
    cache_file = os.path.join(FUNDAMENTALS_CACHE_DIR, f"{safe_code}.json")

    if allow_cache:
        cached = _load_json_cache_payload(cache_file)
        if cached:
            try:
                cached_at = datetime.fromisoformat(cached.get('cached_at'))
                if (datetime.now() - cached_at).days <= max_age_days:
                    return jsonify({'success': True, 'ts_code': ts_code, 'cached': True, 'data': cached.get('data')})
            except Exception:
                pass

    pro_client = _get_pro()
    if pro_client is None:
        return jsonify({'success': False, 'message': 'æœªé…ç½®Tushare Tokenï¼Œæ— æ³•è·å–åŸºæœ¬é¢æ•°æ®', 'ts_code': ts_code, 'cached': False, 'data': None})

    def _fmt_ymd(v):
        s = str(v or '').strip()
        if len(s) == 8 and s.isdigit():
            return f"{s[0:4]}-{s[4:6]}-{s[6:8]}"
        return s

    now = datetime.now()
    end_ymd = now.strftime('%Y%m%d')
    fundamentals = {'ts_code': ts_code}
    sources = {'daily_basic': False, 'fina_indicator': False, 'cashflow': False}

    try:
        start_basic = (now - timedelta(days=90)).strftime('%Y%m%d')
        df_basic = pro_client.daily_basic(
            ts_code=ts_code,
            start_date=start_basic,
            end_date=end_ymd,
            fields='ts_code,trade_date,pe,pb,total_mv,circ_mv,turnover_rate'
        )
        if df_basic is not None and not df_basic.empty:
            df_basic = df_basic.sort_values('trade_date').reset_index(drop=True)
            row = df_basic.iloc[-1].to_dict()
            fundamentals.update({
                'trade_date': _fmt_ymd(row.get('trade_date')),
                'pe': row.get('pe'),
                'pb': row.get('pb'),
                'turnover_rate': row.get('turnover_rate'),
                'total_mv': row.get('total_mv'),
                'circ_mv': row.get('circ_mv'),
            })
            sources['daily_basic'] = True
    except Exception as e:
        if debug:
            fundamentals['daily_basic_error'] = str(e)

    try:
        start_fina = (now - timedelta(days=730)).strftime('%Y%m%d')
        df_fina = pro_client.fina_indicator(
            ts_code=ts_code,
            start_date=start_fina,
            end_date=end_ymd,
            fields='ts_code,end_date,roe,roe_dt,roa,netprofit_yoy,dt_netprofit_yoy,or_yoy,grossprofit_margin,debt_to_assets'
        )
        if df_fina is not None and not df_fina.empty:
            df_fina = df_fina.sort_values('end_date').reset_index(drop=True)
            row = df_fina.iloc[-1].to_dict()
            fundamentals.update({
                'report_end_date': _fmt_ymd(row.get('end_date')),
                'roe': row.get('roe'),
                'roe_dt': row.get('roe_dt'),
                'roa': row.get('roa'),
                'netprofit_yoy': row.get('netprofit_yoy'),
                'dt_netprofit_yoy': row.get('dt_netprofit_yoy'),
                'or_yoy': row.get('or_yoy'),
                'grossprofit_margin': row.get('grossprofit_margin'),
                'debt_to_assets': row.get('debt_to_assets'),
            })
            sources['fina_indicator'] = True
    except Exception as e:
        if debug:
            fundamentals['fina_indicator_error'] = str(e)

    try:
        start_cf = (now - timedelta(days=730)).strftime('%Y%m%d')
        df_cf = pro_client.cashflow(
            ts_code=ts_code,
            start_date=start_cf,
            end_date=end_ymd,
            fields='ts_code,end_date,n_cashflow_act'
        )
        if df_cf is not None and not df_cf.empty:
            df_cf = df_cf.sort_values('end_date').reset_index(drop=True)
            row = df_cf.iloc[-1].to_dict()
            fundamentals.update({
                'cashflow_end_date': _fmt_ymd(row.get('end_date')),
                'n_cashflow_act': row.get('n_cashflow_act'),
            })
            sources['cashflow'] = True
    except Exception as e:
        if debug:
            fundamentals['cashflow_error'] = str(e)

    fundamentals['sources'] = sources
    _save_json_cache_payload(cache_file, fundamentals)
    return jsonify({'success': True, 'ts_code': ts_code, 'cached': False, 'data': fundamentals})


@app.route('/api/index_stocks', methods=['GET'])
def api_get_index_stocks():
    """è·å–æŒ‡æ•°æˆåˆ†è‚¡"""
    index_code = request.args.get('index_code', '000300.SH')
    
    # æ£€æŸ¥ç¼“å­˜
    cache_key = f'index_stocks_{index_code}'
    cached = cache_manager.get('params', cache_key)
    if cached:
        cached_at = datetime.fromisoformat(cached['meta']['cached_at'])
        if (datetime.now() - cached_at).days < 7: # ç¼“å­˜7å¤©
            cached_data = cached.get('data')
            expected = 50 if index_code == '000016.SH' else (300 if index_code == '000300.SH' else None)
            if expected and isinstance(cached_data, list) and len(cached_data) < max(10, expected // 2):
                cached_data = None
            if cached_data is not None:
                return jsonify({'success': True, 'count': len(cached_data) if isinstance(cached_data, list) else 0, 'data': cached_data})
            
    data = TushareDataFetcher.get_index_weights(index_code)
    
    # å¦‚æœè·å–å¤±è´¥ï¼Œè¿”å›ä¸€äº›æ¨¡æ‹Ÿæ•°æ®ä½œä¸º fallback
    if not data:
        if index_code == '000016.SH': # ä¸Šè¯50
            data = [
                {'ts_code': '600519.SH', 'name': 'è´µå·èŒ…å°'}, {'ts_code': '601318.SH', 'name': 'ä¸­å›½å¹³å®‰'},
                {'ts_code': '600036.SH', 'name': 'æ‹›å•†é“¶è¡Œ'}, {'ts_code': '600276.SH', 'name': 'æ’ç‘åŒ»è¯'},
                {'ts_code': '600030.SH', 'name': 'ä¸­ä¿¡è¯åˆ¸'}, {'ts_code': '601012.SH', 'name': 'éš†åŸºç»¿èƒ½'}
            ]
        elif index_code == '000300.SH': # æ²ªæ·±300
             data = [
                {'ts_code': '600519.SH', 'name': 'è´µå·èŒ…å°'}, {'ts_code': '300750.SZ', 'name': 'å®å¾·æ—¶ä»£'},
                {'ts_code': '000858.SZ', 'name': 'äº”ç²®æ¶²'}, {'ts_code': '002594.SZ', 'name': 'æ¯”äºšè¿ª'},
             ]
             
    if data:
        cache_manager.set('params', cache_key, data)
        
    return jsonify({'success': True, 'count': len(data) if data else 0, 'data': data})


@app.route('/api/stock_list', methods=['GET'])
def api_get_stock_list():
    """è·å–è‚¡ç¥¨åˆ—è¡¨ï¼ˆå¸¦ç¼“å­˜ï¼‰"""
    file_cached = _load_json_cache_file(STOCK_LIST_CACHE_FILE)
    if file_cached:
        try:
            cached_at = datetime.fromisoformat(file_cached['cached_at'])
            if (datetime.now() - cached_at).days < 30 and len(file_cached['data']) >= 2000:
                return jsonify({
                    'success': True,
                    'count': len(file_cached['data']),
                    'data': file_cached['data'],
                    'cached': True
                })
        except Exception:
            pass

    cache_key = 'stock_list'
    cached = cache_manager.get('params', cache_key)
    
    if cached:
        # æ£€æŸ¥ç¼“å­˜æ˜¯å¦è¶…è¿‡7å¤©
        cached_at = datetime.fromisoformat(cached['meta']['cached_at'])
        if (datetime.now() - cached_at).days < 7:
            print("[ç¼“å­˜å‘½ä¸­] è‚¡ç¥¨åˆ—è¡¨")
            return jsonify({
                'success': True,
                'count': len(cached['data']),
                'data': cached['data'],
                'cached': True
            })
    
    pro_client = _get_pro()
    if pro_client is None:
        default_stocks = [
            {'ts_code': '000001.SZ', 'symbol': '000001', 'name': 'å¹³å®‰é“¶è¡Œ', 'industry': 'é“¶è¡Œ'},
            {'ts_code': '000002.SZ', 'symbol': '000002', 'name': 'ä¸‡ç§‘A', 'industry': 'æˆ¿åœ°äº§'},
            {'ts_code': '600519.SH', 'symbol': '600519', 'name': 'è´µå·èŒ…å°', 'industry': 'ç™½é…’'},
            {'ts_code': '000858.SZ', 'symbol': '000858', 'name': 'äº”ç²®æ¶²', 'industry': 'ç™½é…’'},
            {'ts_code': '601318.SH', 'symbol': '601318', 'name': 'ä¸­å›½å¹³å®‰', 'industry': 'ä¿é™©'},
            {'ts_code': '600036.SH', 'symbol': '600036', 'name': 'æ‹›å•†é“¶è¡Œ', 'industry': 'é“¶è¡Œ'},
            {'ts_code': '000333.SZ', 'name': 'ç¾çš„é›†å›¢', 'industry': 'å®¶ç”µ'},
            {'ts_code': '300750.SZ', 'name': 'å®å¾·æ—¶ä»£', 'industry': 'ç”µæ± '},
        ]
        return jsonify({
            'success': True,
            'count': len(default_stocks),
            'data': default_stocks,
            'cached': False,
            'message': 'æœªé…ç½®Tushare Tokenï¼Œè¡Œä¸š/è‚¡ç¥¨åˆ—è¡¨ä»…æä¾›å°‘é‡é»˜è®¤æ•°æ®ã€‚'
        })

    try:
        # å°è¯•è·å–è‚¡ç¥¨åˆ—è¡¨ï¼Œä¸é™åˆ¶æ•°é‡ä»¥è·å–å…¨é‡è¡Œä¸šä¿¡æ¯
        df = pro_client.stock_basic(
            exchange='',
            list_status='L',
            fields='ts_code,symbol,name,industry'
        )
        
        if df is not None and not df.empty:
            stocks = df.to_dict(orient='records')
            cache_manager.set('params', cache_key, stocks)
            _save_json_cache_file(STOCK_LIST_CACHE_FILE, stocks)
            print(f"[æ›´æ–°ç¼“å­˜] è‚¡ç¥¨åˆ—è¡¨: {len(stocks)} åª")
            return jsonify({
                'success': True,
                'count': len(stocks),
                'data': stocks,
                'cached': False
            })
    except Exception as e:
        print(f"è·å–è‚¡ç¥¨åˆ—è¡¨é”™è¯¯: {e}")
    
    # è¿”å›é»˜è®¤åˆ—è¡¨ï¼ˆå½“APIå¤±è´¥æ—¶ï¼‰
    default_stocks = [
        {'ts_code': '000001.SZ', 'symbol': '000001', 'name': 'å¹³å®‰é“¶è¡Œ', 'industry': 'é“¶è¡Œ'},
        {'ts_code': '000002.SZ', 'symbol': '000002', 'name': 'ä¸‡ç§‘A', 'industry': 'æˆ¿åœ°äº§'},
        {'ts_code': '600519.SH', 'symbol': '600519', 'name': 'è´µå·èŒ…å°', 'industry': 'ç™½é…’'},
        {'ts_code': '000858.SZ', 'symbol': '000858', 'name': 'äº”ç²®æ¶²', 'industry': 'ç™½é…’'},
        {'ts_code': '601318.SH', 'symbol': '601318', 'name': 'ä¸­å›½å¹³å®‰', 'industry': 'ä¿é™©'},
        {'ts_code': '600036.SH', 'symbol': '600036', 'name': 'æ‹›å•†é“¶è¡Œ', 'industry': 'é“¶è¡Œ'},
        {'ts_code': '000333.SZ', 'name': 'ç¾çš„é›†å›¢', 'industry': 'å®¶ç”µ'},
        {'ts_code': '300750.SZ', 'name': 'å®å¾·æ—¶ä»£', 'industry': 'ç”µæ± '},
    ]
    return jsonify({
        'success': True,
        'count': len(default_stocks),
        'data': default_stocks,
        'cached': False,
        'message': 'è‚¡ç¥¨åˆ—è¡¨è·å–å¤±è´¥ï¼Œå·²å›é€€åˆ°å†…ç½®å°‘é‡åˆ—è¡¨ã€‚'
    })


@app.route('/api/concept_list', methods=['GET'])
def api_get_concept_list():
    file_cached = _load_json_cache_file(CONCEPT_LIST_CACHE_FILE)
    if file_cached:
        try:
            cached_at = datetime.fromisoformat(file_cached['cached_at'])
            if (datetime.now() - cached_at).days < 30 and len(file_cached['data']) >= 200:
                return jsonify({'success': True, 'count': len(file_cached['data']), 'data': file_cached['data'], 'cached': True})
        except Exception:
            pass

    pro_client = _get_pro()
    if pro_client is None:
        return jsonify({'success': True, 'count': 0, 'data': [], 'cached': False, 'message': 'æœªé…ç½®Tushare Tokenï¼Œæ— æ³•æ‹‰å–æ¦‚å¿µåˆ—è¡¨ã€‚'})

    try:
        df = None
        try:
            concept_func = getattr(pro_client, 'concept', None)
            if callable(concept_func):
                try:
                    df = concept_func(src='ts')
                except Exception:
                    df = concept_func()
            else:
                try:
                    df = pro_client.query('concept', src='ts')
                except Exception:
                    df = pro_client.query('concept')
        except Exception:
            df = pro_client.query('concept')
        if df is None or df.empty:
            return jsonify({'success': True, 'count': 0, 'data': [], 'cached': False})

        records = df.to_dict(orient='records')
        normalized = []
        for r in records:
            name = r.get('name') or r.get('concept_name') or r.get('ts_name')
            code = r.get('code') or r.get('concept_code') or r.get('ts_code')
            if not name:
                continue
            normalized.append({'name': str(name), 'code': str(code) if code is not None else ''})

        normalized.sort(key=lambda x: x.get('name', ''))
        _save_json_cache_file(CONCEPT_LIST_CACHE_FILE, normalized)
        return jsonify({'success': True, 'count': len(normalized), 'data': normalized, 'cached': False})
    except Exception as e:
        print(f"è·å–æ¦‚å¿µåˆ—è¡¨é”™è¯¯: {e}")
        debug = (request.args.get('debug') or '').strip() == '1'
        payload = {'success': True, 'count': 0, 'data': [], 'cached': False, 'message': 'æ¦‚å¿µåˆ—è¡¨è·å–å¤±è´¥'}
        if debug:
            payload['error'] = str(e)
        return jsonify(payload)


@app.route('/api/concept_members', methods=['GET'])
def api_get_concept_members():
    codes_raw = request.args.get('codes', '') or ''
    codes = [c.strip() for c in codes_raw.split(',') if c.strip()]
    if not codes:
        return jsonify({'success': False, 'message': 'codes ä¸èƒ½ä¸ºç©º', 'ts_codes': [], 'count': 0}), 400

    pro_client = _get_pro()
    if pro_client is None:
        return jsonify({'success': True, 'message': 'æœªé…ç½®Tushare Tokenï¼Œæ— æ³•æ‹‰å–æ¦‚å¿µæˆåˆ†è‚¡ã€‚', 'ts_codes': [], 'count': 0, 'cached': False})

    all_ts_codes = set()
    by_code = {}
    any_cached = False

    for concept_code in codes[:50]:
        cache_file = os.path.join(CONCEPT_MEMBERS_CACHE_DIR, f"{concept_code}.json")
        file_cached = _load_json_cache_file(cache_file)
        if file_cached:
            try:
                cached_at = datetime.fromisoformat(file_cached['cached_at'])
                if (datetime.now() - cached_at).days < 30 and len(file_cached['data']) >= 1:
                    members = file_cached['data']
                    by_code[concept_code] = members
                    for ts in members:
                        all_ts_codes.add(ts)
                    any_cached = True
                    continue
            except Exception:
                pass

        try:
            df = None
            try:
                detail_func = getattr(pro_client, 'concept_detail', None)
                if callable(detail_func):
                    try:
                        df = detail_func(id=concept_code)
                    except Exception:
                        df = detail_func(concept_id=concept_code)
                else:
                    try:
                        df = pro_client.query('concept_detail', id=concept_code)
                    except Exception:
                        df = pro_client.query('concept_detail', concept_id=concept_code)
            except Exception:
                df = pro_client.query('concept_detail', id=concept_code)
            if df is None or df.empty:
                by_code[concept_code] = []
                continue
            records = df.to_dict(orient='records')
            members = []
            for r in records:
                ts_code = r.get('ts_code') or r.get('con_code')
                if not ts_code:
                    continue
                members.append(str(ts_code))
                all_ts_codes.add(str(ts_code))
            members = sorted(list(set(members)))
            by_code[concept_code] = members
            _save_json_cache_file(cache_file, members)
        except Exception as e:
            print(f"è·å–æ¦‚å¿µæˆåˆ†è‚¡å¤±è´¥: {concept_code} {e}")
            by_code[concept_code] = []

    out = sorted(all_ts_codes)
    return jsonify({'success': True, 'count': len(out), 'ts_codes': out, 'by_code': by_code, 'cached': any_cached})


@app.route('/api/best_strategy', methods=['POST'])
def api_best_strategy():
    payload = request.get_json(silent=True) or {}

    ts_codes = payload.get('ts_codes') or []
    if not isinstance(ts_codes, list) or len(ts_codes) == 0:
        return jsonify({'success': False, 'message': 'ts_codes ä¸èƒ½ä¸ºç©ºä¸”å¿…é¡»ä¸ºæ•°ç»„', 'data': []}), 400

    if len(ts_codes) > 300:
        ts_codes = ts_codes[:300]

    start_date = payload.get('start_date', '2022-01-01')
    end_date = payload.get('end_date', datetime.now().strftime('%Y-%m-%d'))
    signal_period = int(payload.get('signal_period', 3))
    history_window = int(payload.get('history_window', 180))
    init_capital = float(payload.get('init_capital', 1000000))

    strategies = payload.get('strategies') or list(StrategyEngine.STRATEGIES.keys())
    if not isinstance(strategies, list) or len(strategies) == 0:
        strategies = list(StrategyEngine.STRATEGIES.keys())

    base_config = payload.get('config') or {}
    if not isinstance(base_config, dict):
        base_config = {}

    results = []

    for ts_code in ts_codes:
        try:
            data = cache_manager.get_stock_data(ts_code, start_date, end_date) or []
            if not data or len(data) < 80:
                mock = TushareDataFetcher.gen_mock_data(ts_code, start_date, end_date)
                if mock and len(mock) >= 80:
                    data = mock
                else:
                    continue

            df = pd.DataFrame(data)
            df = calculate_indicators(df)
            records = df.where(pd.notnull(df), None).to_dict('records')
            if not records:
                continue

            last_date_str = records[-1].get('date')
            if not last_date_str:
                continue

            last_dt = datetime.strptime(last_date_str, '%Y-%m-%d')
            recent_cutoff = last_dt - timedelta(days=signal_period)
            history_start = last_dt - timedelta(days=history_window)

            history_records = [
                r for r in records
                if r.get('date') and history_start <= datetime.strptime(r['date'], '%Y-%m-%d') <= recent_cutoff
            ]

            best = None

            for strategy in strategies:
                config = {**base_config}
                if 'takeProfit' not in config:
                    config['takeProfit'] = 0.15
                if 'stopLoss' not in config:
                    config['stopLoss'] = 0.08
                if 'volumeMulti' not in config:
                    config['volumeMulti'] = 1.5

                signals = StrategyEngine.execute_strategy(records, strategy, config)
                if not signals:
                    continue

                recent_buys = [
                    s for s in signals
                    if s.get('type') == 'buy' and s.get('date') and datetime.strptime(s['date'], '%Y-%m-%d') >= recent_cutoff
                ]
                if not recent_buys:
                    continue

                recent_buy_date = max(s['date'] for s in recent_buys if s.get('date'))

                hist_score = 0
                hist_return = 0
                hist_win_rate = 0
                hist_trade_count = 0

                if len(history_records) >= 80:
                    hist_signals = StrategyEngine.execute_strategy(history_records, strategy, config)
                    hist_res = StrategyEngine.calculate_backtest(history_records, hist_signals, init_capital)
                    hist_score = float(hist_res.get('score') or 0)
                    hist_return = float(hist_res.get('totalReturn') or 0)
                    hist_win_rate = float(hist_res.get('winRate') or 0)
                    hist_trade_count = int(hist_res.get('tradeCount') or 0)

                cand = {
                    'ts_code': ts_code,
                    'best_strategy': strategy,
                    'recent_buy_date': recent_buy_date,
                    'history_score': hist_score,
                    'history_return': hist_return,
                    'history_win_rate': hist_win_rate,
                    'history_trade_count': hist_trade_count
                }

                if best is None or cand['history_score'] > best['history_score'] or (
                    cand['history_score'] == best['history_score'] and cand['history_return'] > best['history_return']
                ):
                    best = cand

            if best is not None:
                meta = StrategyEngine.STRATEGIES.get(best['best_strategy'], {})
                best['best_strategy_name'] = meta.get('name', best['best_strategy'])
                best['best_strategy_icon'] = meta.get('icon', '')
                results.append(best)
        except Exception as e:
            print(f"best_strategy error: {ts_code} {e}")
            continue

    results.sort(key=lambda x: (x.get('history_score', 0), x.get('history_return', 0)), reverse=True)
    return jsonify({'success': True, 'count': len(results), 'data': results})


# ä»£è¡¨æ€§è‚¡ç¥¨åˆ—è¡¨ï¼ˆæ¯è¡Œä¸š1åªï¼Œç”¨äºæ‰¹é‡å‚æ•°ä¼˜åŒ– - ç²¾ç®€ç‰ˆä»¥æé«˜é€Ÿåº¦ï¼‰
REPRESENTATIVE_STOCKS = {
    'é“¶è¡Œ': ['000001.SZ'],        # å¹³å®‰é“¶è¡Œï¼ˆä½æ³¢åŠ¨ï¼‰
    'ç§‘æŠ€': ['300750.SZ'],        # å®å¾·æ—¶ä»£ï¼ˆé«˜æ³¢åŠ¨ï¼‰
    'åŒ»è¯': ['600276.SH'],        # æ’ç‘åŒ»è¯ï¼ˆä¸­æ³¢åŠ¨ï¼‰
    'ç™½é…’': ['600519.SH'],        # è´µå·èŒ…å°ï¼ˆè¶‹åŠ¿æ€§å¼ºï¼‰
    'æ–°èƒ½æº': ['002594.SZ'],      # æ¯”äºšè¿ªï¼ˆé«˜æ³¢åŠ¨ï¼‰
    'ç”µå­': ['002415.SZ'],        # æµ·åº·å¨è§†ï¼ˆä¸­æ³¢åŠ¨ï¼‰
    'åŒ–å·¥': ['600309.SH'],        # ä¸‡ååŒ–å­¦ï¼ˆå‘¨æœŸè‚¡ï¼‰
    'æœ‰è‰²': ['601899.SH'],        # ç´«é‡‘çŸ¿ä¸šï¼ˆèµ„æºè‚¡ï¼‰
}


class AutoParamOptimizer:
    """è‡ªåŠ¨æ‰¹é‡å‚æ•°ä¼˜åŒ–å™¨"""
    
    @staticmethod
    def get_industry_stocks():
        """è·å–ä»£è¡¨æ€§è‚¡ç¥¨åˆ—è¡¨"""
        stocks = []
        for industry, codes in REPRESENTATIVE_STOCKS.items():
            for code in codes:
                stocks.append({
                    'ts_code': code,
                    'industry': industry
                })
        return stocks
    
    @staticmethod
    def optimize_stock_params(ts_code, industry, start_date, end_date, strategies):
        """å¯¹å•åªè‚¡ç¥¨è¿›è¡Œå…¨ç­–ç•¥å‚æ•°ä¼˜åŒ–"""
        try:
            print(f"å¼€å§‹ä¼˜åŒ– {ts_code} ({industry})...")
            
            # è·å–è‚¡ç¥¨æ•°æ®
            data = cache_manager.get_stock_data(ts_code, start_date, end_date) or []
            print(f"  ä»ç¼“å­˜è·å–æ•°æ®: {len(data)} æ¡")
            
            if not data or len(data) < 30:  # é™ä½æ•°æ®è¦æ±‚
                print(f"  ç¼“å­˜æ•°æ®ä¸è¶³ï¼Œç”Ÿæˆmockæ•°æ®...")
                mock = TushareDataFetcher.gen_mock_data(ts_code, start_date, end_date)
                if mock and len(mock) >= 30:
                    data = mock
                    print(f"  ç”Ÿæˆmockæ•°æ®: {len(data)} æ¡")
                else:
                    print(f"  âŒ æ•°æ®ä¸è¶³ï¼Œè·³è¿‡ {ts_code}")
                    return None
            
            df = pd.DataFrame(data)
            df = calculate_indicators(df)
            
            # åˆ†æè‚¡ç¥¨ç‰¹å¾
            stock_features = StockFeatureAnalyzer.analyze_stock_features(df)
            records = df.where(pd.notnull(df), None).to_dict('records')
            
            results = {}
            
            for strategy in strategies:
                # è·å–è¯¥ç­–ç•¥çš„å‚æ•°ç½‘æ ¼
                param_grid = AutoParamOptimizer.get_param_grid(strategy)
                
                best_result = None
                best_params = None
                best_score = -999
                
                # éå†å‚æ•°ç»„åˆ
                for params in param_grid:
                    config = {**params}
                    
                    # æ‰§è¡Œç­–ç•¥
                    signals = StrategyEngine.execute_strategy(records, strategy, config, stock_features)
                    
                    if signals:
                        result = StrategyEngine.calculate_backtest(records, signals, 1000000)
                        score = result.get('score', 0)
                        
                        if score > best_score:
                            best_score = score
                            best_result = result
                            best_params = params
                
                if best_params:
                    results[strategy] = {
                        'best_params': best_params,
                        'best_result': {
                            'totalReturn': best_result.get('totalReturn', 0),
                            'maxDrawdown': best_result.get('maxDrawdown', 0),
                            'sharpe': best_result.get('sharpe', 0),
                            'winRate': best_result.get('winRate', 0),
                            'score': best_score,
                            'tradeCount': best_result.get('tradeCount', 0)
                        }
                    }
            
            return {
                'ts_code': ts_code,
                'industry': industry,
                'features': stock_features,
                'strategy_results': results
            }
            
        except Exception as e:
            print(f"ä¼˜åŒ–è‚¡ç¥¨å‚æ•°å¤±è´¥ {ts_code}: {e}")
            return None
    
    @staticmethod
    def get_param_grid(strategy):
        """è·å–ç­–ç•¥çš„å‚æ•°ç½‘æ ¼ï¼ˆæ‰©å±•ç‰ˆä»¥æé«˜æ•ˆæœï¼‰"""
        base_params = CrossStockParamOptimizer.UNIVERSAL_PARAMS.get(strategy, {})

        # å®šä¹‰å‚æ•°æœç´¢èŒƒå›´ï¼ˆæ‰©å¤§èŒƒå›´ä»¥æé«˜æ•ˆæœï¼‰
        param_ranges = {
            'takeProfit': [0.08, 0.10, 0.12, 0.15, 0.18, 0.20, 0.25],
            'stopLoss': [0.03, 0.05, 0.06, 0.08, 0.10, 0.12, 0.15],
            'volumeMulti': [1.2, 1.5, 1.8, 2.0, 2.5, 3.0],
        }

        # ç”Ÿæˆå‚æ•°ç»„åˆ
        import itertools
        keys = list(param_ranges.keys())
        values = [param_ranges[k] for k in keys]

        grids = []
        for combo in itertools.product(*values):
            params = dict(zip(keys, combo))
            # æ·»åŠ ç­–ç•¥ç‰¹æœ‰å‚æ•°é»˜è®¤å€¼
            params.update(base_params)
            grids.append(params)

        return grids
    
    @staticmethod
    def build_param_system(optimization_results):
        """æ ¹æ®ä¼˜åŒ–ç»“æœæ„å»ºå‚æ•°ä½“ç³»"""
        industry_params = {}
        
        # æŒ‰è¡Œä¸šå½’ç±»
        for result in optimization_results:
            industry = result['industry']
            features = result['features']
            strategy_results = result['strategy_results']
            
            if industry not in industry_params:
                industry_params[industry] = {
                    'stocks': [],
                    'strategies': {}
                }
            
            industry_params[industry]['stocks'].append({
                'ts_code': result['ts_code'],
                'features': features
            })
            
            # ç´¯åŠ ç­–ç•¥å‚æ•°
            for strategy, data in strategy_results.items():
                if strategy not in industry_params[industry]['strategies']:
                    industry_params[industry]['strategies'][strategy] = {
                        'params_list': [],
                        'scores': []
                    }
                
                industry_params[industry]['strategies'][strategy]['params_list'].append(
                    data['best_params']
                )
                industry_params[industry]['strategies'][strategy]['scores'].append(
                    data['best_result']['score']
                )
        
        # é€‰æ‹©æ¯ä¸ªç­–ç•¥çš„æœ€ä¼˜å‚æ•°ï¼ˆå¾—åˆ†æœ€é«˜çš„é‚£ç»„ï¼‰
        param_system = {}
        for industry, data in industry_params.items():
            param_system[industry] = {}
            
            for strategy, strategy_data in data['strategies'].items():
                params_list = strategy_data['params_list']
                scores = strategy_data['scores']
                
                if not params_list:
                    continue
                
                # æ‰¾åˆ°å¾—åˆ†æœ€é«˜çš„å‚æ•°
                max_score_idx = scores.index(max(scores))
                best_params = params_list[max_score_idx]
                
                param_system[industry][strategy] = best_params
        
        return param_system


# å…¨å±€è¿›åº¦å­˜å‚¨
auto_optimize_progress = {
    'is_running': False,
    'current': 0,
    'total': 0,
    'current_stock': '',
    'results': []
}


@app.route('/api/auto_optimize_params', methods=['POST'])
def api_auto_optimize_params():
    """è‡ªåŠ¨æ‰¹é‡å‚æ•°ä¼˜åŒ–API - å¸¦è¿›åº¦åé¦ˆ"""
    global auto_optimize_progress

    payload = request.get_json(silent=True) or {}

    start_date = payload.get('start_date', '2023-01-01')
    end_date = payload.get('end_date', datetime.now().strftime('%Y-%m-%d'))
    # é»˜è®¤ä¼˜åŒ–æ‰€æœ‰ç­–ç•¥
    all_strategies = list(StrategyEngine.STRATEGIES.keys())
    strategies = payload.get('strategies', all_strategies)

    # è·å–ä»£è¡¨æ€§è‚¡ç¥¨ï¼ˆ8ä¸ªè¡Œä¸šï¼‰
    stocks = AutoParamOptimizer.get_industry_stocks()

    # åˆå§‹åŒ–è¿›åº¦
    auto_optimize_progress = {
        'is_running': True,
        'current': 0,
        'total': len(stocks),
        'current_stock': '',
        'results': []
    }

    print(f"å¼€å§‹è‡ªåŠ¨æ‰¹é‡å‚æ•°ä¼˜åŒ–ï¼Œè‚¡ç¥¨æ•°é‡: {len(stocks)}, ç­–ç•¥: {strategies}")

    optimization_results = []

    for i, stock in enumerate(stocks):
        # æ›´æ–°è¿›åº¦
        auto_optimize_progress['current'] = i + 1
        auto_optimize_progress['current_stock'] = stock['ts_code']

        print(f"ä¼˜åŒ–è¿›åº¦: {i+1}/{len(stocks)} - {stock['ts_code']} ({stock['industry']})")

        try:
            # è°ƒç”¨çœŸå®çš„å‚æ•°ä¼˜åŒ–æ–¹æ³•
            result = AutoParamOptimizer.optimize_stock_params(
                stock['ts_code'],
                stock['industry'],
                start_date,
                end_date,
                strategies
            )

            if result:
                optimization_results.append(result)
                auto_optimize_progress['results'].append(result)
                print(f"  âœ“ {stock['ts_code']} ä¼˜åŒ–æˆåŠŸ")
            else:
                print(f"  âœ— {stock['ts_code']} ä¼˜åŒ–å¤±è´¥ï¼ˆæ— ç»“æœï¼‰")

        except Exception as e:
            print(f"  âœ— {stock['ts_code']} ä¼˜åŒ–å¼‚å¸¸: {e}")
            continue

    print(f"ä¼˜åŒ–å®Œæˆï¼ŒæˆåŠŸ: {len(optimization_results)}/{len(stocks)} åªè‚¡ç¥¨")

    # æ„å»ºå‚æ•°ä½“ç³»
    param_system = AutoParamOptimizer.build_param_system(optimization_results)

    # è½¬æ¢ä¸ºå‰ç«¯éœ€è¦çš„æ ¼å¼ï¼š{strategy: {bestParams: {...}}}
    strategy_params = {}
    for industry, strategies in param_system.items():
        for strategy, params in strategies.items():
            if strategy not in strategy_params:
                strategy_params[strategy] = {
                    'bestParams': params,
                    'industries': {}
                }
            # ä¿å­˜æ¯ä¸ªè¡Œä¸šçš„å‚æ•°
            strategy_params[strategy]['industries'][industry] = params

    # æ ‡è®°å®Œæˆ
    auto_optimize_progress['is_running'] = False

    return jsonify({
        'success': True,
        'message': f'å·²å®Œæˆ{len(optimization_results)}åªè‚¡ç¥¨çš„å‚æ•°ä¼˜åŒ–',
        'optimized_stocks': [r['ts_code'] for r in optimization_results],
        'industries': list(param_system.keys()),
        'param_system': param_system,
        'strategy_params': strategy_params  # å‰ç«¯å¯ä»¥ç›´æ¥ä½¿ç”¨çš„æ ¼å¼
    })


@app.route('/api/auto_optimize_progress', methods=['GET'])
def api_auto_optimize_progress():
    """è·å–è‡ªåŠ¨æ‰¹é‡å‚æ•°ä¼˜åŒ–è¿›åº¦"""
    return jsonify({
        'success': True,
        'is_running': auto_optimize_progress['is_running'],
        'current': auto_optimize_progress['current'],
        'total': auto_optimize_progress['total'],
        'current_stock': auto_optimize_progress['current_stock'],
        'progress_percent': round(auto_optimize_progress['current'] / auto_optimize_progress['total'] * 100, 1) if auto_optimize_progress['total'] > 0 else 0
    })


# é…ç½®æ–‡ä»¶è·¯å¾„
CONFIG_DIR = os.path.join(os.path.dirname(os.path.abspath(__file__)), 'config')
CONFIG_FILE = os.path.join(CONFIG_DIR, 'user_config.json')
_CONFIG_LOCK = threading.Lock()

def ensure_config_dir():
    """ç¡®ä¿é…ç½®ç›®å½•å­˜åœ¨"""
    if not os.path.exists(CONFIG_DIR):
        os.makedirs(CONFIG_DIR)

def _atomic_write_json(file_path, data_obj):
    ensure_config_dir()
    dir_path = os.path.dirname(file_path)
    prefix = os.path.basename(file_path) + '.'
    tmp_path = None
    try:
        fd, tmp_path = tempfile.mkstemp(prefix=prefix, suffix='.tmp', dir=dir_path)
        with os.fdopen(fd, 'w', encoding='utf-8') as f:
            json.dump(data_obj, f, ensure_ascii=False, indent=2)
            f.flush()
            try:
                os.fsync(f.fileno())
            except Exception:
                pass
        os.replace(tmp_path, file_path)
    finally:
        if tmp_path and os.path.exists(tmp_path):
            try:
                os.remove(tmp_path)
            except Exception:
                pass

@app.route('/api/config', methods=['GET'])
def api_get_config():
    """ä»æ–‡ä»¶åŠ è½½ç”¨æˆ·é…ç½®"""
    try:
        ensure_config_dir()
        if os.path.exists(CONFIG_FILE):
            with _CONFIG_LOCK:
                with open(CONFIG_FILE, 'r', encoding='utf-8') as f:
                    config = json.load(f)
            print(f"[è·å–é…ç½®] ä»æ–‡ä»¶åŠ è½½: {CONFIG_FILE}, keys: {list(config.keys())}")
            resp = jsonify({'success': True, 'config': config})
            resp.headers['Cache-Control'] = 'no-store'
            return resp
        else:
            print(f"[è·å–é…ç½®] é…ç½®æ–‡ä»¶ä¸å­˜åœ¨: {CONFIG_FILE}")
            resp = jsonify({'success': True, 'config': {}})
            resp.headers['Cache-Control'] = 'no-store'
            return resp
    except Exception as e:
        print(f"[è·å–é…ç½®] é”™è¯¯: {e}")
        resp = jsonify({'success': False, 'message': str(e)})
        resp.headers['Cache-Control'] = 'no-store'
        return resp

@app.route('/api/config', methods=['POST'])
def api_save_config():
    """ä¿å­˜ç”¨æˆ·é…ç½®åˆ°æ–‡ä»¶"""
    try:
        ensure_config_dir()
        config = request.get_json(silent=True) or {}
        with _CONFIG_LOCK:
            _atomic_write_json(CONFIG_FILE, config)
        resp = jsonify({'success': True, 'message': 'é…ç½®å·²ä¿å­˜'})
        resp.headers['Cache-Control'] = 'no-store'
        return resp
    except Exception as e:
        resp = jsonify({'success': False, 'message': str(e)})
        resp.headers['Cache-Control'] = 'no-store'
        return resp

@app.route('/api/config/export', methods=['GET'])
def api_export_config():
    """å¯¼å‡ºé…ç½®æ–‡ä»¶"""
    try:
        if os.path.exists(CONFIG_FILE):
            with _CONFIG_LOCK:
                raw = None
                try:
                    with open(CONFIG_FILE, 'r', encoding='utf-8') as f:
                        config = json.load(f)
                except json.JSONDecodeError:
                    with open(CONFIG_FILE, 'r', encoding='utf-8', errors='replace') as f:
                        raw = f.read()
                    try:
                        decoder = json.JSONDecoder()
                        config, _ = decoder.raw_decode(raw.lstrip())
                    except Exception:
                        config = None

            if isinstance(config, dict):
                ui_key = 'quant_ui_state_v1'
                ui = config.get(ui_key)
                if isinstance(ui, dict):
                    ui = dict(ui)
                    ui.pop('stockCode', None)
                    ui.pop('startDate', None)
                    ui.pop('endDate', None)
                    config = dict(config)
                    config[ui_key] = ui

            if config is None:
                payload_text = raw if isinstance(raw, str) else ''
                payload = payload_text.encode('utf-8', errors='replace')
            else:
                payload = json.dumps(config, ensure_ascii=False, indent=2).encode('utf-8')
            resp = send_file(
                io.BytesIO(payload),
                mimetype='application/json',
                as_attachment=True,
                download_name='quant_config_backup.json'
            )
            resp.headers['Cache-Control'] = 'no-store'
            return resp
        else:
            resp = jsonify({'success': False, 'message': 'é…ç½®æ–‡ä»¶ä¸å­˜åœ¨'})
            resp.headers['Cache-Control'] = 'no-store'
            return resp
    except Exception as e:
        resp = jsonify({'success': False, 'message': str(e)})
        resp.headers['Cache-Control'] = 'no-store'
        return resp

@app.route('/api/config/import', methods=['POST'])
def api_import_config():
    """å¯¼å…¥é…ç½®æ–‡ä»¶"""
    try:
        if 'file' not in request.files:
            resp = jsonify({'success': False, 'message': 'æœªä¸Šä¼ æ–‡ä»¶'})
            resp.headers['Cache-Control'] = 'no-store'
            return resp
        file = request.files['file']
        if file.filename == '':
            resp = jsonify({'success': False, 'message': 'æ–‡ä»¶åä¸ºç©º'})
            resp.headers['Cache-Control'] = 'no-store'
            return resp
        
        config = json.load(file)
        print(f"[å¯¼å…¥é…ç½®] è¯»å–åˆ°é…ç½®: {list(config.keys())}")
        
        ensure_config_dir()
        with _CONFIG_LOCK:
            _atomic_write_json(CONFIG_FILE, config)
        print(f"[å¯¼å…¥é…ç½®] é…ç½®å·²ä¿å­˜åˆ°: {CONFIG_FILE}")
        resp = jsonify({'success': True, 'message': 'é…ç½®å·²å¯¼å…¥'})
        resp.headers['Cache-Control'] = 'no-store'
        return resp
    except Exception as e:
        print(f"[å¯¼å…¥é…ç½®] é”™è¯¯: {e}")
        resp = jsonify({'success': False, 'message': str(e)})
        resp.headers['Cache-Control'] = 'no-store'
        return resp


@app.route('/api/strategy_backtest', methods=['POST'])
def api_strategy_backtest():
    """ç­–ç•¥å›æµ‹API - æ”¯æŒLSTMå’ŒEnsembleç­–ç•¥"""
    try:
        payload = request.get_json(silent=True) or {}
        
        ts_code = payload.get('ts_code', '000001.SZ')
        start_date = payload.get('start_date', '2023-01-01')
        end_date = payload.get('end_date', datetime.now().strftime('%Y-%m-%d'))
        strategy = payload.get('strategy', 'xgboost_ml')
        config = payload.get('config', {})
        init_capital = float(payload.get('init_capital', 1000000))
        
        import sys
        print(f"[ç­–ç•¥å›æµ‹] ts_code={ts_code}, strategy={strategy}, start={start_date}, end={end_date}", flush=True)
        print(f"[ç­–ç•¥å›æµ‹] config={config}", flush=True)
        print(f"[ç­–ç•¥å›æµ‹] TENSORFLOW_AVAILABLE={TENSORFLOW_AVAILABLE}", flush=True)
        sys.stdout.flush()
        
        # è·å–è‚¡ç¥¨æ•°æ®
        data = cache_manager.get_stock_data(ts_code, start_date, end_date) or []
        if not data or len(data) < 80:
            print(f"[ç­–ç•¥å›æµ‹] æ•°æ®ä¸è¶³: {len(data)} æ¡")
            return jsonify({
                'success': False,
                'message': f'æ•°æ®ä¸è¶³ï¼Œéœ€è¦è‡³å°‘80æ¡æ•°æ®ï¼Œå½“å‰åªæœ‰{len(data)}æ¡'
            })
        
        print(f"[ç­–ç•¥å›æµ‹] æ•°æ®é‡: {len(data)} æ¡", flush=True)
        sys.stdout.flush()
        
        df = pd.DataFrame(data)
        df = calculate_indicators(df)
        records = df.where(pd.notnull(df), None).to_dict('records')
        
        if not records:
            return jsonify({
                'success': False,
                'message': 'æ•°æ®å¤„ç†å¤±è´¥'
            })
        
        # æ‰§è¡Œç­–ç•¥
        print(f"[ç­–ç•¥å›æµ‹] å¼€å§‹æ‰§è¡Œç­–ç•¥: {strategy}", flush=True)
        sys.stdout.flush()
        signals = StrategyEngine.execute_strategy(records, strategy, config)
        print(f"[ç­–ç•¥å›æµ‹] ç­–ç•¥æ‰§è¡Œå®Œæˆ: {strategy}, ä¿¡å·æ•°={len(signals)}", flush=True)
        sys.stdout.flush()
        
        if not signals:
            return jsonify({
                'success': True,
                'strategy': strategy,
                'ts_code': ts_code,
                'signals': [],
                'result': {
                    'totalReturn': 0,
                    'annualReturn': 0,
                    'maxDrawdown': 0,
                    'sharpeRatio': 0,
                    'calmarRatio': 0,
                    'winRate': 0,
                    'profitLossRatio': 0,
                    'tradeCount': 0,
                    'score': 0,
                    'equity': [init_capital],
                    'initCapital': init_capital
                }
            })
        
        # è®¡ç®—å›æµ‹ç»“æœ
        result = StrategyEngine.calculate_backtest(records, signals, init_capital)
        
        # ç¡®ä¿æ‰€æœ‰å­—æ®µéƒ½æœ‰å€¼
        result = {
            'totalReturn': result.get('totalReturn', 0) or 0,
            'annualReturn': result.get('annualReturn', 0) or 0,
            'maxDrawdown': result.get('maxDrawdown', 0) or 0,
            'sharpeRatio': result.get('sharpeRatio', 0) or 0,
            'calmarRatio': result.get('calmarRatio', 0) or 0,
            'winRate': result.get('winRate', 0) or 0,
            'profitRatio': result.get('profitLossRatio', 0) or 0,
            'tradeCount': result.get('tradeCount', 0) or 0,
            'score': result.get('score', 0) or 0
        }
        
        print(f"[ç­–ç•¥å›æµ‹] å®Œæˆ: ä¿¡å·æ•°={len(signals)}, æ”¶ç›Š={result.get('totalReturn', 0):.2f}%")
        
        return jsonify({
            'success': True,
            'strategy': strategy,
            'ts_code': ts_code,
            'signals': signals,
            'result': result
        })
    except Exception as e:
        print(f"[ç­–ç•¥å›æµ‹] é”™è¯¯: {e}")
        import traceback
        traceback.print_exc()
        return jsonify({
            'success': False,
            'message': str(e)
        })


if __name__ == '__main__':
    print("="*50)
    print("Tushareé‡åŒ–äº¤æ˜“ç³»ç»Ÿåç«¯å¯åŠ¨")
    print("è¯·ç¡®ä¿å·²è®¾ç½®æ­£ç¡®çš„Tushare Token")
    print("è®¿é—®: http://localhost:5000")
    print("="*50)
    debug = (os.getenv('FLASK_DEBUG') or '0').strip() == '1'
    app.run(host='0.0.0.0', port=5000, debug=debug, use_reloader=False)
